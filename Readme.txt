Before starting to utilize any program: be sure to have downloaded the NLTK stopword.
You can do that executing the following line of code:

import nltk
nltk.download('stopwords')


To run the test follow these steps:
1) Go to folder \production\run 
2) Execute the command python .\exec_testing.py
3) The tests should be started


To run the building of the all datastructures
(Lexicon, Doc.Index, Inv.Index, Coll.Stats.) follow these step:

1) Put in the folder \production\run your compressed original collection
2) Rename it as collection.tar.gz
3) Execute the command python .\exec_indexing.py in the "production\run" folder
4) Follow the "wizard" to configure the indexing according to your memory capability and necessity:
4.1) Decide if using or not Stemming and Stop Word remouval and/or Compression to save space on disk
4.2) Decide if using Debug mode to print a human readable file to check the correctness of the produced data structures
5) The program should start working


Before starting to use the Query Processing, be sure to have installed Java on your machine at least version 1.8.

To run the query execution follow these steps:
1) Go to folder "production\run" folder
2) Initialize with your paths location about the datastructures generated by the previous program.
2.1) You can uncomment the already present path ajusting with your location
2.2) Don't forget to specify the two options COMPRESSION_MODE (true/false) and USE_STEMMING_AND_STOPWORD_REMOVAL(true/false)
3) Execute the command java -jar .\QueryProcessing.jar to run the Query processer program.
4) Follow the "wizard" to configure the processing according to your necessity:
4.1) Specify what Algorithm(DAAT/MaxScore) and Function (TFIDF/BM25) to use.
4.2) Specify if you want to do conjunctive or disjuntive searches.
4.3) Specify if you want cache enabled.
4.4) Specify the number of top k relevant documents to be shown.
5) The program should ask you to submit a query and press ENTER.
 
   