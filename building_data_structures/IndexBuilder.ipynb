{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b74a3f4f-5831-497c-9632-0129b8b291ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\building_data_structures\\..\\structures\\InvertedIndex.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\building_data_structures\\..\\structures\\LexiconRow.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\building_data_structures\\..\\structures\\DocumentIndex.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\building_data_structures\\..\\utilities\\General_Utilities.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\building_data_structures\\..\\structures\\DocumentIndexRow.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\building_data_structures\\..\\utilities\\Compression.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\building_data_structures\\..\\structures\\Lexicon.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\building_data_structures\\..\\building_data_structures\\CollectionStatistics.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\building_data_structures\\..\\structures\\BlockDescriptor.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\building_data_structures\\..\\query_processing\\Scoring.ipynb\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import shutil\n",
    "import os\n",
    "import math\n",
    "\n",
    "from typing import List, Dict, Tuple, Union, Any, Callable\n",
    "from typing import TextIO, BinaryIO\n",
    "from collections import Counter, defaultdict,OrderedDict\n",
    "\n",
    "\n",
    "import import_ipynb\n",
    "sys.path.append('../')  # Go up two folders to the project root\n",
    "\n",
    "from structures.InvertedIndex import Posting,InvertedIndex\n",
    "from structures.Lexicon import Lexicon\n",
    "from structures.LexiconRow import LexiconRow\n",
    "from structures.DocumentIndex import DocumentIndex\n",
    "from structures.DocumentIndexRow import DocumentIndexRow\n",
    "from structures.BlockDescriptor import BlockDescriptorBuilder,BlockDescriptor\n",
    "from building_data_structures.CollectionStatistics import  Collection_statistics\n",
    "from query_processing.Scoring import Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bbc5be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Costants\n",
    "\n",
    "DIR_TEMP_FOLDER=\"TEMP\"\n",
    "DIR_TEMP_DOC_ID=\"DOC_ID_TEMP\"\n",
    "DIR_TEMP_FREQ=\"FREQ_TEMP\"\n",
    "DIR_TEMP_LEXICON=\"LEXICON_TEMP\"\n",
    "\n",
    "DIR_LEXICON=\"LEXICON\"\n",
    "DIR_DOC_INDEX=\"DOC_INDEX\"\n",
    "DIR_INVERTED_INDEX=\"INV_INDEX\"\n",
    "\n",
    "PATH_FINAL_LEXICON=\"lexicon.bin\"\n",
    "PATH_FINAL_DOC_IDS=\"doc_ids.bin\"\n",
    "PATH_FINAL_FREQ=\"freq.bin\"\n",
    "PATH_FINAL_BLOCK_DESCRIPTOR=\"block_descriptors.bin\"\n",
    "PATH_COLLECTION_STATISTICS=\"collection_statistics.bin\"\n",
    "\n",
    "PATH_FINAL_INVERTED_INDEX_DEBUG=\"inverted_index.txt\"\n",
    "PATH_FINAL_LEXICON_DEBUG=\"lexicon.txt\"\n",
    "PATH_FINAL_DOCUMENT_INDEX=\"document_index.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "295cbd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexBuilder:\n",
    "\n",
    "    debug_mode:bool\n",
    "    compression_mode:bool\n",
    "    \n",
    "    \n",
    "    #For writing the final result\n",
    "    file_Final_Lexicon:BinaryIO\n",
    "    file_Final_DocIds:BinaryIO\n",
    "    file_Final_Freq:BinaryIO\n",
    "        \n",
    "    file_Final_Block_Descriptor:BinaryIO\n",
    "        \n",
    "    file_Final_InvertedIndex_Debug:TextIO\n",
    "        \n",
    "    file_Final_Lexicon_Debug:TextIO\n",
    "    \n",
    "        \n",
    "    #For merging operation\n",
    "    input_lex_temp_files:List[BinaryIO]\n",
    "    input_doc_id_temp_files:List[BinaryIO]\n",
    "    input_freq_temp_files:List[BinaryIO]\n",
    "        \n",
    "    b_d_b:BlockDescriptorBuilder \n",
    "    coll_statistics: Collection_statistics\n",
    "    scorer:Scoring\n",
    "    \n",
    "    def __init__(self,debug_mode:bool, compression_mode:bool)->None:\n",
    "        \"\"\"\n",
    "            Default contructor method for initialization of IndexBuilder class.\n",
    "        Args:\n",
    "            debug_mode: if true, it enable the modality for a clear human readable debug.\n",
    "            compression_mode: if true, it enable saving of posting list in a compression mode. \n",
    "        \n",
    "        \"\"\"\n",
    "        print (\"Index Builder Costructor\")\n",
    "        \n",
    "        self.debug_mode=debug_mode\n",
    "        self.compression_mode=compression_mode\n",
    "        \n",
    "        self.input_lex_temp_files = []\n",
    "        self.input_doc_id_temp_files= []\n",
    "        self.input_freq_temp_files= []\n",
    "        \n",
    "        self.coll_statistics=Collection_statistics(DIR_DOC_INDEX+\"/\"+PATH_COLLECTION_STATISTICS)\n",
    "        self.scorer=Scoring(self.coll_statistics)\n",
    "        self.b_d_b=BlockDescriptorBuilder(\"PATH_DA_DEFINIRE\")\n",
    "        \n",
    "        print (\"Using: \")\n",
    "        print (\"Debug Mode :\"+str(debug_mode))\n",
    "        print (\"Compression Mode :\"+str(compression_mode))\n",
    "        print (\"Nr of posting in each block descriptor: \"+str(self.b_d_b.min_posting_list_size))\n",
    "        print (\"\\n\\n\")\n",
    "              \n",
    "        \n",
    "        \n",
    "    def build_in_memory_index(self,list_of_documents:list)->InvertedIndex:\n",
    "        \"\"\"Given a list of document, build an Inverted Index in main Memory (RAM) and return it.\n",
    "           !! THIS METHOD IS NOT USED TO BUILD THE EFFECTIVE INDEX !!\n",
    "           \n",
    "           This is used in test phase to check rapidly if the output obtained is correct or not.\n",
    "        Args:\n",
    "            list_of_documents: list of strings representing a document.\n",
    "            \n",
    "        Returns:\n",
    "            An Inverted Index in memory object of the list_of_documents.\n",
    "        \"\"\"\n",
    "        invertedIndex = InvertedIndex()\n",
    "        for doc in list_of_documents:\n",
    "            doc_list = doc.split()\n",
    "            doc_id = int(doc_list[0])\n",
    "            text = ' '.join(doc_list[1:])\n",
    "            tc = Counter(text.lower().split())  # dict with term counts, QUI USARE DIRETTAMENTE IL CONTENUTO GIA' PRE-PROCESSATO\n",
    "            for term, freq in tc.items():\n",
    "                invertedIndex.add_posting(term, doc_id, freq)\n",
    "        return invertedIndex\n",
    "        \n",
    "    \n",
    "    def init_spimi(self)->None:\n",
    "        \"\"\" Function to initialize a clear environment to start building the needed data structures for the spimi phase.\"\"\"\n",
    "        \n",
    "        if os.path.exists(DIR_TEMP_FOLDER):\n",
    "            shutil.rmtree(DIR_TEMP_FOLDER)\n",
    "\n",
    "        os.makedirs(DIR_TEMP_FOLDER)\n",
    "        os.makedirs(DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_DOC_ID)\n",
    "        os.makedirs(DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_FREQ)\n",
    "        os.makedirs(DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_LEXICON)\n",
    "        \n",
    "        if os.path.exists(DIR_DOC_INDEX):\n",
    "            shutil.rmtree(DIR_DOC_INDEX)\n",
    "            \n",
    "        os.makedirs(DIR_DOC_INDEX)\n",
    "        \n",
    "        \n",
    "    def init_index_merging(self)->None:\n",
    "        \"\"\" Function to initialize a clear environment to start building the effective datastructures. \"\"\"\n",
    "        \n",
    "        if os.path.exists(DIR_LEXICON):\n",
    "            shutil.rmtree(DIR_LEXICON)\n",
    "                \n",
    "        if os.path.exists(DIR_INVERTED_INDEX):\n",
    "            shutil.rmtree(DIR_INVERTED_INDEX)\n",
    "\n",
    "        os.makedirs(DIR_LEXICON)\n",
    "        os.makedirs(DIR_INVERTED_INDEX)\n",
    "                    \n",
    "        if (self.debug_mode):\n",
    "            if os.path.exists(PATH_FINAL_INVERTED_INDEX_DEBUG):\n",
    "                os.remove(PATH_FINAL_INVERTED_INDEX_DEBUG)\n",
    "            \n",
    "    \n",
    "    \n",
    "    def __open_files_for_merging_operation(self)->None:\n",
    "        \"\"\" Function to open the needed files for the merging operation. \"\"\"\n",
    "        \n",
    "        file_lex_temp_paths = [DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_LEXICON+\"/\"+f for f in os.listdir(DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_LEXICON)]\n",
    "        file_doc_id_temp_paths = [DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_DOC_ID+\"/\"+f for f in os.listdir(DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_DOC_ID)] \n",
    "        file_freq_temp_paths = [DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_FREQ+\"/\"+f for f in os.listdir(DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_FREQ)] \n",
    "\n",
    "        self.input_lex_temp_files = [open(file, 'rb') for file in file_lex_temp_paths]  #Open all the blocks in parallel\n",
    "        self.input_doc_id_temp_files = [open(file, 'rb') for file in file_doc_id_temp_paths]  #Open all the blocks in parallel\n",
    "        self.input_freq_temp_files = [open(file, 'rb') for file in file_freq_temp_paths]  #Open all the blocks in parallel\n",
    "        \n",
    "        self.file_Final_Lexicon=open(DIR_LEXICON+\"/\"+PATH_FINAL_LEXICON, 'ab') \n",
    "        self.file_Final_DocIds=open(DIR_INVERTED_INDEX+\"/\"+PATH_FINAL_DOC_IDS, 'ab') \n",
    "        self.file_Final_Freq=open(DIR_INVERTED_INDEX+\"/\"+PATH_FINAL_FREQ, 'ab') \n",
    "        self.file_Final_Block_Descriptor=open(DIR_INVERTED_INDEX+\"/\"+PATH_FINAL_BLOCK_DESCRIPTOR, 'ab')\n",
    "\n",
    "        if (self.debug_mode):\n",
    "            self.file_Final_InvertedIndex_Debug=open(DIR_INVERTED_INDEX+\"/\"+PATH_FINAL_INVERTED_INDEX_DEBUG,'a')\n",
    "            self.file_Final_Lexicon_Debug=open(DIR_LEXICON+\"/\"+PATH_FINAL_LEXICON_DEBUG,'a')\n",
    "            self.file_Final_Lexicon_Debug.write(LexiconRow.to_string_header()+\"\\n\")\n",
    "        \n",
    "        \n",
    "    def __close_files_for_merging_operation(self)->None:\n",
    "        \"\"\" This function is used to close all the opened resource needed for the creation of the data structures\n",
    "            in merging operation.\n",
    "        \"\"\"\n",
    "        \n",
    "        for file in self.input_lex_temp_files:\n",
    "            file.close()  \n",
    "\n",
    "        for file in self.input_doc_id_temp_files:\n",
    "            file.close()  \n",
    "\n",
    "        for file in self.input_freq_temp_files:\n",
    "            file.close()  \n",
    "\n",
    "        self.file_Final_Lexicon.close()\n",
    "        self.file_Final_DocIds.close()\n",
    "        self.file_Final_Freq.close()\n",
    "        self.file_Final_Block_Descriptor.close()\n",
    "\n",
    "        if (self.debug_mode):\n",
    "            self.file_Final_InvertedIndex_Debug.close()\n",
    "            self.file_Final_Lexicon_Debug.close()\n",
    "        \n",
    "    def __check_all_blocks_are_read(self,offset_lexicon_terms:List):\n",
    "        \"\"\" This functions checks if the all the blocks opened in parallel are read or not.\n",
    "            The condition is matched when the list contains all None elements. \n",
    "        Args:\n",
    "            offset_lexicon_terms: a list of offset\n",
    "        \n",
    "        Returns:\n",
    "            True if the list contains all None elements.\n",
    "        \"\"\"\n",
    "        #print (offset_lexicon_terms)\n",
    "        return sum(1 if element is None else 0 for element in offset_lexicon_terms) == len(offset_lexicon_terms)\n",
    "    \n",
    "    def __find_min_term(self,lexicon_temp_terms:List,offset_lex_temp:List):\n",
    "        \"\"\" This function checks and returns the minimum term (lexicographically) among blocks \n",
    "             at the current reading offset.\n",
    "             If a offset_lex_temp[i] contains None means that the i block is completely read.\n",
    "             \n",
    "         Args:\n",
    "             lexicon_temp_terms: the list of current lexicon element (each position is a different block)\n",
    "             offset_lexicon_terms: the list of current lexicon element position inside the file (each position is a different block)\n",
    "         Return:\n",
    "             a string representing the current min term or None if all blocks are read\n",
    "         \n",
    "        \"\"\"\n",
    "    \n",
    "        if not lexicon_temp_terms:\n",
    "            return None  # Return None for an empty list\n",
    "    \n",
    "        min_term=None\n",
    "\n",
    "        for index,lex_elem in enumerate(lexicon_temp_terms):\n",
    "            if(offset_lex_temp[index]!=None):\n",
    "                if (min_term==None):\n",
    "                    min_term=lexicon_temp_terms[index].term\n",
    "\n",
    "                if (lex_elem.term<min_term):\n",
    "                    min_term=lex_elem.term\n",
    "    \n",
    "        return min_term\n",
    "    \n",
    "    \n",
    "    def __save_postings_and_block_descriptor(self,new_term:bool,\n",
    "                                             min_term:str,\n",
    "                                             merged_posting_list:List[Posting],\n",
    "                                             current_offset_doc_ids:int,current_offset_freq:int,\n",
    "                                             current_offset_block_descriptor:int,\n",
    "                                             block_descriptor:BlockDescriptor):\n",
    "        \"\"\"\n",
    "        This function is used to save a posting list to related disk files calling the proper Inverted Index methods \n",
    "        and the related Block Descriptor.\n",
    "        This function is created to avoid redundancy in the code considering that it is called in at least 2 points.\n",
    "        \n",
    "        Args:\n",
    "            new_term: if true, in debug enabled is used to write different a new line in the output clear file.\n",
    "            merged_posting_list: the posting list to be stored on disk\n",
    "            current_offset_doc_ids: the offset position to save the doc ids of the merged_posting_list inside a file\n",
    "            current_offset_freq: the offset position to save the freq of the merged_posting_list inside a file\n",
    "            current_offset_block_descriptor: the offset position to save the block descriptor of the merged_posting_list inside a file\n",
    "            block_descriptor: the block descriptor to use for saving on disk the information about the merged_posting_list\n",
    "            \n",
    "        Returns:\n",
    "            current_offset_doc_ids: the new offset free position inside the file of the doc ids\n",
    "            current_offset_freq: the new offset free position inside the file of the freq\n",
    "            current_offset_block_descriptor: the new offset free position inside the file of the block descriptor\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        saved_offset_doc_ids=current_offset_doc_ids\n",
    "        saved_offset_freq=current_offset_freq\n",
    "                                \n",
    "        #Write the posting on disk.\n",
    "        current_offset_doc_ids,current_offset_freq=InvertedIndex.write_to_files_a_posting_list(merged_posting_list,self.compression_mode,self.file_Final_DocIds,self.file_Final_Freq,current_offset_doc_ids,current_offset_freq)\n",
    "        block_descriptor.nr_postings+=len(merged_posting_list)\n",
    "                                \n",
    "        block_descriptor.doc_ids_bytes_size=(current_offset_doc_ids-saved_offset_doc_ids)\n",
    "        block_descriptor.freq_bytes_size=(current_offset_freq-saved_offset_freq)\n",
    "                                \n",
    "        block_descriptor.min_doc_id=merged_posting_list[0].doc_id\n",
    "        block_descriptor.max_doc_id=merged_posting_list[-1].doc_id\n",
    "                                \n",
    "        #Writing the block descriptor.\n",
    "        current_offset_block_descriptor=block_descriptor.write_block_descriptor_on_disk_to_opened_file(self.file_Final_Block_Descriptor,current_offset_block_descriptor)\n",
    "        \n",
    "        if (self.debug_mode):\n",
    "            InvertedIndex.write_to_file_a_posting_list_debug_mode(self.file_Final_InvertedIndex_Debug,min_term, merged_posting_list, new_term)\n",
    "        \n",
    "        return current_offset_doc_ids,current_offset_freq,current_offset_block_descriptor\n",
    "        \n",
    "    \n",
    "    \n",
    "    def single_pass_in_memory_indexing(self,list_of_documents:list,inv_index_block_size: int=2200,doc_index_block_size: int=2200,debug_mode:bool=False)-> None:\n",
    "\n",
    "            ind = InvertedIndex()\n",
    "            document_index = DocumentIndex()\n",
    "\n",
    "            nr_block=0\n",
    "\n",
    "            self.init_spimi()\n",
    "            \n",
    "            #Read all the documents and write the index at blocks on disk when memory is full, cleaning the memory data structure.\n",
    "            \n",
    "            for doc in list_of_documents:\n",
    "                # Separate the doc_id from the content of the real document \n",
    "                doc_list = doc.split()\n",
    "                doc_id = int(doc_list[0])\n",
    "                text = ' '.join(doc_list[1:])\n",
    "\n",
    "                if (sys.getsizeof(document_index.get_structure()) > doc_index_block_size):\n",
    "                    if (self.debug_mode):\n",
    "                        document_index.write_document_index_to_file(DIR_DOC_INDEX+\"/\"+PATH_FINAL_DOCUMENT_INDEX, document_index.get_structure())\n",
    "                    document_index.clear_structure()\n",
    "\n",
    "                document_index.add_document(doc_id, text)\n",
    "\n",
    "                tc = Counter(text.lower().split())  # dict with term counts, Here there is the already preprocessed content\n",
    "                for term, freq in tc.items():\n",
    "                    if (sys.getsizeof(ind.get_structure()) > inv_index_block_size):  #Free memory available\n",
    "\n",
    "                        LEXICON_TEMP_BLOCK_PATH=DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_LEXICON+\"/block_nr_\"+str(nr_block)\n",
    "                        DOC_IDS_TEMP_BLOCK_PATH=DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_DOC_ID+\"/block_nr_\"+str(nr_block)\n",
    "                        FREQ_TEMP_BLOCK_PATH=DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_FREQ+\"/block_nr_\"+str(nr_block)\n",
    "\n",
    "                        ind.write_to_block_all_index_in_memory(LEXICON_TEMP_BLOCK_PATH,DOC_IDS_TEMP_BLOCK_PATH,FREQ_TEMP_BLOCK_PATH)\n",
    "\n",
    "                        if (self.debug_mode):\n",
    "                            ind.write_to_block_debug_mode(DIR_TEMP_FOLDER+\"/inv_index_\"+str(nr_block)+\".txt\")\n",
    "                        ind.clear_structure()\n",
    "                        nr_block=nr_block+1 \n",
    "\n",
    "                    ind.add_posting(term, doc_id, freq)\n",
    "\n",
    "            if (not document_index.is_empty()):   \n",
    "                if (self.debug_mode):\n",
    "                    document_index.write_document_index_to_file(DIR_DOC_INDEX+\"/\"+ PATH_FINAL_DOCUMENT_INDEX, document_index.get_structure())\n",
    "\n",
    "            #Finally, saving the last remaing block.       \n",
    "            if (not ind.is_empty()):\n",
    "                LEXICON_TEMP_BLOCK_PATH=DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_LEXICON+\"/block_nr_\"+str(nr_block)\n",
    "                DOC_IDS_TEMP_BLOCK_PATH=DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_DOC_ID+\"/block_nr_\"+str(nr_block)\n",
    "                FREQ_TEMP_BLOCK_PATH=DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_FREQ+\"/block_nr_\"+str(nr_block)\n",
    "\n",
    "                ind.write_to_block_all_index_in_memory(LEXICON_TEMP_BLOCK_PATH,DOC_IDS_TEMP_BLOCK_PATH,FREQ_TEMP_BLOCK_PATH)\n",
    "\n",
    "                if (self.debug_mode):\n",
    "                    ind.write_to_block_debug_mode(DIR_TEMP_FOLDER+\"/inv_index_\"+str(nr_block)+\".txt\")\n",
    "            \n",
    "            self.coll_statistics.num_documents=document_index.number_of_documents\n",
    "            self.coll_statistics.sum_document_lengths=document_index.total_document_length\n",
    "            #self.coll_statistics.save_statistics()\n",
    "            self.coll_statistics.write_binary_mode()\n",
    "        \n",
    "    def index_merging(self)-> None:\n",
    "\n",
    "        self.init_index_merging()\n",
    "        \n",
    "        try:\n",
    "\n",
    "            self.__open_files_for_merging_operation()\n",
    "\n",
    "            #Initialization of empty lexicon row elements for each block.\n",
    "            lexicon_temp_elems=[LexiconRow(\"\",0) for i in range (len (self.input_lex_temp_files))]\n",
    "            \n",
    "            #Start reading the first element in the lexicon of each block and saving the offset of each read.\n",
    "            offset_lex_temp=[terms.read_lexicon_row_on_disk_from_opened_file(self.input_lex_temp_files[index],0) for index,terms in enumerate(lexicon_temp_elems)]\n",
    "            \n",
    "            #print(offset_lex_temp)\n",
    "            \n",
    "            number_of_distinct_terms=0\n",
    "            \n",
    "            current_offset_lexicon=0\n",
    "            current_offset_doc_ids=0\n",
    "            current_offset_freq=0\n",
    "            current_offset_block_descriptor=0\n",
    "\n",
    "            #First, check if all blocks opened blocks are read.\n",
    "            while (not self.__check_all_blocks_are_read(offset_lex_temp)):\n",
    "\n",
    "                #Find the minimum term among the opened blocks.\n",
    "                min_term=self.__find_min_term(lexicon_temp_elems,offset_lex_temp)\n",
    "                tot_posting=sum(lex_elem.dft if (lex_elem.term==min_term) else 0 for lex_elem in lexicon_temp_elems) \n",
    "\n",
    "                #This variable is used to mark if it is the first time a new term is elaborated among blocks, to print in debug a new line.\n",
    "                new_term=True\n",
    "                \n",
    "                print(\"\\nMin termine corrente: \"+min_term+ \" nr. postings: \"+str(tot_posting))\n",
    "                \n",
    "                #New Term to add definitively\n",
    "                new_Lexicon_Def=LexiconRow(min_term,tot_posting)\n",
    "                new_Lexicon_Def.docidOffset=current_offset_doc_ids\n",
    "                new_Lexicon_Def.frequencyOffset=current_offset_freq\n",
    "                new_Lexicon_Def.blockOffset=current_offset_block_descriptor  \n",
    "                \n",
    "                new_Lexicon_Def.numBlocks=self.b_d_b.get_number_of_blocks(tot_posting)\n",
    "                \n",
    "                nr_of_postings_per_block_descriptor=math.ceil(tot_posting/new_Lexicon_Def.numBlocks)\n",
    "                \n",
    "                \n",
    "                #Initialization of empty block descriptor.\n",
    "                block_descriptor=BlockDescriptor(0,current_offset_doc_ids,current_offset_freq,0,0,0,0)\n",
    "                \n",
    "                merged_posting_list=[]\n",
    "                MAX_TF=0\n",
    "                #Number of postings that I can load in memory RAM.\n",
    "                readable_postings=nr_of_postings_per_block_descriptor-len(merged_posting_list)\n",
    "                \n",
    "                for index,lex_term in enumerate(lexicon_temp_elems):\n",
    "                    \n",
    "                    if (lex_term.term==min_term):\n",
    "                        \n",
    "                        #Nr of posting in this block \"file\".\n",
    "                        posting_to_be_read=lex_term.dft\n",
    "                        \n",
    "                        doc_id_block_offset=lex_term.docidOffset\n",
    "                        freq_block_offset=lex_term.frequencyOffset\n",
    "                        \n",
    "                        #There are still some posting related to this min_term to be read in this block.\n",
    "                        while(posting_to_be_read>0):\n",
    "                            \n",
    "                            #Here I can read in one shot all the posting list of that term in the block in memory.\n",
    "                            if (posting_to_be_read<readable_postings):\n",
    "                                nr_eff_readable=posting_to_be_read\n",
    "                                posting_to_be_read=0\n",
    "                            else:\n",
    "                                #I read just what can be \"currently\" contained in main memory until block descriptor is full.\n",
    "                                nr_eff_readable=readable_postings\n",
    "                                posting_to_be_read-=readable_postings\n",
    "\n",
    "                            postingList,doc_id_block_offset,freq_block_offset=InvertedIndex.read_from_files_a_posting_list(\n",
    "                                                                                self.input_doc_id_temp_files[index],self.input_freq_temp_files[index],\n",
    "                                                                                False,\n",
    "                                                                                doc_id_block_offset,freq_block_offset,\n",
    "                                                                                nr_eff_readable)  \n",
    "                            #print(postingList) \n",
    "                            \n",
    "                            curr_max_tf=InvertedIndex.compute_max_term_frequency_of_posting_list(postingList)\n",
    "                            if (curr_max_tf>MAX_TF):\n",
    "                                MAX_TF=curr_max_tf\n",
    "                            #I combine the posting I have just read with those read previously\n",
    "                            merged_posting_list=InvertedIndex.merge_posting_lists(merged_posting_list,postingList)\n",
    "                        \n",
    "                            readable_postings=nr_of_postings_per_block_descriptor-len(merged_posting_list)\n",
    "                            \n",
    "                            if(readable_postings==0):\n",
    "                               \n",
    "                                #I write the complete posting to disk (with possible compression) and the related block descriptor.\n",
    "                                current_offset_doc_ids,current_offset_freq,current_offset_block_descriptor= self.__save_postings_and_block_descriptor(new_term,min_term,\n",
    "                                                                          merged_posting_list,\n",
    "                                                                          current_offset_doc_ids,current_offset_freq,\n",
    "                                                                          current_offset_block_descriptor,\n",
    "                                                                          block_descriptor)\n",
    "                                new_term=False\n",
    "                                #Re-set the datastructures for new block descriptor.\n",
    "                                merged_posting_list.clear()\n",
    "                                block_descriptor=BlockDescriptor(0,current_offset_doc_ids,current_offset_freq,0,0,0,0)\n",
    "                                readable_postings=nr_of_postings_per_block_descriptor\n",
    "                        \n",
    "                        ### END-WHILE ###\n",
    "                       ### END - IF \n",
    "                    \n",
    "                        # I read the next lexicon term related to this block file.\n",
    "                        offset_lex_temp[index]=lex_term.read_lexicon_row_on_disk_from_opened_file(self.input_lex_temp_files[index],offset_lex_temp[index])\n",
    "                \n",
    "                ### END-FOR       \n",
    "                if (len(merged_posting_list)>0):\n",
    "            \n",
    "                    #Here I finish to write the remaining posting list to disk and the related block descriptor.\n",
    "                    current_offset_doc_ids,current_offset_freq,current_offset_block_descriptor= self.__save_postings_and_block_descriptor(new_term,min_term,merged_posting_list,\n",
    "                                                                          current_offset_doc_ids,current_offset_freq,\n",
    "                                                                          current_offset_block_descriptor,\n",
    "                                                                          block_descriptor)\n",
    "                \n",
    "                \n",
    "                # In questa parte qui si vanno a calcolare le definitive metriche per le query ed anche \n",
    "                # i descrittori di blocco per skipping e altro. \n",
    "                #Valutare se aggiungere calcolo metriche o altre informazioni utili per la query execution.\n",
    "                \n",
    "                new_Lexicon_Def.idft=self.scorer.compute_IDFT(tot_posting)\n",
    "                new_Lexicon_Def.max_tf=MAX_TF\n",
    "                new_Lexicon_Def.maxTFIDF=self.scorer.compute_TFIDF(MAX_TF,new_Lexicon_Def.idft)\n",
    "                \n",
    "                \n",
    "                new_Lexicon_Def.docidSize=(current_offset_doc_ids-new_Lexicon_Def.docidOffset)\n",
    "                new_Lexicon_Def.frequencySize=(current_offset_freq-new_Lexicon_Def.frequencyOffset)\n",
    "                \n",
    "                #Save all the information related to the term just elaborated to disk.\n",
    "                if (self.debug_mode):\n",
    "                    new_Lexicon_Def.write_lexicon_row_on_disk_debug_mode(self.file_Final_Lexicon_Debug)\n",
    "                current_offset_lexicon=new_Lexicon_Def.write_lexicon_row_on_disk_to_opened_file(self.file_Final_Lexicon,current_offset_lexicon)\n",
    "                \n",
    "                \n",
    "                number_of_distinct_terms+=1\n",
    "            \n",
    "            self.coll_statistics.num_distinct_terms=number_of_distinct_terms\n",
    "            self.coll_statistics.save_statistics()\n",
    "            print(\"END METHOD!\")   \n",
    "            \n",
    "        except Exception as e:   \n",
    "                raise e\n",
    "        finally:\n",
    "                #Be sure to close all the opened files in parallel\n",
    "                self.__close_files_for_merging_operation()\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97f44be",
   "metadata": {},
   "source": [
    "# Example of usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "561ce13e-bc7a-4357-bb3b-9102802a83ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_doc=[\n",
    "    \"0     The pen is on the table\",\n",
    "    \"1     The day is very sunny\",\n",
    "    \"2     Goodmoring new article\",\n",
    "    \"3     A cat is faster then a dog\",\n",
    "    \"4     How are you\",\n",
    "    \"5     A boy is a man with low age\",\n",
    "    \"6     Lake Ontario is one of the biggest lake in the world\",\n",
    "    \"7     English is worst than Italian\",\n",
    "    \"8     Spiderman is the best superhero in Marvel universe\",\n",
    "    \"9     Last night I saw a Netflix series\",\n",
    "    \"10    A penny for your thoughts\",\n",
    "    \"11    Actions speak louder than words\",\n",
    "    \"12    All that glitters is not gold\",\n",
    "    \"13    Beauty is in the eye of the beholder\",\n",
    "    \"14    Birds of a feather flock together\",\n",
    "    \"15    Cleanliness is next to godliness\",\n",
    "    \"16    Don't count your chickens before they hatch\",\n",
    "    \"17    Every people cloud has a silver lining people\",\n",
    "    \"18    Fool me once shame on you fool me twice shame on me\",\n",
    "    \"19    Honesty is the best policy.\",\n",
    "    \"20    If the shoe fits, wear it\",\n",
    "    \"21    It's a piece of cake\",\n",
    "    \"22    Jump on the bandwagon\",\n",
    "    \"23    Keep your chin up\",\n",
    "    \"24    Let the cat out of the bag\",\n",
    "    \"25    Make a long story short\",\n",
    "    \"26    Necessity is the mother of invention\",\n",
    "    \"27    Once in a blue moon\",\n",
    "    \"28    Practice makes perfect\",\n",
    "    \"29    Read between the lines\",\n",
    "    \"30    The early bird catches people the worm\",\n",
    "    \"31    The pen is mightier than the sword\",\n",
    "    \"32    There's no smoke without fire\",\n",
    "    \"33    To each his own\",\n",
    "    \"34    Two heads are better than one\",\n",
    "    \"35    You can't have your cake and eat it too\",\n",
    "    \"36    A watched pot never boils\",\n",
    "    \"37    Beggars can't be choosers\",\n",
    "    \"38    Better late than never\",\n",
    "    \"39    Calm before the storm\",\n",
    "    \"40    Curiosity killed the cat\",\n",
    "    \"41    Every dog has its day\",\n",
    "    \"42    Great minds think alike\",\n",
    "    \"43    Hope for the best prepare for the worst\",\n",
    "    \"44    Ignorance is bliss.\",\n",
    "    \"45    It's the last straw that breaks the camel's back\",\n",
    "    \"46    Laugh and the world laughs with you weep and you weep alone\",\n",
    "    \"47    Money can't buy happiness\",\n",
    "    \"48    No news is good news\",\n",
    "    \"49    Out of sight out of mind\",\n",
    "    \"50    People who live in glass houses shouldn't throw stones\",\n",
    "    \"51    Rome wasn't built in a day\",\n",
    "    \"52    Silence is golden\",\n",
    "    \"53    The apple doesn't fall far from the tree\",\n",
    "    \"54    The more, the merrier\",\n",
    "    \"55    There's no place like home\",\n",
    "    \"56    Two wrongs don't make a right\",\n",
    "    \"57    When in Rome do as the Romans do\",\n",
    "    \"58    You reap what you sow\",\n",
    "    \"59    People people people\"\n",
    "]\n",
    "\n",
    "\n",
    "#indexBuilder=IndexBuilder()\n",
    "#indexBuilder.build_block_sort_base_indexing(tot_doc,\"complete_inverted_index\",2220,False,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1861f8b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Builder Costructor\n",
      "Using: \n",
      "Debug Mode :True\n",
      "Compression Mode :False\n",
      "Nr of posting in each block descriptor: 1024\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Min termine corrente: a                              nr. postings: 12\n",
      "\n",
      "Min termine corrente: actions                        nr. postings: 1\n",
      "\n",
      "Min termine corrente: age                            nr. postings: 1\n",
      "\n",
      "Min termine corrente: alike                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: all                            nr. postings: 1\n",
      "\n",
      "Min termine corrente: alone                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: and                            nr. postings: 2\n",
      "\n",
      "Min termine corrente: apple                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: are                            nr. postings: 2\n",
      "\n",
      "Min termine corrente: article                        nr. postings: 1\n",
      "\n",
      "Min termine corrente: as                             nr. postings: 1\n",
      "\n",
      "Min termine corrente: back                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: bag                            nr. postings: 1\n",
      "\n",
      "Min termine corrente: bandwagon                      nr. postings: 1\n",
      "\n",
      "Min termine corrente: be                             nr. postings: 1\n",
      "\n",
      "Min termine corrente: beauty                         nr. postings: 1\n",
      "\n",
      "Min termine corrente: before                         nr. postings: 2\n",
      "\n",
      "Min termine corrente: beggars                        nr. postings: 1\n",
      "\n",
      "Min termine corrente: beholder                       nr. postings: 1\n",
      "\n",
      "Min termine corrente: best                           nr. postings: 3\n",
      "\n",
      "Min termine corrente: better                         nr. postings: 2\n",
      "\n",
      "Min termine corrente: between                        nr. postings: 1\n",
      "\n",
      "Min termine corrente: biggest                        nr. postings: 1\n",
      "\n",
      "Min termine corrente: bird                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: birds                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: bliss.                         nr. postings: 1\n",
      "\n",
      "Min termine corrente: blue                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: boils                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: boy                            nr. postings: 1\n",
      "\n",
      "Min termine corrente: breaks                         nr. postings: 1\n",
      "\n",
      "Min termine corrente: built                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: buy                            nr. postings: 1\n",
      "\n",
      "Min termine corrente: cake                           nr. postings: 2\n",
      "\n",
      "Min termine corrente: calm                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: camel's                        nr. postings: 1\n",
      "\n",
      "Min termine corrente: can't                          nr. postings: 3\n",
      "\n",
      "Min termine corrente: cat                            nr. postings: 3\n",
      "\n",
      "Min termine corrente: catches                        nr. postings: 1\n",
      "\n",
      "Min termine corrente: chickens                       nr. postings: 1\n",
      "\n",
      "Min termine corrente: chin                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: choosers                       nr. postings: 1\n",
      "\n",
      "Min termine corrente: cleanliness                    nr. postings: 1\n",
      "\n",
      "Min termine corrente: cloud                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: count                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: curiosity                      nr. postings: 1\n",
      "\n",
      "Min termine corrente: day                            nr. postings: 3\n",
      "\n",
      "Min termine corrente: do                             nr. postings: 1\n",
      "\n",
      "Min termine corrente: doesn't                        nr. postings: 1\n",
      "\n",
      "Min termine corrente: dog                            nr. postings: 2\n",
      "\n",
      "Min termine corrente: don't                          nr. postings: 2\n",
      "\n",
      "Min termine corrente: each                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: early                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: eat                            nr. postings: 1\n",
      "\n",
      "Min termine corrente: english                        nr. postings: 1\n",
      "\n",
      "Min termine corrente: every                          nr. postings: 2\n",
      "\n",
      "Min termine corrente: eye                            nr. postings: 1\n",
      "\n",
      "Min termine corrente: fall                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: far                            nr. postings: 1\n",
      "\n",
      "Min termine corrente: faster                         nr. postings: 1\n",
      "\n",
      "Min termine corrente: feather                        nr. postings: 1\n",
      "\n",
      "Min termine corrente: fire                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: fits,                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: flock                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: fool                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: for                            nr. postings: 2\n",
      "\n",
      "Min termine corrente: from                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: glass                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: glitters                       nr. postings: 1\n",
      "\n",
      "Min termine corrente: godliness                      nr. postings: 1\n",
      "\n",
      "Min termine corrente: gold                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: golden                         nr. postings: 1\n",
      "\n",
      "Min termine corrente: good                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: goodmoring                     nr. postings: 1\n",
      "\n",
      "Min termine corrente: great                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: happiness                      nr. postings: 1\n",
      "\n",
      "Min termine corrente: has                            nr. postings: 2\n",
      "\n",
      "Min termine corrente: hatch                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: have                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: heads                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: his                            nr. postings: 1\n",
      "\n",
      "Min termine corrente: home                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: honesty                        nr. postings: 1\n",
      "\n",
      "Min termine corrente: hope                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: houses                         nr. postings: 1\n",
      "\n",
      "Min termine corrente: how                            nr. postings: 1\n",
      "\n",
      "Min termine corrente: i                              nr. postings: 1\n",
      "\n",
      "Min termine corrente: if                             nr. postings: 1\n",
      "\n",
      "Min termine corrente: ignorance                      nr. postings: 1\n",
      "\n",
      "Min termine corrente: in                             nr. postings: 7\n",
      "\n",
      "Min termine corrente: invention                      nr. postings: 1\n",
      "\n",
      "Min termine corrente: is                             nr. postings: 16\n",
      "\n",
      "Min termine corrente: it                             nr. postings: 2\n",
      "\n",
      "Min termine corrente: it's                           nr. postings: 2\n",
      "\n",
      "Min termine corrente: italian                        nr. postings: 1\n",
      "\n",
      "Min termine corrente: its                            nr. postings: 1\n",
      "\n",
      "Min termine corrente: jump                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: keep                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: killed                         nr. postings: 1\n",
      "\n",
      "Min termine corrente: lake                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: last                           nr. postings: 2\n",
      "\n",
      "Min termine corrente: late                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: laugh                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: laughs                         nr. postings: 1\n",
      "\n",
      "Min termine corrente: let                            nr. postings: 1\n",
      "\n",
      "Min termine corrente: like                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: lines                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: lining                         nr. postings: 1\n",
      "\n",
      "Min termine corrente: live                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: long                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: louder                         nr. postings: 1\n",
      "\n",
      "Min termine corrente: low                            nr. postings: 1\n",
      "\n",
      "Min termine corrente: make                           nr. postings: 2\n",
      "\n",
      "Min termine corrente: makes                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: man                            nr. postings: 1\n",
      "\n",
      "Min termine corrente: marvel                         nr. postings: 1\n",
      "\n",
      "Min termine corrente: me                             nr. postings: 1\n",
      "\n",
      "Min termine corrente: merrier                        nr. postings: 1\n",
      "\n",
      "Min termine corrente: mightier                       nr. postings: 1\n",
      "\n",
      "Min termine corrente: mind                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: minds                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: money                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: moon                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: more,                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: mother                         nr. postings: 1\n",
      "\n",
      "Min termine corrente: necessity                      nr. postings: 1\n",
      "\n",
      "Min termine corrente: netflix                        nr. postings: 1\n",
      "\n",
      "Min termine corrente: never                          nr. postings: 2\n",
      "\n",
      "Min termine corrente: new                            nr. postings: 1\n",
      "\n",
      "Min termine corrente: news                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: next                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: night                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: no                             nr. postings: 3\n",
      "\n",
      "Min termine corrente: not                            nr. postings: 1\n",
      "\n",
      "Min termine corrente: of                             nr. postings: 7\n",
      "\n",
      "Min termine corrente: on                             nr. postings: 3\n",
      "\n",
      "Min termine corrente: once                           nr. postings: 2\n",
      "\n",
      "Min termine corrente: one                            nr. postings: 2\n",
      "\n",
      "Min termine corrente: ontario                        nr. postings: 1\n",
      "\n",
      "Min termine corrente: out                            nr. postings: 2\n",
      "\n",
      "Min termine corrente: own                            nr. postings: 1\n",
      "\n",
      "Min termine corrente: pen                            nr. postings: 2\n",
      "\n",
      "Min termine corrente: penny                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: people                         nr. postings: 4\n",
      "\n",
      "Min termine corrente: perfect                        nr. postings: 1\n",
      "\n",
      "Min termine corrente: piece                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: place                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: policy.                        nr. postings: 1\n",
      "\n",
      "Min termine corrente: pot                            nr. postings: 1\n",
      "\n",
      "Min termine corrente: practice                       nr. postings: 1\n",
      "\n",
      "Min termine corrente: prepare                        nr. postings: 1\n",
      "\n",
      "Min termine corrente: read                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: reap                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: right                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: romans                         nr. postings: 1\n",
      "\n",
      "Min termine corrente: rome                           nr. postings: 2\n",
      "\n",
      "Min termine corrente: saw                            nr. postings: 1\n",
      "\n",
      "Min termine corrente: series                         nr. postings: 1\n",
      "\n",
      "Min termine corrente: shame                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: shoe                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: short                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: shouldn't                      nr. postings: 1\n",
      "\n",
      "Min termine corrente: sight                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: silence                        nr. postings: 1\n",
      "\n",
      "Min termine corrente: silver                         nr. postings: 1\n",
      "\n",
      "Min termine corrente: smoke                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: sow                            nr. postings: 1\n",
      "\n",
      "Min termine corrente: speak                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: spiderman                      nr. postings: 1\n",
      "\n",
      "Min termine corrente: stones                         nr. postings: 1\n",
      "\n",
      "Min termine corrente: storm                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: story                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: straw                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: sunny                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: superhero                      nr. postings: 1\n",
      "\n",
      "Min termine corrente: sword                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: table                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: than                           nr. postings: 5\n",
      "\n",
      "Min termine corrente: that                           nr. postings: 2\n",
      "\n",
      "Min termine corrente: the                            nr. postings: 21\n",
      "\n",
      "Min termine corrente: then                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: there's                        nr. postings: 2\n",
      "\n",
      "Min termine corrente: they                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: think                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: thoughts                       nr. postings: 1\n",
      "\n",
      "Min termine corrente: throw                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: to                             nr. postings: 2\n",
      "\n",
      "Min termine corrente: together                       nr. postings: 1\n",
      "\n",
      "Min termine corrente: too                            nr. postings: 1\n",
      "\n",
      "Min termine corrente: tree                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: twice                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: two                            nr. postings: 2\n",
      "\n",
      "Min termine corrente: universe                       nr. postings: 1\n",
      "\n",
      "Min termine corrente: up                             nr. postings: 1\n",
      "\n",
      "Min termine corrente: very                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: wasn't                         nr. postings: 1\n",
      "\n",
      "Min termine corrente: watched                        nr. postings: 1\n",
      "\n",
      "Min termine corrente: wear                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: weep                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: what                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: when                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: who                            nr. postings: 1\n",
      "\n",
      "Min termine corrente: with                           nr. postings: 2\n",
      "\n",
      "Min termine corrente: without                        nr. postings: 1\n",
      "\n",
      "Min termine corrente: words                          nr. postings: 1\n",
      "\n",
      "Min termine corrente: world                          nr. postings: 2\n",
      "\n",
      "Min termine corrente: worm                           nr. postings: 1\n",
      "\n",
      "Min termine corrente: worst                          nr. postings: 2\n",
      "\n",
      "Min termine corrente: wrongs                         nr. postings: 1\n",
      "\n",
      "Min termine corrente: you                            nr. postings: 5\n",
      "\n",
      "Min termine corrente: your                           nr. postings: 4\n",
      "END METHOD!\n"
     ]
    }
   ],
   "source": [
    "indexBuilder=IndexBuilder(True,False)\n",
    "#invIndex=indexBuilder.build_in_memory_index(tot_doc)\n",
    "indexBuilder.single_pass_in_memory_indexing(tot_doc,2220,2220)\n",
    "indexBuilder.index_merging()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4b36108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.85"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexBuilder.coll_statistics.get_average_Document_Length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7986527d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexBuilder.coll_statistics.sum_document_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bb53982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a05ea09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
