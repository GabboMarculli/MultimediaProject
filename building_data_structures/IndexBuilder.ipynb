{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b74a3f4f-5831-497c-9632-0129b8b291ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\MultimediaProject\\building_data_structures\\..\\building_data_structures\\Lexicon.ipynb\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import shutil\n",
    "import os\n",
    "import operator\n",
    "\n",
    "import time\n",
    "\n",
    "from typing import List, Dict, Union, Any, Callable\n",
    "from collections import Counter, defaultdict,OrderedDict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import import_ipynb\n",
    "sys.path.append('../')  # Go up two folders to the project root\n",
    "import building_data_structures.Lexicon as Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e383ee0d-12b4-4e72-81df-dfd3cbd6e522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this is a simple data class, intializing it can be abstracted with\n",
    "# the use of dataclass decorator.\n",
    "# https://docs.python.org/3/library/dataclasses.html\n",
    "\n",
    "@dataclass\n",
    "class Posting:\n",
    "    doc_id: int\n",
    "    payload: Any = None\n",
    "    \n",
    "    @classmethod \n",
    "    def from_string(cls, description:str):\n",
    "        docId,payl=description.split(\":\")\n",
    "        doc_id=int(docId)\n",
    "        payload=int(payl)\n",
    "        return cls(doc_id, payload)\n",
    "               \n",
    "class InvertedIndex:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._index = defaultdict(list)\n",
    "        \n",
    "    def add_posting(self, term: str, doc_id: int, payload: Any=None) -> None:\n",
    "        \"\"\"Adds a document to the posting list of a term.\"\"\"\n",
    "        # append new posting to the posting list\n",
    "        if (self.get_postings(term)==None):\n",
    "            self._index[term]=[]\n",
    "        self._index[term].append(Posting(doc_id,payload))\n",
    "             \n",
    "    def get_postings(self, term: str) -> List[Posting]:\n",
    "        \"\"\"Fetches the posting list for a given term.\"\"\"\n",
    "        if (term in self._index):\n",
    "            return self._index[term]\n",
    "        return None\n",
    "    \n",
    "    def write_to_block(self,file_name_index: str) -> None:\n",
    "        \"\"\" Write the inverted index in lexicographical oreder into a file_name_index on disk.\"\"\"\n",
    "        sorted_lexicon=sorted(self._index.items())\n",
    "        with open(file_name_index, \"w\") as f:\n",
    "            for term,postings in sorted_lexicon:\n",
    "                f.write(term)\n",
    "                for posting in postings:\n",
    "                    f.write(f\" {posting.doc_id}\")\n",
    "                    if posting.payload:\n",
    "                        f.write(f\":{str(posting.payload)}\")\n",
    "                f.write(\"\\n\")\n",
    "    \n",
    "    def is_empty(self)->bool:\n",
    "        \"\"\"Check if there is no term in the inverted index.\"\"\"\n",
    "        return len(self.get_terms())==0\n",
    "    \n",
    "    def get_terms(self) -> List[str]:\n",
    "        \"\"\"Returns all unique terms in the index.\"\"\"\n",
    "        return self._index.keys() \n",
    "    \n",
    "    def clear_structure(self):\n",
    "        \"\"\" It clears the inverted index data structure.\"\"\"\n",
    "        self._index.clear()\n",
    "    \n",
    "    def get_structure(self):\n",
    "        \"\"\"Returns the inverted index data structure.\"\"\"\n",
    "        return self._index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfb4c610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexBuilder:\n",
    "    \n",
    "    MAX_CHARATER=chr(1114111)\n",
    "    \n",
    "    OUTPUT_FILE_FORMAT=\".txt\"\n",
    "    NAME_DOC_IDS_FILE=\"docIds\"\n",
    "    NAME_TERM_FREQ_FILE=\"termFreq\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print (\"Index Builder costructor\")\n",
    "        #self._invertedIndex = InvertedIndex()\n",
    "\n",
    "    def build_in_memory_index(self,list_of_documents:list)->InvertedIndex:\n",
    "        \"\"\"Given a list of document, build an Inverted Index in main Memory (RAM) and return it.\"\"\"\n",
    "        invertedIndex = InvertedIndex()\n",
    "        for doc in list_of_documents:\n",
    "            doc_id=list_of_documents.index(doc)\n",
    "            tc = Counter(doc.lower().split())  # dict with term counts, QUI USARE DIRETTAMENTE IL CONTENUTO GIA' PRE-PROCESSATO\n",
    "            for term, freq in tc.items():\n",
    "                invertedIndex.add_posting(term, doc_id, freq)\n",
    "        return invertedIndex\n",
    "\n",
    "\n",
    "    def build_block_sort_base_indexing(self,list_of_documents:list,output_file_name:str,block_size: int=2200,split_results_in_files:bool=False, delete_intermediete_results:bool=True)-> None:\n",
    "        \"\"\" Given a list of document, build an Inverted Index exploiting both Memory(RAM) at blocks and Disk. \n",
    "            It saves the entire structure on disk at location output_file_name.\n",
    "            The file structure is like term doc_id:term_freq\n",
    "               \n",
    "         Args:\n",
    "            list_of_documents: List of document to be processed.\n",
    "            output_file_name: The location of where the structure is saved.\n",
    "            block_size: The size of the block to elaborate in main memory and store on disk as intermediate result.\n",
    "            split_results_in_files: Specify if you want or not the inderted index in two different files: the doc_ids file and the term_freq file\n",
    "            delete_intermediete_results: Flag to remove partial results at the end of the procedure or not.\n",
    "         \n",
    "        \"\"\"\n",
    "\n",
    "        ind = InvertedIndex()\n",
    "        document_index = Lexicon.DocumentIndex()\n",
    "        nr_block=0\n",
    "\n",
    "        DIR_FOLDER=\"TEMP\"\n",
    "\n",
    "        if os.path.exists(DIR_FOLDER):\n",
    "            shutil.rmtree(DIR_FOLDER)\n",
    "\n",
    "        os.makedirs(DIR_FOLDER)\n",
    "\n",
    "    \n",
    "        if (split_results_in_files):\n",
    "            if os.path.exists(output_file_name+\"_\"+self.NAME_DOC_IDS_FILE+self.OUTPUT_FILE_FORMAT):\n",
    "                os.remove(output_file_name+\"_\"+self.NAME_DOC_IDS_FILE+self.OUTPUT_FILE_FORMAT)\n",
    "            \n",
    "            if os.path.exists(output_file_name+\"_\"+self.NAME_TERM_FREQ_FILE+self.OUTPUT_FILE_FORMAT):\n",
    "                os.remove(output_file_name+\"_\"+self.NAME_TERM_FREQ_FILE+self.OUTPUT_FILE_FORMAT)\n",
    "        else:\n",
    "            if os.path.exists(output_file_name+self.OUTPUT_FILE_FORMAT):\n",
    "                os.remove(output_file_name+self.OUTPUT_FILE_FORMAT)\n",
    "    \n",
    "        #Map phase - read all the documents and write the index at blocks on disk when memory is full, cleaning the memory data structure.\n",
    "\n",
    "        Lexicon.create_folder(\"Document_index\")\n",
    "        \n",
    "        for doc in list_of_documents:\n",
    "            # Divido il doc_id dal contenuto del documento vero e proprio\n",
    "            doc_list = doc.split()\n",
    "            doc_id = int(doc_list[0])\n",
    "            text = ' '.join(doc_list[1:])\n",
    "\n",
    "            if (sys.getsizeof(document_index.get_structure()) > 2200):  \n",
    "                Lexicon.write_to_block(\"Document_index/document_index.txt\", document_index.get_structure())\n",
    "                document_index.clear_structure()\n",
    "\n",
    "            document_index.add_document(doc_id, text)\n",
    "\n",
    "            tc = Counter(text.lower().split())  # dict with term counts, QUI USARE DIRETTAMENTE IL CONTENUTO GIA' PRE-PROCESSATO\n",
    "            for term, freq in tc.items():\n",
    "                if (sys.getsizeof(ind.get_structure()) > block_size):  #Free memory available\n",
    "                    ind.write_to_block(DIR_FOLDER+\"/inv_index_\"+str(nr_block)+\".txt\")\n",
    "                    ind.clear_structure()\n",
    "                    nr_block=nr_block+1 \n",
    "                ind.add_posting(term, doc_id, freq)\n",
    "            \n",
    "        if (not document_index.is_empty()):   \n",
    "            Lexicon.write_to_block(\"Document_index/document_index.txt\", document_index.get_structure())\n",
    "                \n",
    "        #Finally, saving the last remaing block.       \n",
    "        if (not ind.is_empty()):   \n",
    "            ind.write_to_block(DIR_FOLDER+\"/inv_index_\"+str(nr_block)+\".txt\")\n",
    "\n",
    "\n",
    "        # ----  Second part ----\n",
    "\n",
    "        #Reduce phase --> merging all the blocks (multi-way merge sort) in one unique result and save it on disk. \n",
    "                       \n",
    "        try:\n",
    "            file_paths = [DIR_FOLDER+\"/\"+f for f in os.listdir(DIR_FOLDER)] \n",
    "            input_files = [open(file, 'r') for file in file_paths]  #Open all the blocks in parallel\n",
    "            lines=[file.readline().strip() for file in input_files] #Read the first line of each block\n",
    "\n",
    "            while (not self.__check_all_blocks_are_read(lines)):\n",
    "\n",
    "                terms=[line.split()[0] if line else self.MAX_CHARATER for line in lines] #Contains a list of strings: the terms present at each line read in the blocks, if file is over put the max-unicode charater\n",
    "                postings=[\" \".join(line.split()[1:]) for line in lines] #Contains a list of strings: the postings present at each line read in the blocks.\n",
    "\n",
    "                min_term=min(terms)  #Enstablishing what is the term to be considered: the term lexicografical ordered.\n",
    "\n",
    "                #Merging the postings with the same min term\n",
    "                mergePostings=[]\n",
    "                for i in range (0,len(postings)):\n",
    "                    if (min_term==terms[i]):  \n",
    "                        self.__decode_string_posting_list_and_merge_to_current(mergePostings,postings[i].split())\n",
    "\n",
    "                #Sorting by doc_id   \n",
    "                mergePostings=sorted(mergePostings, key=operator.attrgetter('doc_id'))    \n",
    "                self.__write_term_posting_list_to_disk(output_file_name,min_term,mergePostings,split_results_in_files)\n",
    "\n",
    "                #Advance on reading the files in parallel only for blocks where was present the last min term.\n",
    "                lines=[input_files[i].readline().strip() if (min_term==terms[i]) else lines[i] for i in range (0,len(terms))] \n",
    "        except Exception as e:   \n",
    "            raise e\n",
    "        finally:\n",
    "            #Be sure to close all the opened files in parallel\n",
    "            for file in input_files:\n",
    "                file.close()  \n",
    "\n",
    "        if (delete_intermediete_results):\n",
    "            shutil.rmtree(DIR_FOLDER)\n",
    "\n",
    "        Lexicon.create_lexicon(\"complete_inverted_index.txt\", \"lexicon\", \"Lexicon/\", \".txt\", 2200, document_index)   \n",
    "\n",
    "    \n",
    "    #Private methods: all utilities used inside main methods to make code more readable.\n",
    "    \n",
    "    def __check_all_blocks_are_read(self,lines:list)->bool:\n",
    "        \"\"\" This method is used to check whether all files opened in parallel have been completely read or not\n",
    "            It checks the contents that have been read to determine the condition.\n",
    "            \n",
    "            Args:\n",
    "                 lines: A list of lines(str) read before.\n",
    "        \"\"\"\n",
    "        for line in lines:\n",
    "            if line:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def __decode_string_posting_list_and_merge_to_current(self,current_postings_list:list, new_string_posting_list:str):\n",
    "        \"\"\" This method is used for adding/merging a posting list in string format to the list of current_postings. \n",
    "            This method check if current_postings already contains a posting with a doc_id, in that case the term_freq is sum\n",
    "            otherwise the posting is append to the current_postings.\n",
    "            \n",
    "            Args:\n",
    "                current_postings_list: the actual posting list\n",
    "                new_string_posting_list: the new posting list to be merged in string format ex. doc_id_1:term_freq_1 doc_id_2:term_freq_2  \n",
    "        \"\"\"\n",
    "        current_docIds=[] #Used to store and retrieve rapidly the actual doc_id in the current_postings_list\n",
    "        if (len(current_postings_list)>0):\n",
    "            current_docIds=[curr_posting.doc_id for curr_posting in current_postings_list]\n",
    "        \n",
    "        for posting_str in new_string_posting_list: #posting_str is a single posting in the form \"docId:freq\"\n",
    "            posting=Posting.from_string(posting_str)\n",
    "            if (posting.doc_id in current_docIds): \n",
    "                current_postings_list[current_docIds.index(posting.doc_id)]+=posting.payload \n",
    "            else:\n",
    "                current_postings_list.append(posting)\n",
    "        \n",
    "    def __write_term_posting_list_to_disk(self,file_name:str,term:str,merged_postings_list:list,split_results_in_files:bool):\n",
    "        \"\"\" This method is used to write in a file on disk in append mode a full entry of the lexicon in the format ex.\n",
    "            term doc_id_1:term_freq_1 doc_id_2:term_freq_2 doc_id_3:term_freq_3 ... \n",
    "            \n",
    "            Args:\n",
    "                file_name: the name of the output file \n",
    "                term: the lexicon term\n",
    "                merged_postings_list: the full merged posting list\n",
    "                split_results_in_files: Specify if you want or not the inderted index in two different files: the doc_ids file and the term_freq file\n",
    "        \"\"\"\n",
    "       \n",
    "        \n",
    "        if(not split_results_in_files):\n",
    "            with open(file_name+self.OUTPUT_FILE_FORMAT, \"a\") as f:\n",
    "                f.write(term)\n",
    "                for posting in merged_postings_list:\n",
    "                    f.write(f\" {posting.doc_id}\")\n",
    "                    if posting.payload:\n",
    "                        f.write(f\":{str(posting.payload)}\")\n",
    "                f.write(\"\\n\")\n",
    "        else:\n",
    "            \n",
    "             with open(file_name+\"_\"+self.NAME_DOC_IDS_FILE+self.OUTPUT_FILE_FORMAT, \"a\") as f:\n",
    "                for index, posting in enumerate(merged_postings_list):\n",
    "                    f.write(str(posting.doc_id))\n",
    "                    if index != len(merged_postings_list) - 1:\n",
    "                        f.write(\",\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "             with open(file_name+\"_\"+self.NAME_TERM_FREQ_FILE+self.OUTPUT_FILE_FORMAT, \"a\") as f:\n",
    "                for index, posting in enumerate(merged_postings_list):\n",
    "                    f.write(str(posting.payload))\n",
    "                    if index != len(merged_postings_list) - 1:\n",
    "                        f.write(\",\")\n",
    "                f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97f44be",
   "metadata": {},
   "source": [
    "# Example of usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13c24246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tot_doc=[\n",
    "#             \"The pen is on the table\",\n",
    "#             \"The day is very sunny\",\n",
    "#             \"Goodmoring new article\",\n",
    "#             \"A cat is faster then a dog\",\n",
    "#             \"How are you\",\n",
    "#             \"A boy is a man with low age\",\n",
    "#             \"Lake Ontario is one of the biggest lake in the world\",\n",
    "#             \"English is worst than Italian\",\n",
    "#             \"Spiderman is the best superhero in Marvel universe\",\n",
    "#             \"Last night I saw a Netflix series\",\n",
    "#             \"A penny for your thoughts\",\n",
    "#             \"Actions speak louder than words\",\n",
    "#             \"All that glitters is not gold\",\n",
    "#             \"Beauty is in the eye of the beholder\",\n",
    "#             \"Birds of a feather flock together\",\n",
    "#             \"Cleanliness is next to godliness\",\n",
    "#             \"Don't count your chickens before they hatch\",\n",
    "#             \"Every people cloud has a silver lining people\",\n",
    "#             \"Fool me once shame on you fool me twice shame on me\",\n",
    "#             \"Honesty is the best policy.\",\n",
    "#             \"If the shoe fits, wear it\",\n",
    "#             \"It's a piece of cake\",\n",
    "#             \"Jump on the bandwagon\",\n",
    "#             \"Keep your chin up\",\n",
    "#             \"Let the cat out of the bag\",\n",
    "#             \"Make a long story short\",\n",
    "#             \"Necessity is the mother of invention\",\n",
    "#             \"Once in a blue moon\",\n",
    "#             \"Practice makes perfect\",\n",
    "#             \"Read between the lines\",\n",
    "#             \"The early bird catches people the worm\",\n",
    "#             \"The pen is mightier than the sword\",\n",
    "#             \"There's no smoke without fire\",\n",
    "#             \"To each his own\",\n",
    "#             \"Two heads are better than one\",\n",
    "#             \"You can't have your cake and eat it too\",\n",
    "#             \"A watched pot never boils\",\n",
    "#             \"Beggars can't be choosers\",\n",
    "#             \"Better late than never\",\n",
    "#             \"Calm before the storm\",\n",
    "#             \"Curiosity killed the cat\",\n",
    "#             \"Every dog has its day\",\n",
    "#             \"Great minds think alike\",\n",
    "#             \"Hope for the best prepare for the worst\",\n",
    "#             \"Ignorance is bliss.\",\n",
    "#             \"It's the last straw that breaks the camel's back\",\n",
    "#             \"Laugh and the world laughs with you weep and you weep alone\",\n",
    "#             \"Money can't buy happiness\",\n",
    "#             \"No news is good news\",\n",
    "#             \"Out of sight out of mind\",\n",
    "#             \"People who live in glass houses shouldn't throw stones\",\n",
    "#             \"Rome wasn't built in a day\",\n",
    "#             \"Silence is golden\",\n",
    "#             \"The apple doesn't fall far from the tree\",\n",
    "#             \"The more, the merrier\",\n",
    "#             \"There's no place like home\",\n",
    "#             \"Two wrongs don't make a right\",\n",
    "#             \"When in Rome do as the Romans do\",\n",
    "#             \"You reap what you sow\",\n",
    "#             \"People people people\"]\n",
    "\n",
    "\n",
    "# indexBuilder=IndexBuilder()\n",
    "# #ii=indexBuilder.build_in_memory_index(tot_doc)\n",
    "# indexBuilder.build_block_sort_base_indexing(tot_doc,\"complete_inverted_index\",2220,False,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "561ce13e-bc7a-4357-bb3b-9102802a83ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Builder costructor\n"
     ]
    }
   ],
   "source": [
    "tot_doc=[\n",
    "    \"0     The pen is on the table\",\n",
    "    \"1     The day is very sunny\",\n",
    "    \"2     Goodmoring new article\",\n",
    "    \"3     A cat is faster then a dog\",\n",
    "    \"4     How are you\",\n",
    "    \"5     A boy is a man with low age\",\n",
    "    \"6     Lake Ontario is one of the biggest lake in the world\",\n",
    "    \"7     English is worst than Italian\",\n",
    "    \"8     Spiderman is the best superhero in Marvel universe\",\n",
    "    \"9     Last night I saw a Netflix series\",\n",
    "    \"10    A penny for your thoughts\",\n",
    "    \"11    Actions speak louder than words\",\n",
    "    \"12    All that glitters is not gold\",\n",
    "    \"13    Beauty is in the eye of the beholder\",\n",
    "    \"14    Birds of a feather flock together\",\n",
    "    \"15    Cleanliness is next to godliness\",\n",
    "    \"16    Don't count your chickens before they hatch\",\n",
    "    \"17    Every people cloud has a silver lining people\",\n",
    "    \"18    Fool me once shame on you fool me twice shame on me\",\n",
    "    \"19    Honesty is the best policy.\",\n",
    "    \"20    If the shoe fits, wear it\",\n",
    "    \"21    It's a piece of cake\",\n",
    "    \"22    Jump on the bandwagon\",\n",
    "    \"23    Keep your chin up\",\n",
    "    \"24    Let the cat out of the bag\",\n",
    "    \"25    Make a long story short\",\n",
    "    \"26    Necessity is the mother of invention\",\n",
    "    \"27    Once in a blue moon\",\n",
    "    \"28    Practice makes perfect\",\n",
    "    \"29    Read between the lines\",\n",
    "    \"30    The early bird catches people the worm\",\n",
    "    \"31    The pen is mightier than the sword\",\n",
    "    \"32    There's no smoke without fire\",\n",
    "    \"33    To each his own\",\n",
    "    \"34    Two heads are better than one\",\n",
    "    \"35    You can't have your cake and eat it too\",\n",
    "    \"36    A watched pot never boils\",\n",
    "    \"37    Beggars can't be choosers\",\n",
    "    \"38    Better late than never\",\n",
    "    \"39    Calm before the storm\",\n",
    "    \"40    Curiosity killed the cat\",\n",
    "    \"41    Every dog has its day\",\n",
    "    \"42    Great minds think alike\",\n",
    "    \"43    Hope for the best prepare for the worst\",\n",
    "    \"44    Ignorance is bliss.\",\n",
    "    \"45    It's the last straw that breaks the camel's back\",\n",
    "    \"46    Laugh and the world laughs with you weep and you weep alone\",\n",
    "    \"47    Money can't buy happiness\",\n",
    "    \"48    No news is good news\",\n",
    "    \"49    Out of sight out of mind\",\n",
    "    \"50    People who live in glass houses shouldn't throw stones\",\n",
    "    \"51    Rome wasn't built in a day\",\n",
    "    \"52    Silence is golden\",\n",
    "    \"53    The apple doesn't fall far from the tree\",\n",
    "    \"54    The more, the merrier\",\n",
    "    \"55    There's no place like home\",\n",
    "    \"56    Two wrongs don't make a right\",\n",
    "    \"57    When in Rome do as the Romans do\",\n",
    "    \"58    You reap what you sow\",\n",
    "    \"59    People people people\"\n",
    "]\n",
    "\n",
    "\n",
    "indexBuilder=IndexBuilder()\n",
    "indexBuilder.build_block_sort_base_indexing(tot_doc,\"complete_inverted_index\",2220,False,True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
