{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b74a3f4f-5831-497c-9632-0129b8b291ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\building_data_structures\\..\\structures\\InvertedIndex.ipynb\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'building_data_structures.Lexicon'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20596\\3553906488.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Go up two folders to the project root\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mstructures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvertedIndex\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPosting\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mInvertedIndex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstructures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLexicon\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLexicon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLexiconRow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDocumentIndex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\import_ipynb.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(self, fullname)\u001b[0m\n\u001b[0;32m     59\u001b[0m                 \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_transformer_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                 \u001b[1;31m# run the code in themodule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                 \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_ns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msave_user_ns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\IR\\Progetto\\building_data_structures\\..\\structures\\InvertedIndex.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'building_data_structures.Lexicon'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "from typing import List, Dict, Tuple, Union, Any, Callable\n",
    "from typing import TextIO, BinaryIO\n",
    "from collections import Counter, defaultdict,OrderedDict\n",
    "\n",
    "\n",
    "import import_ipynb\n",
    "sys.path.append('../')  # Go up two folders to the project root\n",
    "\n",
    "from structures.InvertedIndex import Posting,InvertedIndex\n",
    "from structures.Lexicon import Lexicon,LexiconRow,DocumentIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bbc5be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Costants\n",
    "TYPE_DOC_ID=\"type_doc_id\"\n",
    "TYPE_FREQ=\"type_freq\"\n",
    "DIR_TEMP_FOLDER=\"TEMP\"\n",
    "DIR_TEMP_DOC_ID=\"DOC_ID_TEMP\"\n",
    "DIR_TEMP_FREQ=\"FREQ_TEMP\"\n",
    "DIR_TEMP_LEXICON=\"LEXICON_TEMP\"\n",
    "\n",
    "DIR_LEXICON=\"LEXICON\"\n",
    "DIR_DOC_INDEX=\"DOCUMENT_INDEX\"\n",
    "DIR_INVERTED_INDEX=\"INV_INDEX\"\n",
    "\n",
    "DIR_DOC_INDEX=\"Document_index\"\n",
    "\n",
    "PATH_FINAL_LEXICON=\"lexicon.bin\"\n",
    "PATH_FINAL_DOC_IDS=\"doc_ids.bin\"\n",
    "PATH_FINAL_FREQ=\"freq.bin\"\n",
    "\n",
    "PATH_FINAL_INVERTED_INDEX_DEBUG=\"inverted_index.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "295cbd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexBuilder:\n",
    "\n",
    "    #For writing the final result\n",
    "    file_Final_Lexicon:BinaryIO\n",
    "    file_Final_DocIds:BinaryIO\n",
    "    file_Final_Freq:BinaryIO\n",
    "    file_Final_InvertedIndex_Debug:TextIO\n",
    "        \n",
    "    #For merging operation\n",
    "    input_lex_temp_files:List[BinaryIO]\n",
    "    input_doc_id_temp_files:List[BinaryIO]\n",
    "    input_freq_temp_files:List[BinaryIO]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __init__(self):\n",
    "        print (\"Index Builder costructor\")\n",
    "        \n",
    "        \n",
    "    def build_in_memory_index(self,list_of_documents:list)->InvertedIndex:\n",
    "        \"\"\"Given a list of document, build an Inverted Index in main Memory (RAM) and return it.\n",
    "           !! THIS METHOD IS NOT USED TO BUILD THE EFFECTIVE INDEX !!\n",
    "           \n",
    "           This is used in test phase to check rapidly if the output obtained is correct or not.\n",
    "        Args:\n",
    "            list_of_documents: list of strings representing a document\n",
    "           \n",
    "        \"\"\"\n",
    "        invertedIndex = InvertedIndex()\n",
    "        for doc in list_of_documents:\n",
    "            doc_list = doc.split()\n",
    "            doc_id = int(doc_list[0])\n",
    "            text = ' '.join(doc_list[1:])\n",
    "            tc = Counter(text.lower().split())  # dict with term counts, QUI USARE DIRETTAMENTE IL CONTENUTO GIA' PRE-PROCESSATO\n",
    "            for term, freq in tc.items():\n",
    "                invertedIndex.add_posting(term, doc_id, freq)\n",
    "        return invertedIndex\n",
    "        \n",
    "    \n",
    "    def init_spimi(self):\n",
    "        \"\"\" Function to initialize a clear environment to start building the needed data structures for the spimi phase.\"\"\"\n",
    "        \n",
    "        if os.path.exists(DIR_TEMP_FOLDER):\n",
    "            shutil.rmtree(DIR_TEMP_FOLDER)\n",
    "\n",
    "        os.makedirs(DIR_TEMP_FOLDER)\n",
    "        os.makedirs(DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_DOC_ID)\n",
    "        os.makedirs(DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_FREQ)\n",
    "        os.makedirs(DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_LEXICON)\n",
    "        \n",
    "        if os.path.exists(DIR_DOC_INDEX):\n",
    "            shutil.rmtree(DIR_DOC_INDEX)\n",
    "            \n",
    "        os.makedirs(DIR_DOC_INDEX)\n",
    "        \n",
    "        \n",
    "    def init_index_merging(self,debug_mode:bool):\n",
    "        \"\"\" Function to initialize a clear environment to start building the effective datastructures.\n",
    "        \n",
    "        Args:\n",
    "            debug_mode: if true, it prepares also the environment to a human readable inverted index file.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if os.path.exists(DIR_LEXICON):\n",
    "            shutil.rmtree(DIR_LEXICON)\n",
    "                \n",
    "        if os.path.exists(DIR_INVERTED_INDEX):\n",
    "            shutil.rmtree(DIR_INVERTED_INDEX)\n",
    "\n",
    "        os.makedirs(DIR_LEXICON)\n",
    "        os.makedirs(DIR_INVERTED_INDEX)\n",
    "               \n",
    "        if os.path.exists(PATH_FINAL_LEXICON):\n",
    "            os.remove(PATH_FINAL_LEXICON)\n",
    "            \n",
    "        if os.path.exists(PATH_FINAL_DOC_IDS):\n",
    "            os.remove(PATH_FINAL_DOC_IDS)\n",
    "                \n",
    "        if os.path.exists(PATH_FINAL_FREQ):\n",
    "            os.remove(PATH_FINAL_FREQ)\n",
    "            \n",
    "        if (debug_mode):\n",
    "            if os.path.exists(PATH_FINAL_INVERTED_INDEX_DEBUG):\n",
    "                os.remove(PATH_FINAL_INVERTED_INDEX_DEBUG)\n",
    "    \n",
    "    \n",
    "    def open_files_for_merging_operation(self,debug_mode:bool):\n",
    "        \n",
    "        file_lex_temp_paths = [DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_LEXICON+\"/\"+f for f in os.listdir(DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_LEXICON)]\n",
    "        file_doc_id_temp_paths = [DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_DOC_ID+\"/\"+f for f in os.listdir(DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_DOC_ID)] \n",
    "        file_freq_temp_paths = [DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_FREQ+\"/\"+f for f in os.listdir(DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_FREQ)] \n",
    "\n",
    "        self.input_lex_temp_files = [open(file, 'rb') for file in file_lex_temp_paths]  #Open all the blocks in parallel\n",
    "        self.input_doc_id_temp_files = [open(file, 'rb') for file in file_doc_id_temp_paths]  #Open all the blocks in parallel\n",
    "        self.input_freq_temp_files = [open(file, 'rb') for file in file_freq_temp_paths]  #Open all the blocks in parallel\n",
    "\n",
    "\n",
    "        self.file_Final_Lexicon=open(PATH_FINAL_LEXICON, 'ab') \n",
    "        self.file_Final_DocIds=open(PATH_FINAL_DOC_IDS, 'ab') \n",
    "        self.file_Final_Freq=open(PATH_FINAL_FREQ, 'ab') \n",
    "\n",
    "        if (debug_mode):\n",
    "            self.file_Final_InvertedIndex_Debug=open(PATH_FINAL_INVERTED_INDEX_DEBUG,'a')\n",
    "        \n",
    "    def close_file_for_merging_operation(self,debug_mode:bool):\n",
    "        \n",
    "        for file in self.input_lex_temp_files:\n",
    "            file.close()  \n",
    "\n",
    "        for file in self.input_doc_id_temp_files:\n",
    "            file.close()  \n",
    "\n",
    "        for file in self.input_freq_temp_files:\n",
    "            file.close()  \n",
    "\n",
    "        self.file_Final_Lexicon.close()\n",
    "        self.file_Final_DocIds.close()\n",
    "        self.file_Final_Freq.close()\n",
    "\n",
    "        if (debug_mode):\n",
    "            self.file_Final_InvertedIndex_Debug.close()\n",
    "        \n",
    "    def check_all_blocks_are_read(self,offset_lexicon_terms:List):\n",
    "        \"\"\" This functions checks if the all the blocks opened in parallel are read or not.\n",
    "            The condition is matched when the list contains all None elements. \n",
    "        Args:\n",
    "            offset_lexicon_terms: a list of offset\n",
    "        \n",
    "        Returns:\n",
    "            True if the list contains all None elements.\n",
    "        \"\"\"\n",
    "        #print (offset_lexicon_terms)\n",
    "        return sum(1 if element is None else 0 for element in offset_lexicon_terms) == len(offset_lexicon_terms)\n",
    "    \n",
    "    def find_min_term(self,lexicon_temp_terms:List,offset_lex_temp:List):\n",
    "        \"\"\" This function checks and returns the minimum term (lexicographically) among blocks \n",
    "             at the current reading offset.\n",
    "             If a offset_lex_temp[i] contains None means that the i block is completely read.\n",
    "             \n",
    "         Args:\n",
    "             lexicon_temp_terms: the list of current lexicon element (each position is a different block)\n",
    "             offset_lexicon_terms: the list of current lexicon element position inside the file (each position is a different block)\n",
    "         Return:\n",
    "             a string representing the current min term or None if all blocks are read\n",
    "         \n",
    "        \"\"\"\n",
    "    \n",
    "        if not lexicon_temp_terms:\n",
    "            return None  # Return None for an empty list\n",
    "    \n",
    "        min_term=None\n",
    "\n",
    "        for index,lex_elem in enumerate(lexicon_temp_terms):\n",
    "            if(offset_lex_temp[index]!=None):\n",
    "                if (min_term==None):\n",
    "                    min_term=lexicon_temp_terms[index].term\n",
    "\n",
    "                if (lex_elem.term<min_term):\n",
    "                    min_term=lex_elem.term\n",
    "    \n",
    "        return min_term\n",
    "    \n",
    "    \n",
    "    def single_pass_in_memory_indexing(self,list_of_documents:list,inv_index_block_size: int=2200,doc_index_block_size: int=2200,debug_mode:bool=False)-> None:\n",
    "\n",
    "            ind = InvertedIndex()\n",
    "            document_index = DocumentIndex()\n",
    "\n",
    "            nr_block=0\n",
    "\n",
    "            self.init_spimi()\n",
    "            \n",
    "            #Read all the documents and write the index at blocks on disk when memory is full, cleaning the memory data structure.\n",
    "            \n",
    "            for doc in list_of_documents:\n",
    "                # Separate the doc_id from the content of the real document \n",
    "                doc_list = doc.split()\n",
    "                doc_id = int(doc_list[0])\n",
    "                text = ' '.join(doc_list[1:])\n",
    "\n",
    "                if (sys.getsizeof(document_index.get_structure()) > doc_index_block_size):\n",
    "                    if (debug_mode):\n",
    "                        #Lexicon.write_to_block(DIR_DOC_INDEX+\"/document_index.txt\", document_index.get_structure())\n",
    "                        document_index.write_document_index_to_file(DIR_DOC_INDEX+\"/document_index.txt\", document_index.get_structure())\n",
    "                    document_index.clear_structure()\n",
    "\n",
    "                document_index.add_document(doc_id, text)\n",
    "\n",
    "                tc = Counter(text.lower().split())  # dict with term counts, Here there is the already preprocessed content\n",
    "                for term, freq in tc.items():\n",
    "                    if (sys.getsizeof(ind.get_structure()) > inv_index_block_size):  #Free memory available\n",
    "\n",
    "                        LEXICON_TEMP_BLOCK_PATH=DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_LEXICON+\"/block_nr_\"+str(nr_block)\n",
    "                        DOC_IDS_TEMP_BLOCK_PATH=DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_DOC_ID+\"/block_nr_\"+str(nr_block)\n",
    "                        FREQ_TEMP_BLOCK_PATH=DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_FREQ+\"/block_nr_\"+str(nr_block)\n",
    "\n",
    "                        ind.write_to_block_all_index_in_memory(LEXICON_TEMP_BLOCK_PATH,DOC_IDS_TEMP_BLOCK_PATH,FREQ_TEMP_BLOCK_PATH)\n",
    "\n",
    "                        if (debug_mode):\n",
    "                            ind.write_to_block_debug_mode(DIR_TEMP_FOLDER+\"/inv_index_\"+str(nr_block)+\".txt\")\n",
    "                        ind.clear_structure()\n",
    "                        nr_block=nr_block+1 \n",
    "\n",
    "                    ind.add_posting(term, doc_id, freq)\n",
    "\n",
    "            if (not document_index.is_empty()):   \n",
    "                if (debug_mode):\n",
    "                    #Lexicon.write_to_block(DIR_DOC_INDEX+\"/document_index.txt\", document_index.get_structure())\n",
    "                    document_index.write_document_index_to_file(DIR_DOC_INDEX+\"/document_index.txt\", document_index.get_structure())\n",
    "\n",
    "            #Finally, saving the last remaing block.       \n",
    "            if (not ind.is_empty()):\n",
    "                LEXICON_TEMP_BLOCK_PATH=DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_LEXICON+\"/block_nr_\"+str(nr_block)\n",
    "                DOC_IDS_TEMP_BLOCK_PATH=DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_DOC_ID+\"/block_nr_\"+str(nr_block)\n",
    "                FREQ_TEMP_BLOCK_PATH=DIR_TEMP_FOLDER+\"/\"+DIR_TEMP_FREQ+\"/block_nr_\"+str(nr_block)\n",
    "\n",
    "                ind.write_to_block_all_index_in_memory(LEXICON_TEMP_BLOCK_PATH,DOC_IDS_TEMP_BLOCK_PATH,FREQ_TEMP_BLOCK_PATH)\n",
    "\n",
    "                if (debug_mode):\n",
    "                    ind.write_to_block_debug_mode(DIR_TEMP_FOLDER+\"/inv_index_\"+str(nr_block)+\".txt\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    def index_merging(self,debug_mode:bool=False,compression_mode:bool=False)-> None:\n",
    "\n",
    "        self.init_index_merging(debug_mode)\n",
    "\n",
    "        try:\n",
    "\n",
    "            self.open_files_for_merging_operation(debug_mode)\n",
    "\n",
    "            #Initialization of empty lexicon row elements for each block.\n",
    "            lexicon_temp_elems=[LexiconRow(\"\",0) for i in range (len (self.input_lex_temp_files))]\n",
    "            \n",
    "            #Start reading the first element in the lexicon of each block and saving the offset of each read.\n",
    "            offset_lex_temp=[terms.read_lexicon_row_on_disk_from_opened_file(self.input_lex_temp_files[index],0) for index,terms in enumerate(lexicon_temp_elems)]\n",
    "            \n",
    "            #print(offset_lex_temp)\n",
    "\n",
    "            current_offset_lexicon=0\n",
    "            current_offset_doc_ids=0\n",
    "            current_offset_freq=0\n",
    "\n",
    "            while (not self.check_all_blocks_are_read(offset_lex_temp)):\n",
    "\n",
    "\n",
    "                min_term=self.find_min_term(lexicon_temp_elems,offset_lex_temp)\n",
    "                tot_posting=sum(lex_elem.dft if (lex_elem.term==min_term) else 0 for lex_elem in lexicon_temp_elems) \n",
    "\n",
    "                print(\"Min termine corrente: \"+min_term+ \" nr. postings: \"+str(tot_posting))\n",
    "\n",
    "                #New Term to add definitively\n",
    "                new_Lexicon_Def=LexiconRow(min_term,tot_posting)\n",
    "\n",
    "                new_Lexicon_Def.docidOffset=current_offset_doc_ids\n",
    "                new_Lexicon_Def.frequencyOffset=current_offset_freq\n",
    "\n",
    "                #This variable is used to mark if it is the first time a new term is elaborated among blocks.\n",
    "                #Just to write correctly in the debug inverted index file.\n",
    "                new_term=True\n",
    "                \n",
    "                for index,lex_term in enumerate(lexicon_temp_elems):\n",
    "\n",
    "                    if (lex_term.term==min_term):\n",
    "                        print(\"index \"+str(index))\n",
    "\n",
    "                        postingList,_,_=InvertedIndex.read_from_files_a_posting_list(lex_term.dft,self.input_doc_id_temp_files[index],self.input_freq_temp_files[index],\n",
    "                                                                  lex_term.docidOffset,lex_term.frequencyOffset)\n",
    "\n",
    "                        #This part must be finished!!\n",
    "                        new_Lexicon_Def.docidSize+=len(postingList)\n",
    "                        new_Lexicon_Def.frequencySize+=len(postingList)\n",
    "                        new_Lexicon_Def.numBlocks=1    #Per ora\n",
    "                        new_Lexicon_Def.blockOffset=0  #Per ora\n",
    "                        \n",
    "                        #Fare tutti i calcoli sulle metriche varie\n",
    "                       \n",
    "                        \n",
    "                        #This part must be finished!!\n",
    "                        if (debug_mode):\n",
    "                            InvertedIndex.write_to_file_a_posting_list_debug_mode(self.file_Final_InvertedIndex_Debug,min_term, postingList, new_term)\n",
    "\n",
    "\n",
    "                        current_offset_doc_ids,current_offset_freq=InvertedIndex.write_to_files_a_posting_list(postingList,self.file_Final_DocIds,self.file_Final_Freq,current_offset_doc_ids,current_offset_freq)\n",
    "\n",
    "                        \n",
    "                        #Read the next lexicon term\n",
    "\n",
    "                        offset_lex_temp[index]=lex_term.read_lexicon_row_on_disk_from_opened_file(self.input_lex_temp_files[index],offset_lex_temp[index])\n",
    "                        \n",
    "                        new_term=False\n",
    "\n",
    "                # Prima di scrivere il lexiconRow definitivamente:\n",
    "                # Da considerare la questione descrittori di blocco.\n",
    "                # In questa parte qui si vanno a calcolare le definitive metriche per le query ed anche \n",
    "                # i descrittori di blocco per skipping e altro.        \n",
    "\n",
    "                current_offset_lexicon=new_Lexicon_Def.write_lexicon_row_on_disk_to_opened_file(self.file_Final_Lexicon,current_offset_lexicon)\n",
    "\n",
    "            print(\"END METHOD!\")   \n",
    "\n",
    "        except Exception as e:   \n",
    "                raise e\n",
    "        finally:\n",
    "                #Be sure to close all the opened files in parallel\n",
    "                self.close_file_for_merging_operation(debug_mode)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97f44be",
   "metadata": {},
   "source": [
    "# Example of usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "561ce13e-bc7a-4357-bb3b-9102802a83ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_doc=[\n",
    "    \"0     The pen is on the table\",\n",
    "    \"1     The day is very sunny\",\n",
    "    \"2     Goodmoring new article\",\n",
    "    \"3     A cat is faster then a dog\",\n",
    "    \"4     How are you\",\n",
    "    \"5     A boy is a man with low age\",\n",
    "    \"6     Lake Ontario is one of the biggest lake in the world\",\n",
    "    \"7     English is worst than Italian\",\n",
    "    \"8     Spiderman is the best superhero in Marvel universe\",\n",
    "    \"9     Last night I saw a Netflix series\",\n",
    "    \"10    A penny for your thoughts\",\n",
    "    \"11    Actions speak louder than words\",\n",
    "    \"12    All that glitters is not gold\",\n",
    "    \"13    Beauty is in the eye of the beholder\",\n",
    "    \"14    Birds of a feather flock together\",\n",
    "    \"15    Cleanliness is next to godliness\",\n",
    "    \"16    Don't count your chickens before they hatch\",\n",
    "    \"17    Every people cloud has a silver lining people\",\n",
    "    \"18    Fool me once shame on you fool me twice shame on me\",\n",
    "    \"19    Honesty is the best policy.\",\n",
    "    \"20    If the shoe fits, wear it\",\n",
    "    \"21    It's a piece of cake\",\n",
    "    \"22    Jump on the bandwagon\",\n",
    "    \"23    Keep your chin up\",\n",
    "    \"24    Let the cat out of the bag\",\n",
    "    \"25    Make a long story short\",\n",
    "    \"26    Necessity is the mother of invention\",\n",
    "    \"27    Once in a blue moon\",\n",
    "    \"28    Practice makes perfect\",\n",
    "    \"29    Read between the lines\",\n",
    "    \"30    The early bird catches people the worm\",\n",
    "    \"31    The pen is mightier than the sword\",\n",
    "    \"32    There's no smoke without fire\",\n",
    "    \"33    To each his own\",\n",
    "    \"34    Two heads are better than one\",\n",
    "    \"35    You can't have your cake and eat it too\",\n",
    "    \"36    A watched pot never boils\",\n",
    "    \"37    Beggars can't be choosers\",\n",
    "    \"38    Better late than never\",\n",
    "    \"39    Calm before the storm\",\n",
    "    \"40    Curiosity killed the cat\",\n",
    "    \"41    Every dog has its day\",\n",
    "    \"42    Great minds think alike\",\n",
    "    \"43    Hope for the best prepare for the worst\",\n",
    "    \"44    Ignorance is bliss.\",\n",
    "    \"45    It's the last straw that breaks the camel's back\",\n",
    "    \"46    Laugh and the world laughs with you weep and you weep alone\",\n",
    "    \"47    Money can't buy happiness\",\n",
    "    \"48    No news is good news\",\n",
    "    \"49    Out of sight out of mind\",\n",
    "    \"50    People who live in glass houses shouldn't throw stones\",\n",
    "    \"51    Rome wasn't built in a day\",\n",
    "    \"52    Silence is golden\",\n",
    "    \"53    The apple doesn't fall far from the tree\",\n",
    "    \"54    The more, the merrier\",\n",
    "    \"55    There's no place like home\",\n",
    "    \"56    Two wrongs don't make a right\",\n",
    "    \"57    When in Rome do as the Romans do\",\n",
    "    \"58    You reap what you sow\",\n",
    "    \"59    People people people\"\n",
    "]\n",
    "\n",
    "\n",
    "#indexBuilder=IndexBuilder()\n",
    "#indexBuilder.build_block_sort_base_indexing(tot_doc,\"complete_inverted_index\",2220,False,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1861f8b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Builder costructor\n",
      "Min termine corrente: a                              nr. postings: 12\n",
      "index 0\n",
      "index 1\n",
      "index 2\n",
      "index 3\n",
      "index 5\n",
      "index 6\n",
      "Min termine corrente: actions                        nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: age                            nr. postings: 1\n",
      "index 0\n",
      "Min termine corrente: alike                          nr. postings: 1\n",
      "index 4\n",
      "Min termine corrente: all                            nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: alone                          nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: and                            nr. postings: 2\n",
      "index 3\n",
      "index 4\n",
      "Min termine corrente: apple                          nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: are                            nr. postings: 2\n",
      "index 0\n",
      "index 3\n",
      "Min termine corrente: article                        nr. postings: 1\n",
      "index 0\n",
      "Min termine corrente: as                             nr. postings: 1\n",
      "index 6\n",
      "Min termine corrente: back                           nr. postings: 1\n",
      "index 4\n",
      "Min termine corrente: bag                            nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: bandwagon                      nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: be                             nr. postings: 1\n",
      "index 4\n",
      "Min termine corrente: beauty                         nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: before                         nr. postings: 2\n",
      "index 1\n",
      "index 4\n",
      "Min termine corrente: beggars                        nr. postings: 1\n",
      "index 4\n",
      "Min termine corrente: beholder                       nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: best                           nr. postings: 3\n",
      "index 0\n",
      "index 2\n",
      "index 4\n",
      "Min termine corrente: better                         nr. postings: 2\n",
      "index 3\n",
      "index 4\n",
      "Min termine corrente: between                        nr. postings: 1\n",
      "index 3\n",
      "Min termine corrente: biggest                        nr. postings: 1\n",
      "index 0\n",
      "Min termine corrente: bird                           nr. postings: 1\n",
      "index 3\n",
      "Min termine corrente: birds                          nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: bliss.                         nr. postings: 1\n",
      "index 4\n",
      "Min termine corrente: blue                           nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: boils                          nr. postings: 1\n",
      "index 4\n",
      "Min termine corrente: boy                            nr. postings: 1\n",
      "index 0\n",
      "Min termine corrente: breaks                         nr. postings: 1\n",
      "index 4\n",
      "Min termine corrente: built                          nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: buy                            nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: cake                           nr. postings: 2\n",
      "index 2\n",
      "index 3\n",
      "Min termine corrente: calm                           nr. postings: 1\n",
      "index 4\n",
      "Min termine corrente: camel's                        nr. postings: 1\n",
      "index 4\n",
      "Min termine corrente: can't                          nr. postings: 3\n",
      "index 3\n",
      "index 4\n",
      "index 5\n",
      "Min termine corrente: cat                            nr. postings: 3\n",
      "index 0\n",
      "index 2\n",
      "index 4\n",
      "Min termine corrente: catches                        nr. postings: 1\n",
      "index 3\n",
      "Min termine corrente: chickens                       nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: chin                           nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: choosers                       nr. postings: 1\n",
      "index 4\n",
      "Min termine corrente: cleanliness                    nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: cloud                          nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: count                          nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: curiosity                      nr. postings: 1\n",
      "index 4\n",
      "Min termine corrente: day                            nr. postings: 3\n",
      "index 0\n",
      "index 4\n",
      "index 5\n",
      "Min termine corrente: do                             nr. postings: 1\n",
      "index 6\n",
      "Min termine corrente: doesn't                        nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: dog                            nr. postings: 2\n",
      "index 0\n",
      "index 4\n",
      "Min termine corrente: don't                          nr. postings: 2\n",
      "index 1\n",
      "index 6\n",
      "Min termine corrente: each                           nr. postings: 1\n",
      "index 3\n",
      "Min termine corrente: early                          nr. postings: 1\n",
      "index 3\n",
      "Min termine corrente: eat                            nr. postings: 1\n",
      "index 3\n",
      "Min termine corrente: english                        nr. postings: 1\n",
      "index 0\n",
      "Min termine corrente: every                          nr. postings: 2\n",
      "index 1\n",
      "index 4\n",
      "Min termine corrente: eye                            nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: fall                           nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: far                            nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: faster                         nr. postings: 1\n",
      "index 0\n",
      "Min termine corrente: feather                        nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: fire                           nr. postings: 1\n",
      "index 3\n",
      "Min termine corrente: fits,                          nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: flock                          nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: fool                           nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: for                            nr. postings: 2\n",
      "index 1\n",
      "index 4\n",
      "Min termine corrente: from                           nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: glass                          nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: glitters                       nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: godliness                      nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: gold                           nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: golden                         nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: good                           nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: goodmoring                     nr. postings: 1\n",
      "index 0\n",
      "Min termine corrente: great                          nr. postings: 1\n",
      "index 4\n",
      "Min termine corrente: happiness                      nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: has                            nr. postings: 2\n",
      "index 1\n",
      "index 4\n",
      "Min termine corrente: hatch                          nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: have                           nr. postings: 1\n",
      "index 3\n",
      "Min termine corrente: heads                          nr. postings: 1\n",
      "index 3\n",
      "Min termine corrente: his                            nr. postings: 1\n",
      "index 3\n",
      "Min termine corrente: home                           nr. postings: 1\n",
      "index 6\n",
      "Min termine corrente: honesty                        nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: hope                           nr. postings: 1\n",
      "index 4\n",
      "Min termine corrente: houses                         nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: how                            nr. postings: 1\n",
      "index 0\n",
      "Min termine corrente: i                              nr. postings: 1\n",
      "index 0\n",
      "Min termine corrente: if                             nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: ignorance                      nr. postings: 1\n",
      "index 4\n",
      "Min termine corrente: in                             nr. postings: 7\n",
      "index 0\n",
      "index 1\n",
      "index 2\n",
      "index 5\n",
      "index 6\n",
      "Min termine corrente: invention                      nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: is                             nr. postings: 16\n",
      "index 0\n",
      "index 1\n",
      "index 2\n",
      "index 3\n",
      "index 4\n",
      "index 5\n",
      "Min termine corrente: it                             nr. postings: 2\n",
      "index 2\n",
      "index 3\n",
      "Min termine corrente: it's                           nr. postings: 2\n",
      "index 2\n",
      "index 4\n",
      "Min termine corrente: italian                        nr. postings: 1\n",
      "index 0\n",
      "Min termine corrente: its                            nr. postings: 1\n",
      "index 4\n",
      "Min termine corrente: jump                           nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: keep                           nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: killed                         nr. postings: 1\n",
      "index 4\n",
      "Min termine corrente: lake                           nr. postings: 1\n",
      "index 0\n",
      "Min termine corrente: last                           nr. postings: 2\n",
      "index 0\n",
      "index 4\n",
      "Min termine corrente: late                           nr. postings: 1\n",
      "index 4\n",
      "Min termine corrente: laugh                          nr. postings: 1\n",
      "index 4\n",
      "Min termine corrente: laughs                         nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: let                            nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: like                           nr. postings: 1\n",
      "index 6\n",
      "Min termine corrente: lines                          nr. postings: 1\n",
      "index 3\n",
      "Min termine corrente: lining                         nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: live                           nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: long                           nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: louder                         nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: low                            nr. postings: 1\n",
      "index 0\n",
      "Min termine corrente: make                           nr. postings: 2\n",
      "index 2\n",
      "index 6\n",
      "Min termine corrente: makes                          nr. postings: 1\n",
      "index 3\n",
      "Min termine corrente: man                            nr. postings: 1\n",
      "index 0\n",
      "Min termine corrente: marvel                         nr. postings: 1\n",
      "index 0\n",
      "Min termine corrente: me                             nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: merrier                        nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: mightier                       nr. postings: 1\n",
      "index 3\n",
      "Min termine corrente: mind                           nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: minds                          nr. postings: 1\n",
      "index 4\n",
      "Min termine corrente: money                          nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: moon                           nr. postings: 1\n",
      "index 3\n",
      "Min termine corrente: more,                          nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: mother                         nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: necessity                      nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: netflix                        nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: never                          nr. postings: 2\n",
      "index 4\n",
      "Min termine corrente: new                            nr. postings: 1\n",
      "index 0\n",
      "Min termine corrente: news                           nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: next                           nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: night                          nr. postings: 1\n",
      "index 0\n",
      "Min termine corrente: no                             nr. postings: 3\n",
      "index 3\n",
      "index 5\n",
      "index 6\n",
      "Min termine corrente: not                            nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: of                             nr. postings: 7\n",
      "index 0\n",
      "index 1\n",
      "index 2\n",
      "index 5\n",
      "Min termine corrente: on                             nr. postings: 3\n",
      "index 0\n",
      "index 2\n",
      "Min termine corrente: once                           nr. postings: 2\n",
      "index 2\n",
      "Min termine corrente: one                            nr. postings: 2\n",
      "index 0\n",
      "index 3\n",
      "Min termine corrente: ontario                        nr. postings: 1\n",
      "index 0\n",
      "Min termine corrente: out                            nr. postings: 2\n",
      "index 2\n",
      "index 5\n",
      "Min termine corrente: own                            nr. postings: 1\n",
      "index 3\n",
      "Min termine corrente: pen                            nr. postings: 2\n",
      "index 0\n",
      "index 3\n",
      "Min termine corrente: penny                          nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: people                         nr. postings: 4\n",
      "index 1\n",
      "index 3\n",
      "index 5\n",
      "index 6\n",
      "Min termine corrente: perfect                        nr. postings: 1\n",
      "index 3\n",
      "Min termine corrente: piece                          nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: place                          nr. postings: 1\n",
      "index 6\n",
      "Min termine corrente: policy.                        nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: pot                            nr. postings: 1\n",
      "index 4\n",
      "Min termine corrente: practice                       nr. postings: 1\n",
      "index 3\n",
      "Min termine corrente: prepare                        nr. postings: 1\n",
      "index 4\n",
      "Min termine corrente: read                           nr. postings: 1\n",
      "index 3\n",
      "Min termine corrente: reap                           nr. postings: 1\n",
      "index 6\n",
      "Min termine corrente: right                          nr. postings: 1\n",
      "index 6\n",
      "Min termine corrente: romans                         nr. postings: 1\n",
      "index 6\n",
      "Min termine corrente: rome                           nr. postings: 2\n",
      "index 5\n",
      "index 6\n",
      "Min termine corrente: saw                            nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: series                         nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: shame                          nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: shoe                           nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: short                          nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: shouldn't                      nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: sight                          nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: silence                        nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: silver                         nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: smoke                          nr. postings: 1\n",
      "index 3\n",
      "Min termine corrente: sow                            nr. postings: 1\n",
      "index 6\n",
      "Min termine corrente: speak                          nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: spiderman                      nr. postings: 1\n",
      "index 0\n",
      "Min termine corrente: stones                         nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: storm                          nr. postings: 1\n",
      "index 4\n",
      "Min termine corrente: story                          nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: straw                          nr. postings: 1\n",
      "index 4\n",
      "Min termine corrente: sunny                          nr. postings: 1\n",
      "index 0\n",
      "Min termine corrente: superhero                      nr. postings: 1\n",
      "index 0\n",
      "Min termine corrente: sword                          nr. postings: 1\n",
      "index 3\n",
      "Min termine corrente: table                          nr. postings: 1\n",
      "index 0\n",
      "Min termine corrente: than                           nr. postings: 5\n",
      "index 0\n",
      "index 1\n",
      "index 3\n",
      "index 4\n",
      "Min termine corrente: that                           nr. postings: 2\n",
      "index 1\n",
      "index 4\n",
      "Min termine corrente: the                            nr. postings: 21\n",
      "index 0\n",
      "index 1\n",
      "index 2\n",
      "index 3\n",
      "index 4\n",
      "index 5\n",
      "index 6\n",
      "Min termine corrente: then                           nr. postings: 1\n",
      "index 0\n",
      "Min termine corrente: there's                        nr. postings: 2\n",
      "index 3\n",
      "index 6\n",
      "Min termine corrente: they                           nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: think                          nr. postings: 1\n",
      "index 4\n",
      "Min termine corrente: thoughts                       nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: throw                          nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: to                             nr. postings: 2\n",
      "index 1\n",
      "index 3\n",
      "Min termine corrente: together                       nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: too                            nr. postings: 1\n",
      "index 3\n",
      "Min termine corrente: tree                           nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: twice                          nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: two                            nr. postings: 2\n",
      "index 3\n",
      "index 6\n",
      "Min termine corrente: universe                       nr. postings: 1\n",
      "index 0\n",
      "Min termine corrente: up                             nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: very                           nr. postings: 1\n",
      "index 0\n",
      "Min termine corrente: wasn't                         nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: watched                        nr. postings: 1\n",
      "index 3\n",
      "Min termine corrente: wear                           nr. postings: 1\n",
      "index 2\n",
      "Min termine corrente: weep                           nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: what                           nr. postings: 1\n",
      "index 6\n",
      "Min termine corrente: when                           nr. postings: 1\n",
      "index 6\n",
      "Min termine corrente: who                            nr. postings: 1\n",
      "index 5\n",
      "Min termine corrente: with                           nr. postings: 2\n",
      "index 0\n",
      "index 5\n",
      "Min termine corrente: without                        nr. postings: 1\n",
      "index 3\n",
      "Min termine corrente: words                          nr. postings: 1\n",
      "index 1\n",
      "Min termine corrente: world                          nr. postings: 2\n",
      "index 0\n",
      "index 5\n",
      "Min termine corrente: worm                           nr. postings: 1\n",
      "index 3\n",
      "Min termine corrente: worst                          nr. postings: 2\n",
      "index 0\n",
      "index 4\n",
      "Min termine corrente: wrongs                         nr. postings: 1\n",
      "index 6\n",
      "Min termine corrente: you                            nr. postings: 5\n",
      "index 0\n",
      "index 2\n",
      "index 3\n",
      "index 5\n",
      "index 6\n",
      "Min termine corrente: your                           nr. postings: 4\n",
      "index 1\n",
      "index 2\n",
      "index 3\n",
      "END METHOD!\n"
     ]
    }
   ],
   "source": [
    "indexBuilder=IndexBuilder()\n",
    "#invIndex=indexBuilder.build_in_memory_index(tot_doc)\n",
    "indexBuilder.single_pass_in_memory_indexing(tot_doc,2220,2220,True)\n",
    "indexBuilder.index_merging(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f41f055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind = InvertedIndex()\n",
    "# ind.read_from_block_all_index_in_memory(\"lexicon.bin\",\"doc_ids.bin\",\"freq.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "921732d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Posting(doc_id=3, frequency=2),\n",
       " Posting(doc_id=5, frequency=2),\n",
       " Posting(doc_id=9, frequency=1),\n",
       " Posting(doc_id=10, frequency=1),\n",
       " Posting(doc_id=14, frequency=1),\n",
       " Posting(doc_id=17, frequency=1),\n",
       " Posting(doc_id=21, frequency=1),\n",
       " Posting(doc_id=25, frequency=1),\n",
       " Posting(doc_id=27, frequency=1),\n",
       " Posting(doc_id=36, frequency=1),\n",
       " Posting(doc_id=51, frequency=1),\n",
       " Posting(doc_id=56, frequency=1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ind.get_postings(\"a\".ljust(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "036a6c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([Posting(doc_id=3, frequency=2), Posting(doc_id=5, frequency=2), Posting(doc_id=9, frequency=1), Posting(doc_id=10, frequency=1), Posting(doc_id=14, frequency=1), Posting(doc_id=17, frequency=1), Posting(doc_id=21, frequency=1), Posting(doc_id=25, frequency=1), Posting(doc_id=27, frequency=1), Posting(doc_id=36, frequency=1), Posting(doc_id=51, frequency=1)], 44, 44)\n"
     ]
    }
   ],
   "source": [
    "# fileFinalDocIds=open(PATH_FINAL_DOC_IDS, 'rb') \n",
    "# fileFinalFreq=open(PATH_FINAL_FREQ, 'rb') \n",
    "\n",
    "# posting=InvertedIndex.read_from_files_a_posting_list(11,fileFinalDocIds,fileFinalFreq,0,0)\n",
    "# print(posting)\n",
    "# #(nr_postings:int,fileDocIds,fileFreq,offsetDocIds:int,offsetFreq:int):\n",
    "# fileFinalDocIds.close()\n",
    "# fileFinalFreq.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f9af2496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #To test a block\n",
    "# invIndex=InvertedIndex()\n",
    "# invIndex.read_to_block_all_index(\"TEMP\",0)\n",
    "# invIndex.get_postings(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4a8146b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.LexiconRow object at 0x000001BAA0099370>\n"
     ]
    }
   ],
   "source": [
    "# DIR_FOLDER=\"TEMP\"\n",
    "# b=a.read_row_on_disk(DIR_FOLDER+\"/LEXICON_TEMP/lex_nr_0\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a7a315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metodo di callback per leggere input file\n",
    "#collegare la compression e modificare le funzioni\n",
    "#Guardare funzionamento dei blocchi skipping etc\n",
    "#fare test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
