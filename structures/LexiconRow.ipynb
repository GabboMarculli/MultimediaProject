{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a5103db-5fd6-4948-a141-5023e9578ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "from typing import BinaryIO\n",
    "\n",
    "import import_ipynb\n",
    "import sys\n",
    "sys.path.append('../')  \n",
    "import structures.DocumentIndex as doc_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ea884ab-e252-41dd-b361-81e944edfac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexiconRow:\n",
    "    \n",
    "    MAX_TERM_LENGTH=30\n",
    "    STR_SIZE_LEXICON_ROW='30s 2i 2f 3q 3i'\n",
    "    SIZE_LEXICON_ROW=struct.calcsize(STR_SIZE_LEXICON_ROW)\n",
    "    \n",
    "    def __init__(self, term: str, dft: int, max_tf: int=0, bm25dl:float=0, BM25Tf:float=0,\n",
    "                 docidOffset:int=0,frequencyOffset:int=0,blockOffset:int=0,docidSize:int=0,frequencySize:int=0,numBlocks:int=1):\n",
    "        \n",
    "        self.term = term if (len(term)<self.MAX_TERM_LENGTH) else term[:self.MAX_TERM_LENGTH]\n",
    "        self.term = self.term.ljust(self.MAX_TERM_LENGTH)\n",
    "        \n",
    "        # Document frequency of the term\n",
    "        self.dft = dft\n",
    "        \n",
    "        # Max term frequency\n",
    "        self.max_tf = max_tf\n",
    "        \n",
    "\n",
    "        # Inverse of document frequency of the term.              \n",
    "        self.idft = 0\n",
    "        #compute_IDFT(doc_index.number_of_documents, dft)\n",
    "\n",
    "        # Max tfidf\n",
    "        self.maxTFIDF = 0\n",
    "        #compute_TFIDF(max_tf, self.idft)\n",
    "    \n",
    "        \n",
    "        self.docidOffset=docidOffset\n",
    "        self.frequencyOffset=frequencyOffset\n",
    "        self.blockOffset=0\n",
    "        \n",
    "        self.docidSize=0\n",
    "        self.frequencySize=0\n",
    "        self.numBlocks=0\n",
    "        \n",
    "\n",
    "#     def __init__(self, term: str, dft: int, doc_index: DocumentIndex, max_tf: int):\n",
    "#         if not isinstance(term, str) or not isinstance(doc_index, DocumentIndex):\n",
    "#             raise ValueError(\"term must be a string and doc_index a DocumentIndex.\")\n",
    "#         if not isinstance(dft, int) or not isinstance(max_tf, int):\n",
    "#             raise ValueError(\"dft and max_tf must be integers.\")\n",
    "\n",
    "#         self.term = term\n",
    "\n",
    "#         # Document frequency of the term\n",
    "#         self.dft = dft\n",
    "\n",
    "#         # Inverse of document frequency of the term.              \n",
    "#         self.idft = compute_IDFT(doc_index.number_of_documents, dft)\n",
    "\n",
    "#         # Max term frequency\n",
    "#         self.max_tf = max_tf\n",
    "\n",
    "#         # Max tfidf\n",
    "#         self.maxTFIDF = compute_TFIDF(max_tf, self.idft)\n",
    "\n",
    "    def to_string(self):\n",
    "        \"\"\"This function returns a string representation of a LexiconRow.\n",
    "        \n",
    "        Returns:\n",
    "            a human readable string representation of the Lexicon Row\n",
    "        \"\"\"\n",
    "        string = ' '.join([str(self.term) , str(self.dft) , str(self.idft), str(self.max_tf), str(self.maxTFIDF)])\n",
    "        return string    \n",
    "    \n",
    "    def write_lexicon_row_on_disk_to_opened_file(self,file:BinaryIO,offset:int=0):\n",
    "        \"\"\"This function writes on a specific position of an opened file a lexicon row information.\n",
    "           \n",
    "           Args:\n",
    "               file: the file to store the lexicon row\n",
    "               offset: the position inside the file to store the lexicon row\n",
    "           Returns:\n",
    "               the new offset free position after writing on the file\n",
    "        \"\"\"\n",
    "        \n",
    "        file.seek(offset)\n",
    "       \n",
    "        binary_data = struct.pack(self.STR_SIZE_LEXICON_ROW, \n",
    "                                      self.term.encode('utf-8'),\n",
    "                                      self.dft,self.max_tf,\n",
    "                                      self.idft, self.maxTFIDF, \n",
    "                                      self.docidOffset, self.frequencyOffset,self.blockOffset,\n",
    "                                      self.docidSize, self.frequencySize, self.numBlocks)\n",
    "        file.write(binary_data)\n",
    "            \n",
    "        return self.SIZE_LEXICON_ROW+offset\n",
    "        \n",
    "    def read_lexicon_row_on_disk_from_opened_file(self,file:BinaryIO,offset:int):\n",
    "        \"\"\"This function reads a lexicon row informations in a specific position from an opened file.\n",
    "        \n",
    "        Args:\n",
    "            file: the file to read a lexicon row\n",
    "            offset: the position inside the file to read the lexicon row\n",
    "        \n",
    "        Returns:\n",
    "            the offset position after reading\n",
    "            \n",
    "        \"\"\"\n",
    "        file.seek(offset)  \n",
    "        bytesLetti = file.read(self.SIZE_LEXICON_ROW)\n",
    "        \n",
    "        if(not bytesLetti):\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            term,dft,max_tf,idft,maxTFIDF,docidOffset,frequencyOffset,blockOffset, docidSize,frequencySize,numBlocks = struct.unpack(self.STR_SIZE_LEXICON_ROW, bytesLetti)\n",
    "\n",
    "            self.term=term.decode('utf-8')\n",
    "            self.dft=dft\n",
    "            self.idft=idft\n",
    "            self.max_tf=max_tf\n",
    "            self.maxTFIDF=maxTFIDF\n",
    "            self.docidOffset=docidOffset\n",
    "            self.frequencyOffset=frequencyOffset\n",
    "            self.docidSize=docidSize\n",
    "            self.frequencySize=frequencySize\n",
    "            self.numBlocks=numBlocks\n",
    "            self.blockOffset=blockOffset\n",
    "            \n",
    "            \n",
    "        except struct.error as e:\n",
    "            print(f\"Error unpacking data: {e}\")\n",
    "            \n",
    "        return offset+self.SIZE_LEXICON_ROW\n",
    "    \n",
    "   \n",
    "    #USED FOR DEBUGGING\n",
    "    \n",
    "    def write_lexicon_row_on_disk(self,file_path:str,offset:int=0):\n",
    "        \"\"\"This function opens a file and writes on a specific position a lexicon row information.\n",
    "            This is used for debug and tests.\n",
    "        \n",
    "            Args:\n",
    "               file_path: the file to store the lexicon row\n",
    "               offset: the position inside the file to store the lexicon row\n",
    "            Returns:\n",
    "                the new offset free position after writing\n",
    "               \n",
    "        \"\"\"\n",
    "        with open(file_path, 'ab') as file:\n",
    "            return self.write_lexicon_row_on_disk_to_opened_file(file,offset)\n",
    "            \n",
    "    def read_lexicon_row_on_disk(self,file_path:str,offset:int):\n",
    "        \"\"\"This function opens a file and reads in a specific position a lexicon row information.\n",
    "            This is used for debug and tests.\n",
    "        \n",
    "            Args:\n",
    "               file_path: the file to read a lexicon row\n",
    "               offset: the position inside the file to read the lexicon row\n",
    "            Returns:\n",
    "                the offset position after reading\n",
    "        \"\"\"\n",
    "        with open(file_path, 'rb') as file:\n",
    "            return self.read_lexicon_row_on_disk_from_opened_file(file,offset)\n",
    "\n",
    "    def compute_avgDL(doc_index: doc_index.DocumentIndex) -> float:\n",
    "        \"\"\"\n",
    "        Compute the average document length based on the document index (doc_index).\n",
    "    \n",
    "        Args:\n",
    "            doc_index: An instance of DocumentIndex representing the document index.\n",
    "        \"\"\"\n",
    "        if not isinstance(doc_index, doc_index.DocumentIndex):\n",
    "            raise ValueError(\"Invalid parameters.\")\n",
    "            \n",
    "        return doc_index.total_document_length / doc_index.number_of_documents\n",
    "\n",
    "\n",
    "    def compute_IDFT(number_of_documents: int, dft:int) -> float:\n",
    "        \"\"\"\n",
    "        Compute the inverse document frequency for a term based on the document index (doc_index) and document frequency (dft).\n",
    "    \n",
    "        Args:\n",
    "            doc_index: An instance of DocumentIndex representing the document index.\n",
    "            dft: The document frequency of the term.\n",
    "        \"\"\"\n",
    "        if not isinstance(number_of_documents, int) or not isinstance(dft, int):\n",
    "            raise ValueError(\"Invalid parameters.\")\n",
    "    \n",
    "        if dft < 0 or number_of_documents < 0:\n",
    "            return 0\n",
    "        \n",
    "        return math.log(number_of_documents/dft)  \n",
    "\n",
    "    def compute_TFIDF(tf: int, idf: float) -> float:\n",
    "        \"\"\"\n",
    "        Compute the TF-IDF value based on the term frequency (tf) and inverse document frequency (idf).\n",
    "        \n",
    "        Args:\n",
    "            tf: An integer representing the term frequency.\n",
    "            idf: A float representing the inverse document frequency.\n",
    "        \"\"\"\n",
    "        if not isinstance(tf, int) or not isinstance(idf, float):\n",
    "            raise ValueError(\"Invalid parameters.\")\n",
    "    \n",
    "        if tf < 0:\n",
    "            return 0\n",
    "            \n",
    "        return (1 + math.log(tf)) * idf\n",
    "\n",
    "    def compute_max_term_frequency(postings_list: str) -> int:\n",
    "        \"\"\"\n",
    "        Given a postings list of a term, compute the maximum term frequency.\n",
    "    \n",
    "        Args:\n",
    "            postings_list: A string containing elements with colon-separated values.\n",
    "        \"\"\"\n",
    "        if not isinstance(postings_list, str):\n",
    "            raise ValueError(\"Invalid postings list.\")\n",
    "    \n",
    "        if len(postings_list) == 0:\n",
    "            return 0\n",
    "            \n",
    "        # Split the postings list into individual elements\n",
    "        postings_elements = postings_list.split()\n",
    "    \n",
    "        # Initialize the maximum value with the value from the first element\n",
    "        max = int(postings_elements[0].split(':')[1])\n",
    "    \n",
    "        # Iterate through each element and find the maximum value after the colon\n",
    "        for item in postings_elements:\n",
    "            # Split each element to extract the value after the colon\n",
    "            parts = item.split(':')\n",
    "            value = int(parts[1])\n",
    "            \n",
    "            # Compare the extracted value with the current maximum value and update if necessary\n",
    "            if value > max:\n",
    "                max = value\n",
    "    \n",
    "        return max\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
