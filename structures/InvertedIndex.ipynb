{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d721358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\structures\\..\\structures\\Lexicon.ipynb\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import struct\n",
    "import import_ipynb\n",
    "from typing import List, Dict, Tuple, Union, Any, Callable\n",
    "from collections import Counter, defaultdict,OrderedDict\n",
    "from dataclasses import dataclass\n",
    "from typing import TextIO, BinaryIO\n",
    "\n",
    "\n",
    "sys.path.append('../')  # Go up two folders to the project root\n",
    "from structures.Lexicon import Lexicon,LexiconRow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "162f1abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Posting:\n",
    "    doc_id: int\n",
    "    frequency: Any = None\n",
    "    \n",
    "    #The following methods are used mainly for debug and tests.\n",
    "    \n",
    "    @classmethod \n",
    "    def from_string(cls, description:str):\n",
    "        docId,payl=description.split(\":\")\n",
    "        doc_id=int(docId)\n",
    "        frequency=int(payl)\n",
    "        return cls(doc_id, frequency)\n",
    "    \n",
    "    def write_to_disk(self,file_path:str,arg:str, offset:int=0)->None:\n",
    "        \"\"\"Function to open a file and write on it a single posting information in a specific position\n",
    "        \n",
    "        Args:\n",
    "            file_path: the file name to be opened in append mode\n",
    "            arg: indicates TYPE_DOC_ID or TYPE_FREQ the single information to be written\n",
    "            offset: the position in bytes inside a file to be written\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(file_path, 'ab') as file:\n",
    "            self.write_to_disk_from_opened_file(file,arg,offset)\n",
    "        \n",
    "    def write_to_disk_from_opened_file(self,file:BinaryIO,arg,offset:int=0):\n",
    "        \"\"\"Function to write on a opened file an information of a posting in a specific position\n",
    "        \n",
    "        Args:\n",
    "            file: the file to be used\n",
    "            arg: inidcates TYPE_DOC_ID or TYPE_FREQ to write a single information\n",
    "            offset: the position in bytes inside a file to be written\n",
    "            \n",
    "        Returns:\n",
    "               the next offset position in the file after writing the information \n",
    "        \"\"\"\n",
    "        file.seek(offset)\n",
    "        if (arg==TYPE_DOC_ID):\n",
    "            binary_data = struct.pack(\"i\",self.doc_id)\n",
    "        if (arg==TYPE_FREQ):\n",
    "            binary_data = struct.pack(\"i\",self.frequency)\n",
    "    \n",
    "        file.write(binary_data)\n",
    "            \n",
    "        return struct.calcsize('i')+offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d57a055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._index = defaultdict(list)\n",
    "        \n",
    "    def add_posting(self, term: str, doc_id: int, frequency: Any=None) -> None:\n",
    "        \"\"\"Adds a document to the posting list of a term.\n",
    "\n",
    "        Args:\n",
    "            term: the term to be added to the inveted index\n",
    "            doc_id: the document id to be added linked to a specific term\n",
    "            frequency: the frequency of a term inside a specific doc_id to be added \n",
    "            \n",
    "        \"\"\"\n",
    "        # append new posting to the posting list\n",
    "        if (self.get_postings(term)==None):\n",
    "            self._index[term]=[]\n",
    "        self._index[term].append(Posting(doc_id,frequency))\n",
    "             \n",
    "    def get_postings(self, term: str) -> List[Posting]:\n",
    "        \"\"\"Fetches the posting list for a given term.\n",
    "        \n",
    "        Args:\n",
    "            term: the term to be found inside the inverted index\n",
    "\n",
    "        Returns:\n",
    "            the list of postings associated to the term\n",
    "        \"\"\"\n",
    "        \n",
    "        if (term in self._index):\n",
    "            return self._index[term]\n",
    "        return None\n",
    "    \n",
    "    def is_empty(self)->bool:\n",
    "        \"\"\"Check if there is no term in the inverted index.\"\"\"\n",
    "        return len(self.get_terms())==0\n",
    "    \n",
    "    def get_terms(self) -> List[str]:\n",
    "        \"\"\"Returns all unique terms in the index.\"\"\"\n",
    "        return self._index.keys() \n",
    "    \n",
    "    def clear_structure(self)->None:\n",
    "        \"\"\" It clears the inverted index data structure present in main memory.\"\"\"\n",
    "        self._index.clear()\n",
    "    \n",
    "    def get_structure(self)->None:\n",
    "        \"\"\"Returns the inverted index data structure.\"\"\"\n",
    "        return self._index\n",
    "     \n",
    "    @staticmethod\n",
    "    def write_to_files_a_posting_list(list_of_posting:List[Posting],file_doc_ids:BinaryIO,file_freq:BinaryIO,offset_doc_ids:int,offset_freq:int)-> Tuple[int, int]:\n",
    "        \"\"\"Static method to write a posting list in 2 distinct files. \n",
    "           One file is used to save doc_ids and an other is used to save freqs.\n",
    "        \n",
    "        Args:\n",
    "            list_of_posting: the posting list to be saved\n",
    "            file_doc_ids: the file used to save doc_ids\n",
    "            file_freq: the file used to save frequencies\n",
    "            offset_doc_ids: the start offset position for writing the list of doc_ids\n",
    "            offset_freq: the start offset position for writing the list of freq\n",
    "        Returns:\n",
    "            the new free offset position in the file_doc_ids after writing\n",
    "            the new free offset position in the file_freq after writing\n",
    "        \n",
    "        \"\"\"\n",
    "        doc_ids=[posting.doc_id for posting in list_of_posting]\n",
    "        freqs=[posting.frequency for posting in list_of_posting]\n",
    "        \n",
    "        file_doc_ids.seek(offset_doc_ids)  \n",
    "        file_freq.seek(offset_freq)\n",
    "        \n",
    "        packed_data = struct.pack('!{}i'.format(len(doc_ids)), *doc_ids)\n",
    "        file_doc_ids.write(packed_data)\n",
    "        \n",
    "        packed_data = struct.pack('!{}i'.format(len(freqs)), *freqs)\n",
    "        file_freq.write(packed_data)\n",
    "        \n",
    "        return offset_doc_ids+struct.calcsize('!{}i'.format(len(doc_ids))),offset_freq+struct.calcsize('!{}i'.format(len(freqs)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_from_files_a_posting_list(nr_postings:int,file_doc_ids:BinaryIO,file_freq:BinaryIO,offset_doc_ids:int,offset_freq:int)-> Tuple[List[Posting],int, int]:\n",
    "        \"\"\"Static method to read 'a number of postings' of a posting list from 2 distinct files:\n",
    "           One file contains the saved doc_ids and the other contains the saved freqs.\n",
    "           \n",
    "        Args:\n",
    "            nr_postings: number of postings to be read from files\n",
    "            file_doc_ids: the file where doc_ids are saved\n",
    "            file_freq: the file where frequencies are saved\n",
    "            offset_doc_ids: the start offset position for reading the list of doc_ids\n",
    "            offset_freq: the start offset position for reading the list of frequency\n",
    "           \n",
    "        Returns:\n",
    "            posting_list: read from the 2 files\n",
    "            offset_doc_id: new offset of the read file doc_ids\n",
    "            offeset_freqs: new offset of the read file freqs\n",
    "        \"\"\"\n",
    "        \n",
    "        file_doc_ids.seek(offset_doc_ids)  \n",
    "        file_freq.seek(offset_freq)\n",
    "        \n",
    "        bytes_to_read=struct.calcsize('!{}i'.format(nr_postings))\n",
    "        \n",
    "        #print (\"offsetDocIds:\"+str(offsetDocIds))\n",
    "        #print (\"offsetFreq:\"+str(offsetFreq))\n",
    "        #print (\"nr_posting:\"+str(nr_postings))\n",
    "        #print (\"byte_to_read:\"+str(bytes_to_read))\n",
    "        \n",
    "        packed_data = file_doc_ids.read(bytes_to_read)\n",
    "        doc_ids = struct.unpack('!{}i'.format(nr_postings), packed_data)\n",
    "        \n",
    "        packed_data = file_freq.read(bytes_to_read)\n",
    "        freqs = struct.unpack('!{}i'.format(nr_postings), packed_data)\n",
    "        \n",
    "        posting_list=[]\n",
    "        for i in range (len(doc_ids)):\n",
    "            posting_list.append(Posting(doc_ids[i],freqs[i]))\n",
    "\n",
    "        \n",
    "        return posting_list,offset_doc_ids+bytes_to_read,offset_freq+bytes_to_read\n",
    "        \n",
    "        \n",
    "    \n",
    "    def write_to_block_all_index_in_memory(self,path_lexicon: str,path_doc_ids:str,path_freq:str)-> None:\n",
    "        \"\"\" Function to write the overall index in main memory to a file \"block\" during the spimi phase.\n",
    "            \n",
    "        Args:\n",
    "            path_lexicon: the position where to write the block related to the temporal lexicon.\n",
    "            path_doc_ids: the position where to write the block related to the temporal doc_ids.\n",
    "            path_freq: the position where to write the block related to the temporal freqs.\n",
    "            \n",
    "        \"\"\"\n",
    "        sorted_lexicon=sorted(self._index.items())\n",
    "        \n",
    "        with open(path_freq, \"ab\") as f_freq:\n",
    "            \n",
    "            with open(path_doc_ids, \"ab\") as f_doc_id:\n",
    "        \n",
    "                with open(path_lexicon, \"ab\") as f_lexicon:\n",
    "\n",
    "                    offset_lexicon=0\n",
    "                    offset_doc_ids=0\n",
    "                    offset_freq=0\n",
    "                    \n",
    "                    for term,postings in sorted_lexicon:\n",
    "                        \n",
    "                        #Istantiate a lexicon row\n",
    "                        lexRow=LexiconRow(term,len(postings),0,0,0,offset_doc_ids,offset_freq,0,0)                                                                                            \n",
    "                        \n",
    "                        #Save posting list to specifics file\n",
    "                        offset_doc_ids,offset_freq=InvertedIndex.write_to_files_a_posting_list(postings,f_doc_id,f_freq,offset_doc_ids,offset_freq)     \n",
    "                        \n",
    "                        #Save the related lexicon row.\n",
    "                        offset_lexicon+=lexRow.write_lexicon_row_on_disk_to_opened_file(f_lexicon,offset_lexicon)\n",
    "\n",
    "                        \n",
    "    #Debugging output functions                     \n",
    "    \n",
    "    @staticmethod\n",
    "    \n",
    "    def write_to_file_a_posting_list_debug_mode(file_debug:TextIO,term:str, posting_list:List, new_term:bool) -> None:\n",
    "        \"\"\" \n",
    "        This method write in human readable format the entire inverted index processed. It concatenates \n",
    "        the posting lists amongs blocks on the same output file.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        if (new_term):\n",
    "            file_debug.write(\"\\n\")\n",
    "            file_debug.write(term)\n",
    "\n",
    "        for posting in posting_list:\n",
    "            file_debug.write(f\" {posting.doc_id}\")\n",
    "            if posting.frequency:\n",
    "                file_debug.write(f\":{str(posting.frequency)}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def read_from_block_all_index_in_memory(self, path_lexicon: str,path_doc_ids:str, path_freq:str)-> None:\n",
    "        \"\"\" Function to read an inverted index in main memory related to a specific lexicon file.\n",
    "            This function is used for debug and test reasons.\n",
    "        \n",
    "        Args:\n",
    "            path_lexicon: the position where to read the lexicon related to a specific inverted index\n",
    "            path_doc_ids: the position where to read the information of doc_ids.\n",
    "            path_freq: the position where to read the information of freq.\n",
    "            \n",
    "        Returns:\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        self.clear_structure()\n",
    "        \n",
    "        offset_lexicon=0\n",
    "        offset_doc_id=0\n",
    "        offset_freq=0\n",
    "        \n",
    "        with open(path_freq, \"rb\") as f_freq:\n",
    "            with open(path_doc_ids, \"rb\") as f_doc_id:\n",
    "                with open(path_lexicon, \"rb\") as f_lexicon:\n",
    "                \n",
    "                    #Initialize empty lexicon row.\n",
    "                    lexiconTerm=LexiconRow(\"\",0)\n",
    "                    \n",
    "                    while (True):\n",
    "                        \n",
    "                        offset_lexicon=lexiconTerm.read_lexicon_row_on_disk_from_opened_file(f_lexicon,offset_lexicon)\n",
    "                        \n",
    "                        if(offset_lexicon==None):\n",
    "                            #Here, I finished to read all the lexicon block.\n",
    "                            return\n",
    "                        \n",
    "                        #print (\"termine: \"+str(lexiconTerm.term))\n",
    "                        #print (\"dft: \"+str(lexiconTerm.dft))\n",
    "                        #print (\"docIdOff: \"+str(lexiconTerm.docidOffset))\n",
    "                        #print (\"freqOff: \"+str(lexiconTerm.frequencyOffset))\n",
    "                        \n",
    "                        \n",
    "                        posting_list,offset_doc_id,offset_freq=InvertedIndex.read_from_files_a_posting_list(lexiconTerm.dft,f_doc_id,f_freq,offset_doc_id,offset_freq)\n",
    "                        \n",
    "                        for posting in posting_list:\n",
    "                            self.add_posting(lexiconTerm.term,posting.doc_id,posting.frequency)\n",
    "                           \n",
    "    def write_to_block_debug_mode(self,file_name_index: str) -> None:\n",
    "        \"\"\" Write the inverted index (in memory) in lexicographical oreder into a file_name_index on disk human readable.\n",
    "            It is used just for debugging the spimi phase.\n",
    "        Args:\n",
    "            file_name_index: the name of the file where the inverted index will be written in clear\n",
    "            \n",
    "        \"\"\"\n",
    "        sorted_lexicon=sorted(self._index.items())\n",
    "        with open(file_name_index, \"w\") as f:\n",
    "            for term,postings in sorted_lexicon:\n",
    "                f.write(term)\n",
    "                for posting in postings:\n",
    "                    f.write(f\" {posting.doc_id}\")\n",
    "                    if posting.frequency:\n",
    "                        f.write(f\":{str(posting.frequency)}\")\n",
    "                f.write(\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d23f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
