{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2e21d95-98d7-45f1-937f-b59f775699bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\structures\\..\\structures\\LexiconRow.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\structures\\..\\structures\\DocumentIndex.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\structures\\..\\utilities\\General_Utilities.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\structures\\..\\structures\\DocumentIndexRow.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\structures\\..\\building_data_structures\\CollectionStatistics.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import struct\n",
    "\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, TextIO, BinaryIO\n",
    "\n",
    "import import_ipynb\n",
    "import sys\n",
    "sys.path.append('../')  # Go up two folders to the project root\n",
    "\n",
    "import structures.LexiconRow as lex_row\n",
    "import structures.DocumentIndex as doc_index\n",
    "import utilities.General_Utilities as util\n",
    "import building_data_structures.CollectionStatistics as coll_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62a61078-8c2e-484d-bb56-5ef964997cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def create_lexicon(file_input_path: str, file_output_path: str, DIR_FOLDER: str, file_extension: str, block_size: int, document_index: doc_index.DocumentIndex) -> int:\n",
    "    \"\"\"\n",
    "    Function returns a file with one row for each distinct term in the corpus. Rows are composed by:\n",
    "    term, document frequency, inverse document frequency, term upper bound\n",
    "    Each values is separated by a comma.\n",
    "\n",
    "    Args:\n",
    "        file_input_path: file that contains the inverted index\n",
    "        file_output_path: file that will contains the result\n",
    "        DIR_FOLDER: folder that will contains the output file\n",
    "        file_extension: extension of the file\n",
    "        block_size: dimension of rows in main memory\n",
    "    \"\"\"\n",
    "    # Check if the input file path exists and is a file\n",
    "    if not file_input_path or not os.path.exists(file_input_path) or not os.path.isfile(file_input_path):\n",
    "        raise ValueError(\"Invalid file_input_path.\")\n",
    "\n",
    "    # Check if the output folder path exists\n",
    "    if not file_output_path:\n",
    "        raise ValueError(\"Invalid file_output_path.\")\n",
    "\n",
    "    # Check if DIR_FOLDER is a non-empty string\n",
    "    if not DIR_FOLDER or not isinstance(DIR_FOLDER, str):\n",
    "        raise ValueError(\"Invalid DIR_FOLDER.\")\n",
    "\n",
    "    # Check if the file extension is a non-empty string\n",
    "    if not file_extension or not isinstance(file_extension, str):\n",
    "        raise ValueError(\"Invalid file_extension.\")\n",
    "\n",
    "    # Check that block_size is a positive integer\n",
    "    if not isinstance(block_size, int) or block_size <= 0:\n",
    "        raise ValueError(\"Invalid block_size. Must be a positive integer.\")\n",
    "\n",
    "    # Check that document_index is an instance of DocumentIndex\n",
    "    if not document_index or not isinstance(document_index, doc_index.DocumentIndex):\n",
    "        raise ValueError(\"Invalid document_index. Must be an instance of DocumentIndex.\")\n",
    "        \n",
    "    try:\n",
    "        lexicon = Lexicon()\n",
    "        create_folder(DIR_FOLDER)\n",
    "        nr_block = 0\n",
    "        if os.path.exists(DIR_FOLDER + file_output_path + str(nr_block) + file_extension):\n",
    "                os.remove(DIR_FOLDER + file_output_path + str(nr_block) + file_extension)\n",
    "            \n",
    "        with open(file_input_path, 'r') as file:\n",
    "            for line in file:\n",
    "                # term sarà qualcosa tipo \"ciao\", invece la postings list sarà 3:2 3:3 ecc\n",
    "                elements = line.split()\n",
    "                term = elements[0]          \n",
    "                postings_list = ' '.join(elements[1:])\n",
    "                \n",
    "                # il dft si trova facendo la split su spazi e punti e virgola di tutta la posting list\n",
    "                dft = len(postings_list.split())\n",
    "\n",
    "                # la term frequency massima\n",
    "                max_tf = compute_max_term_frequency(postings_list)\n",
    "\n",
    "                if (sys.getsizeof(lexicon.get_structure()) > block_size):  #Free memory available\n",
    "                    write_to_block(DIR_FOLDER + file_output_path + str(nr_block) + file_extension, lexicon.get_structure())\n",
    "                    lexicon.clear_structure()\n",
    "                    nr_block=nr_block + 1 \n",
    "\n",
    "                lexicon.add_term(term, dft, document_index, max_tf)\n",
    "\n",
    "            #Finally, saving the last remaing block.       \n",
    "            if (not lexicon.is_empty()):   \n",
    "                write_to_block(DIR_FOLDER + file_output_path + str(nr_block) + file_extension, lexicon.get_structure())\n",
    "\n",
    "            return 0                \n",
    "    except IOError as e:\n",
    "        print(f\"Error reading from {file_input_path}: {e}\")\n",
    "        return -1\n",
    "'''\n",
    "     \n",
    "\n",
    "class Lexicon(util.Singleton):\n",
    "    def __init__(self):\n",
    "        self._vocabulary = defaultdict(lex_row.LexiconRow) # oppure \"dictionary\"??\n",
    "\n",
    "    def add_term(self, term: str, dft: int, document_index: doc_index.DocumentIndex, maxTf: int) -> None:\n",
    "        \"\"\"Adds a document to the lexicon.\"\"\"\n",
    "        if not isinstance(term, str) or not isinstance(dft, int) or not isinstance(document_index, doc_index.DocumentIndex) or not isinstance(maxTf, int):\n",
    "            raise ValueError(\"There's an error in parameter's type.\")\n",
    "            \n",
    "        # Append new row to the lexicon\n",
    "        if (self.get_terms(term)==None):\n",
    "            self._vocabulary[term]=[]\n",
    "        self._vocabulary[term] = lex_row.LexiconRow(term, dft, document_index, maxTf)\n",
    "             \n",
    "    def get_terms(self, term: str) -> lex_row.LexiconRow:\n",
    "        \"\"\"Fetches a row to the lexicon\"\"\"\n",
    "        if not isinstance(term, str):\n",
    "            raise ValueError(\"Term must be a string.\")\n",
    "            \n",
    "        if (term in self._vocabulary):\n",
    "            return self._vocabulary[term]\n",
    "        return None\n",
    "    \n",
    "    def is_empty(self)->bool:\n",
    "        \"\"\"Check if there is no term in the lexicon.\"\"\"\n",
    "        return len(self.get_term())==0\n",
    "    \n",
    "    def get_term(self) -> List[str]:\n",
    "        \"\"\"Returns all unique terms in the lexicon.\"\"\"\n",
    "        return self._vocabulary.keys() \n",
    "    \n",
    "    def clear_structure(self):\n",
    "        \"\"\" It clears the lexicon data structure.\"\"\"\n",
    "        self._vocabulary.clear()\n",
    "    \n",
    "    def get_structure(self):\n",
    "        \"\"\"Returns the lexicon data structure.\"\"\"\n",
    "        return self._vocabulary \n",
    "\n",
    "    def find_entry(self,term: str) -> lex_row.LexiconRow:\n",
    "        \"\"\"Perform binary search to find a lexicon entry for a given term.\n",
    "\n",
    "        Args:\n",
    "            term: The term to search for in the lexicon.\n",
    "    \n",
    "        Returns:\n",
    "            The LexiconRow object if the term is found, otherwise None.\n",
    "        \"\"\"\n",
    "        entry = lex_row.LexiconRow(\"\",0)  \n",
    "        start = 0 \n",
    "        \n",
    "        # \"end\" is equal (at the beginning) to the total number of distinct terms in the lexicon\n",
    "        collectionStatistics = coll_stat.Collection_statistics(\"Collection_statistics.txt\")\n",
    "        collectionStatistics.read_statistics()\n",
    "        end = collectionStatistics.num_distinct_terms - 1  \n",
    "    \n",
    "        while start <= end:\n",
    "            mid = start + (end - start) // 2\n",
    "    \n",
    "            # Get entry from disk\n",
    "\n",
    "            # Forse bisogna mettere come attributo della classe lexicon un \"lexicon_path\" (dopo averlo costruito) con il percorso del file finale?\n",
    "            # In questo modo qui basterebbe passare self.lexicon_path invece di \"LEXICON/lexicon.bin\"\n",
    "            lexicon_path = os.path.join(\"..\", \"building_data_structures\", \"Lexicon\", \"lexicon.bin\")\n",
    "            with open(lexicon_path, 'rb') as file:\n",
    "                entry.read_lexicon_row_on_disk_from_opened_file(file, mid * entry.SIZE_LEXICON_ROW)\n",
    "            key = entry.term.strip()\n",
    "            print(key)\n",
    "            print(mid)\n",
    "            \n",
    "            # Check if the search was successful\n",
    "            if key == term:\n",
    "                return entry\n",
    "    \n",
    "            # Update search portion parameters\n",
    "            if term > key:\n",
    "                start = mid + 1\n",
    "            else:\n",
    "                end = mid - 1\n",
    "    \n",
    "        return None\n",
    "\n",
    "    def get_entry(self, term: str) -> lex_row.LexiconRow:\n",
    "        entry = self.get_terms(term) # check if term is in cache\n",
    "        if entry is not None:\n",
    "            return entry\n",
    "            \n",
    "        entry = find_entry(term)\n",
    "        \n",
    "        if entry is not None:         # add to cache\n",
    "            self.add_term(term)\n",
    "        \n",
    "        return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5bfdaf6-977c-482e-8774-9c86d1680bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like\n",
      "104\n",
      "early\n",
      "51\n",
      "have\n",
      "77\n",
      "for\n",
      "64\n",
      "golden\n",
      "70\n",
      "great\n",
      "73\n",
      "has\n",
      "75\n",
      "happiness\n",
      "74\n",
      "<structures.LexiconRow.LexiconRow object at 0x0000029891FBC1D0>\n"
     ]
    }
   ],
   "source": [
    "#lexicon = Lexicon()\n",
    "#print(lexicon.find_entry(\"happiness\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
