{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2e21d95-98d7-45f1-937f-b59f775699bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from D:\\GitHub\\structures\\..\\structures\\LexiconRow.ipynb\n",
      "importing Jupyter notebook from D:\\GitHub\\structures\\..\\structures\\DocumentIndex.ipynb\n",
      "importing Jupyter notebook from D:\\GitHub\\structures\\..\\utilities\\General_Utilities.ipynb\n",
      "importing Jupyter notebook from D:\\GitHub\\structures\\..\\structures\\DocumentIndexRow.ipynb\n",
      "importing Jupyter notebook from D:\\GitHub\\structures\\..\\building_data_structures\\CollectionStatistics.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import struct\n",
    "\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, TextIO, BinaryIO\n",
    "\n",
    "import import_ipynb\n",
    "import sys\n",
    "sys.path.append('../')  # Go up two folders to the project root\n",
    "\n",
    "from structures.LexiconRow import LexiconRow\n",
    "from structures.DocumentIndex import DocumentIndex\n",
    "from utilities.General_Utilities import Singleton\n",
    "from building_data_structures.CollectionStatistics import Collection_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62a61078-8c2e-484d-bb56-5ef964997cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIR_LEXICON=\"../building_data_structures/LEXICON\"\n",
    "# PATH_FINAL_LEXICON=\"lexicon.bin\"\n",
    "\n",
    "# '''\n",
    "# def create_lexicon(file_input_path: str, file_output_path: str, DIR_FOLDER: str, file_extension: str, block_size: int, document_index: doc_index.DocumentIndex) -> int:\n",
    "#     \"\"\"\n",
    "#     Function returns a file with one row for each distinct term in the corpus. Rows are composed by:\n",
    "#     term, document frequency, inverse document frequency, term upper bound\n",
    "#     Each values is separated by a comma.\n",
    "\n",
    "#     Args:\n",
    "#         file_input_path: file that contains the inverted index\n",
    "#         file_output_path: file that will contains the result\n",
    "#         DIR_FOLDER: folder that will contains the output file\n",
    "#         file_extension: extension of the file\n",
    "#         block_size: dimension of rows in main memory\n",
    "#     \"\"\"\n",
    "#     # Check if the input file path exists and is a file\n",
    "#     if not file_input_path or not os.path.exists(file_input_path) or not os.path.isfile(file_input_path):\n",
    "#         raise ValueError(\"Invalid file_input_path.\")\n",
    "\n",
    "#     # Check if the output folder path exists\n",
    "#     if not file_output_path:\n",
    "#         raise ValueError(\"Invalid file_output_path.\")\n",
    "\n",
    "#     # Check if DIR_FOLDER is a non-empty string\n",
    "#     if not DIR_FOLDER or not isinstance(DIR_FOLDER, str):\n",
    "#         raise ValueError(\"Invalid DIR_FOLDER.\")\n",
    "\n",
    "#     # Check if the file extension is a non-empty string\n",
    "#     if not file_extension or not isinstance(file_extension, str):\n",
    "#         raise ValueError(\"Invalid file_extension.\")\n",
    "\n",
    "#     # Check that block_size is a positive integer\n",
    "#     if not isinstance(block_size, int) or block_size <= 0:\n",
    "#         raise ValueError(\"Invalid block_size. Must be a positive integer.\")\n",
    "\n",
    "#     # Check that document_index is an instance of DocumentIndex\n",
    "#     if not document_index or not isinstance(document_index, doc_index.DocumentIndex):\n",
    "#         raise ValueError(\"Invalid document_index. Must be an instance of DocumentIndex.\")\n",
    "        \n",
    "#     try:\n",
    "#         lexicon = Lexicon()\n",
    "#         create_folder(DIR_FOLDER)\n",
    "#         nr_block = 0\n",
    "#         if os.path.exists(DIR_FOLDER + file_output_path + str(nr_block) + file_extension):\n",
    "#                 os.remove(DIR_FOLDER + file_output_path + str(nr_block) + file_extension)\n",
    "            \n",
    "#         with open(file_input_path, 'r') as file:\n",
    "#             for line in file:\n",
    "#                 # term sarà qualcosa tipo \"ciao\", invece la postings list sarà 3:2 3:3 ecc\n",
    "#                 elements = line.split()\n",
    "#                 term = elements[0]          \n",
    "#                 postings_list = ' '.join(elements[1:])\n",
    "                \n",
    "#                 # il dft si trova facendo la split su spazi e punti e virgola di tutta la posting list\n",
    "#                 dft = len(postings_list.split())\n",
    "\n",
    "#                 # la term frequency massima\n",
    "#                 max_tf = compute_max_term_frequency(postings_list)\n",
    "\n",
    "#                 if (sys.getsizeof(lexicon.get_structure()) > block_size):  #Free memory available\n",
    "#                     write_to_block(DIR_FOLDER + file_output_path + str(nr_block) + file_extension, lexicon.get_structure())\n",
    "#                     lexicon.clear_structure()\n",
    "#                     nr_block=nr_block + 1 \n",
    "\n",
    "#                 lexicon.add_term(term, dft, document_index, max_tf)\n",
    "\n",
    "#             #Finally, saving the last remaing block.       \n",
    "#             if (not lexicon.is_empty()):   \n",
    "#                 write_to_block(DIR_FOLDER + file_output_path + str(nr_block) + file_extension, lexicon.get_structure())\n",
    "\n",
    "#             return 0                \n",
    "#     except IOError as e:\n",
    "#         print(f\"Error reading from {file_input_path}: {e}\")\n",
    "#         return -1\n",
    "# '''\n",
    "     \n",
    "\n",
    "# class Lexicon(Singleton):\n",
    "#     def __init__(self):\n",
    "#         self._vocabulary = defaultdict(LexiconRow)\n",
    "        \n",
    "#         self.collectionStatistics = Collection_statistics(\"../building_data_structures/DOC_INDEX/collection_statistics.bin\")\n",
    "#         self.collectionStatistics.read_binary_mode()\n",
    "\n",
    "#     def add_term(self, term: str) -> None:\n",
    "#         \"\"\"Adds a document to the lexicon.\"\"\"\n",
    "#         if not isinstance(term, str):\n",
    "#             raise ValueError(\"There's an error in parameter's type.\")\n",
    "            \n",
    "#         # Append new row to the lexicon\n",
    "#         if (self.get_terms(term)==None):\n",
    "#             self._vocabulary[term]=[]\n",
    "#         self._vocabulary[term] = LexiconRow(term)\n",
    "\n",
    "#     def add_term(self, lex_row: LexiconRow) -> None:\n",
    "#         if (self.get_terms(lex_row.term)==None):\n",
    "#             self._vocabulary[lex_row.term]=[]\n",
    "#         self._vocabulary[lex_row.term] = lex_row\n",
    "             \n",
    "#     def get_terms(self, term: str) -> LexiconRow:\n",
    "#         \"\"\"Fetches a row to the lexicon\"\"\"\n",
    "#         if not isinstance(term, str):\n",
    "#             raise ValueError(\"Term must be a string.\")\n",
    "            \n",
    "#         if (term in self._vocabulary):\n",
    "#             return self._vocabulary[term]\n",
    "#         return None\n",
    "    \n",
    "#     def is_empty(self)->bool:\n",
    "#         \"\"\"Check if there is no term in the lexicon.\"\"\"\n",
    "#         return len(self.get_term())==0\n",
    "    \n",
    "#     def get_term(self) -> List[str]:\n",
    "#         \"\"\"Returns all unique terms in the lexicon.\"\"\"\n",
    "#         return self._vocabulary.keys() \n",
    "    \n",
    "#     def clear_structure(self):\n",
    "#         \"\"\" It clears the lexicon data structure.\"\"\"\n",
    "#         self._vocabulary.clear()\n",
    "    \n",
    "#     def get_structure(self):\n",
    "#         \"\"\"Returns the lexicon data structure.\"\"\"\n",
    "#         return self._vocabulary \n",
    "\n",
    "#     def find_entry(self,term: str) -> LexiconRow:\n",
    "#         \"\"\"Perform binary search to find a lexicon entry for a given term.\n",
    "\n",
    "#         Args:\n",
    "#             term: The term to search for in the lexicon.\n",
    "    \n",
    "#         Returns:\n",
    "#             The LexiconRow object if the term is found, otherwise None.\n",
    "#         \"\"\"\n",
    "#         entry = LexiconRow(\"\",0)  \n",
    "#         start = 0 \n",
    "        \n",
    "#         # \"end\" is equal (at the beginning) to the total number of distinct terms in the lexicon\n",
    "#         end = self.collectionStatistics.num_distinct_terms - 1  \n",
    "    \n",
    "#         while start <= end:\n",
    "#             mid = start + (end - start) // 2\n",
    "            \n",
    "#             # Get entry from disk\n",
    "#             with open(DIR_LEXICON+ \"/\" + PATH_FINAL_LEXICON, 'rb') as file:\n",
    "#                 entry.read_lexicon_row_on_disk_from_opened_file(file, mid * entry.SIZE_LEXICON_ROW)\n",
    "#             key = entry.term.strip()\n",
    "            \n",
    "#             # Check if the search was successful\n",
    "#             if key == term:\n",
    "#                 return entry\n",
    "    \n",
    "#             # Update search portion parameters\n",
    "#             if term > key:\n",
    "#                 start = mid + 1\n",
    "#             else:\n",
    "#                 end = mid - 1\n",
    "    \n",
    "#         return None\n",
    "\n",
    "#     # come svuotare la cache? vanno implementate altre funzionalità?\n",
    "#     def get_entry(self, term: str) -> LexiconRow:\n",
    "#         entry = self.get_terms(term) # check if term is in cache\n",
    "#         if entry is not None:\n",
    "#             return entry\n",
    "            \n",
    "#         entry = self.find_entry(term)\n",
    "        \n",
    "#         if entry is not None:         # add to cache\n",
    "#             self.add_term(entry)\n",
    "        \n",
    "#         return entry\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7271659d-7663-40c5-94e7-53ddde1405f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "DIR_LEXICON=\"../building_data_structures/LEXICON\"\n",
    "PATH_FINAL_LEXICON=\"lexicon.bin\"\n",
    "\n",
    "class Lexicon:\n",
    "    collectionStatistics = Collection_statistics(\"../building_data_structures/DOC_INDEX/collection_statistics.bin\")\n",
    "    collectionStatistics.read_binary_mode()\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self._vocabulary = OrderedDict()\n",
    "\n",
    "    def add_term(self, lex_row: LexiconRow) -> None:\n",
    "        if lex_row.term not in self._vocabulary:\n",
    "            if len(self._vocabulary) >= self.capacity:\n",
    "                self._vocabulary.popitem(last=False) # delete less recent element\n",
    "\n",
    "            self._vocabulary[lex_row.term] = lex_row\n",
    "\n",
    "    def get_terms(self, term: str) -> LexiconRow:\n",
    "        \"\"\"Fetches a row to the lexicon\"\"\"\n",
    "        return self._vocabulary.get(term, None)\n",
    "\n",
    "    def is_empty(self)->bool:\n",
    "        \"\"\"Check if there is no term in the lexicon.\"\"\"\n",
    "        return len(self.get_term())==0\n",
    "\n",
    "    def clear_structure(self):\n",
    "        \"\"\" It clears the lexicon data structure.\"\"\"\n",
    "        self._vocabulary.clear()\n",
    "    \n",
    "    def get_structure(self):\n",
    "        \"\"\"Returns the lexicon data structure.\"\"\"\n",
    "        return self._vocabulary \n",
    "\n",
    "    def find_entry(self,term: str) -> LexiconRow:\n",
    "        \"\"\"Perform binary search to find a lexicon entry for a given term.\n",
    "\n",
    "        Args:\n",
    "            term: The term to search for in the lexicon.\n",
    "    \n",
    "        Returns:\n",
    "            The LexiconRow object if the term is found, otherwise None.\n",
    "        \"\"\"\n",
    "        entry = LexiconRow(\"\",0)  \n",
    "        start = 0 \n",
    "        \n",
    "        # \"end\" is equal (at the beginning) to the total number of distinct terms in the lexicon\n",
    "        end = self.collectionStatistics.num_distinct_terms - 1  \n",
    "    \n",
    "        while start <= end:\n",
    "            mid = start + (end - start) // 2\n",
    "            \n",
    "            # Get entry from disk\n",
    "            with open(DIR_LEXICON+ \"/\" + PATH_FINAL_LEXICON, 'rb') as file:\n",
    "                entry.read_lexicon_row_on_disk_from_opened_file(file, mid * entry.SIZE_LEXICON_ROW)\n",
    "            key = entry.term.strip()\n",
    "            \n",
    "            # Check if the search was successful\n",
    "            if key == term:\n",
    "                return entry\n",
    "    \n",
    "            # Update search portion parameters\n",
    "            if term > key:\n",
    "                start = mid + 1\n",
    "            else:\n",
    "                end = mid - 1\n",
    "    \n",
    "        return None\n",
    "\n",
    "    # come svuotare la cache? vanno implementate altre funzionalità?\n",
    "    def get_entry(self, term: str) -> LexiconRow:\n",
    "        entry = self.get_terms(term) # check if term is in cache\n",
    "        if entry is not None:\n",
    "            return entry\n",
    "            \n",
    "        entry = self.find_entry(term)\n",
    "        \n",
    "        if entry is not None:         # add to cache\n",
    "            self.add_term(entry)\n",
    "        \n",
    "        return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c961989-3442-41ba-b8ae-71a77afea01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esempio di utilizzo:\n",
    "# vocabulary_cache = Lexicon(2)\n",
    "# vocabulary_cache.get_entry(\"happiness\")\n",
    "# print(vocabulary_cache.get_structure())\n",
    "# print(\"\\n\")\n",
    "\n",
    "# vocabulary_cache.get_entry(\"dog\")\n",
    "# print(vocabulary_cache.get_structure())\n",
    "# print(\"\\n\")\n",
    "\n",
    "# vocabulary_cache.get_entry(\"cat\")\n",
    "# print(vocabulary_cache.get_structure())\n",
    "# print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
