{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2e21d95-98d7-45f1-937f-b59f775699bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import struct\n",
    "\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List\n",
    "from typing import TextIO, BinaryIO\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a0ddda5-66d5-4389-af77-2799cd5c241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Singleton:\n",
    "    # Private variable to hold the unique instance of the class\n",
    "    _instance = None\n",
    " \n",
    "    \"\"\"\n",
    "        Checks if the instance already exists and returns the existing instance.\n",
    "        If the instance does not exist, it creates a new instance and returns it.\n",
    "    \"\"\"\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        if not cls._instance:\n",
    "            cls._instance = super(Singleton, cls).__new__(cls, *args, **kwargs)\n",
    "            cls._instance._index = None\n",
    "        return cls._instance\n",
    "        \n",
    "def create_folder(folder_name: str) -> None :\n",
    "    \"\"\" Create a folder called \"folder_name\" \"\"\"\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e6a0b54-6b97-4754-80a1-98a1794f93b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentIndexRow:\n",
    "    def __init__(self, doc_no: int, text: str) -> None:\n",
    "        '''\n",
    "            This constructor receives a document number and the content of the document, and save\n",
    "            the first parameter and the length of the second (that represents the document length).\n",
    "        '''\n",
    "        if not isinstance(doc_no, int) or not isinstance(text, str):\n",
    "            raise ValueError(\"doc_no must be an integer and text must be a string.\")\n",
    "        \n",
    "        self.doc_id = doc_no\n",
    "        self.document_length = self.count_words(text)\n",
    "\n",
    "    def count_words(self, text: str):\n",
    "        \"\"\"Receives a document and counts how many tokens it contains.\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            raise ValueError(\"text must be a string.\")\n",
    "\n",
    "        # empty string\n",
    "        if not text.strip():\n",
    "            return 0\n",
    "            \n",
    "        return len(text.split())\n",
    "    \n",
    "    def to_string(self):    \n",
    "        \"\"\" Join in a string the content of the row. \"\"\"\n",
    "        string = ' '.join([str(self.doc_id) , str(self.document_length)])\n",
    "        return string  \n",
    "\n",
    "class DocumentIndex(Singleton):\n",
    "    def __init__(self):\n",
    "        if self._index is None:\n",
    "            self._index = defaultdict(DocumentIndexRow)\n",
    "            self.number_of_documents = 0 \n",
    "            self.total_document_length = 0\n",
    "\n",
    "    def add_document(self, doc_id: int, text: str) -> None:\n",
    "        \"\"\"Adds a document to the document index.\"\"\"\n",
    "        if not isinstance(doc_id, int) or not isinstance(text, str):\n",
    "            raise ValueError(\"doc_id must be an integer and text must be a string.\")\n",
    "            \n",
    "        if (self.get_document(doc_id)==None):\n",
    "            self._index[doc_id]=[]\n",
    "        row = DocumentIndexRow(doc_id,text)\n",
    "        self._index[doc_id] = row\n",
    "\n",
    "        # Update the statistics about total number of documents in the document index and total document length\n",
    "        self.number_of_documents = self.number_of_documents + 1 \n",
    "        self.total_document_length = self.total_document_length + row.document_length\n",
    "             \n",
    "    def get_document(self, doc_id: int) -> DocumentIndexRow:\n",
    "        \"\"\"Fetches a row from the document index\"\"\"\n",
    "        if not isinstance(doc_id, int):\n",
    "            raise ValueError(\"doc_id must be an integer.\")\n",
    "            \n",
    "        if (doc_id in self._index):\n",
    "            return self._index[doc_id]\n",
    "        return None\n",
    "    \n",
    "    def is_empty(self) -> bool:\n",
    "        \"\"\"Check if there are no documents in the document index.\"\"\"\n",
    "        return len(self.get_document_ids()) == 0\n",
    "    \n",
    "    def get_document_ids(self) -> List[str]:\n",
    "        \"\"\"Returns all unique document IDs in the index.\"\"\"\n",
    "        return list(self._index.keys()) \n",
    "    \n",
    "    def clear_structure(self):\n",
    "        \"\"\" It clears the document index data structure.\"\"\"\n",
    "        self._index.clear()\n",
    "        self.number_of_documents = 0\n",
    "        self.total_document_length = 0\n",
    "    \n",
    "    def get_structure(self):\n",
    "        \"\"\"Returns the document index data structure.\"\"\"\n",
    "        return self._index\n",
    "    \n",
    "    # scrivo su disco, sul file \"file_name\" il contenuto della struttura dati \"struct\"\n",
    "    def write_document_index_to_file(self,file_name: str, struct: defaultdict) -> None:\n",
    "        \"\"\" Write to the disk, to the file 'file_name', the content of the data structure 'struct'.\"\"\"\n",
    "        with open(file_name, \"a\") as f:\n",
    "            for index, term in enumerate(struct.keys()):\n",
    "                f.write(struct[term].to_string())\n",
    "\n",
    "                if index != len(struct.keys()) - 1:\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62a61078-8c2e-484d-bb56-5ef964997cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_max_term_frequency(postings_list: str) -> int:\n",
    "    \"\"\"\n",
    "    Given a postings list of a term, compute the maximum term frequency.\n",
    "\n",
    "    Args:\n",
    "        postings_list: A string containing elements with colon-separated values.\n",
    "    \"\"\"\n",
    "    if not isinstance(postings_list, str):\n",
    "        raise ValueError(\"Invalid postings list.\")\n",
    "\n",
    "    if len(postings_list) == 0:\n",
    "        return 0\n",
    "        \n",
    "    # Split the postings list into individual elements\n",
    "    postings_elements = postings_list.split()\n",
    "\n",
    "    # Initialize the maximum value with the value from the first element\n",
    "    max = int(postings_elements[0].split(':')[1])\n",
    "\n",
    "    # Iterate through each element and find the maximum value after the colon\n",
    "    for item in postings_elements:\n",
    "        # Split each element to extract the value after the colon\n",
    "        parts = item.split(':')\n",
    "        value = int(parts[1])\n",
    "        \n",
    "        # Compare the extracted value with the current maximum value and update if necessary\n",
    "        if value > max:\n",
    "            max = value\n",
    "\n",
    "    return max\n",
    "\n",
    "\n",
    "def compute_TFIDF(tf: int, idf: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute the TF-IDF value based on the term frequency (tf) and inverse document frequency (idf).\n",
    "    \n",
    "    Args:\n",
    "        tf: An integer representing the term frequency.\n",
    "        idf: A float representing the inverse document frequency.\n",
    "    \"\"\"\n",
    "    if not isinstance(tf, int) or not isinstance(idf, float):\n",
    "        raise ValueError(\"Invalid parameters.\")\n",
    "\n",
    "    if tf < 0:\n",
    "        return 0\n",
    "        \n",
    "    return (1 + math.log(tf)) * idf\n",
    "\n",
    "def compute_IDFT(number_of_documents: int, dft:int) -> float:\n",
    "    \"\"\"\n",
    "    Compute the inverse document frequency for a term based on the document index (doc_index) and document frequency (dft).\n",
    "\n",
    "    Args:\n",
    "        doc_index: An instance of DocumentIndex representing the document index.\n",
    "        dft: The document frequency of the term.\n",
    "    \"\"\"\n",
    "    if not isinstance(number_of_documents, int) or not isinstance(dft, int):\n",
    "        raise ValueError(\"Invalid parameters.\")\n",
    "\n",
    "    if dft < 0 or number_of_documents < 0:\n",
    "        return 0\n",
    "    \n",
    "    return math.log(number_of_documents/dft)  \n",
    "\n",
    "def compute_avgDL(doc_index: DocumentIndex) -> float:\n",
    "    \"\"\"\n",
    "    Compute the average document length based on the document index (doc_index).\n",
    "\n",
    "    Args:\n",
    "        doc_index: An instance of DocumentIndex representing the document index.\n",
    "    \"\"\"\n",
    "    if not isinstance(doc_index, DocumentIndex):\n",
    "        raise ValueError(\"Invalid parameters.\")\n",
    "        \n",
    "    return doc_index.total_document_length / doc_index.number_of_documents\n",
    "\n",
    "def compute_BM25_term(doc_index: DocumentIndex, doc_id: int, term_freq:int, k1:float = 1.6, b:float = 0.75)-> float:\n",
    "    if not isinstance(doc_index, DocumentIndex) or not isinstance(doc_id, int) or not isinstance(term_freq, int):\n",
    "        raise ValueError(\"Invalid parameters.\") \n",
    "        \n",
    "    if doc_id < 0 or term_freq < 0:\n",
    "        raise ValueError(\"doc_id and term_freq must be positive\")\n",
    "        \n",
    "    idf = compute_IDFT(doc_index.number_of_documents, term_freq)\n",
    "    avgDL = compute_avgDL(doc_index)\n",
    "    log_tf = (1 + math.log(term_freq))\n",
    "    doc_len = doc_index.get_document(doc_id).document_length\n",
    "\n",
    "    return (idf * log_tf)/(log_tf + k1 * ( (1 - b) + b * (doc_len/avgDL) ))\n",
    "\n",
    "def computer_BM25_query(query:str, doc_index: DocumentIndex, doc_id: int, idf: float) -> float:\n",
    "    bm25 = 0.0\n",
    "    \n",
    "    for term in query:\n",
    "        #term_freq =  come si trova la term frequencies? bisogna recuperare la postings list dal doc_index?\n",
    "        bm25 = bm25 + compute_BM25_term(doc_index, doc_id, term_freq)\n",
    "\n",
    "    return bm25\n",
    "\n",
    "\n",
    "def create_lexicon(file_input_path: str, file_output_path: str, DIR_FOLDER: str, file_extension: str, block_size: int, document_index: DocumentIndex) -> int:\n",
    "    \"\"\"\n",
    "    Function returns a file with one row for each distinct term in the corpus. Rows are composed by:\n",
    "    term, document frequency, inverse document frequency, term upper bound\n",
    "    Each values is separated by a comma.\n",
    "\n",
    "    Args:\n",
    "        file_input_path: file that contains the inverted index\n",
    "        file_output_path: file that will contains the result\n",
    "        DIR_FOLDER: folder that will contains the output file\n",
    "        file_extension: extension of the file\n",
    "        block_size: dimension of rows in main memory\n",
    "    \"\"\"\n",
    "    # Check if the input file path exists and is a file\n",
    "    if not file_input_path or not os.path.exists(file_input_path) or not os.path.isfile(file_input_path):\n",
    "        raise ValueError(\"Invalid file_input_path.\")\n",
    "\n",
    "    # Check if the output folder path exists\n",
    "    if not file_output_path:\n",
    "        raise ValueError(\"Invalid file_output_path.\")\n",
    "\n",
    "    # Check if DIR_FOLDER is a non-empty string\n",
    "    if not DIR_FOLDER or not isinstance(DIR_FOLDER, str):\n",
    "        raise ValueError(\"Invalid DIR_FOLDER.\")\n",
    "\n",
    "    # Check if the file extension is a non-empty string\n",
    "    if not file_extension or not isinstance(file_extension, str):\n",
    "        raise ValueError(\"Invalid file_extension.\")\n",
    "\n",
    "    # Check that block_size is a positive integer\n",
    "    if not isinstance(block_size, int) or block_size <= 0:\n",
    "        raise ValueError(\"Invalid block_size. Must be a positive integer.\")\n",
    "\n",
    "    # Check that document_index is an instance of DocumentIndex\n",
    "    if not document_index or not isinstance(document_index, DocumentIndex):\n",
    "        raise ValueError(\"Invalid document_index. Must be an instance of DocumentIndex.\")\n",
    "        \n",
    "    try:\n",
    "        lexicon = Lexicon()\n",
    "        create_folder(DIR_FOLDER)\n",
    "        nr_block = 0\n",
    "        if os.path.exists(DIR_FOLDER + file_output_path + str(nr_block) + file_extension):\n",
    "                os.remove(DIR_FOLDER + file_output_path + str(nr_block) + file_extension)\n",
    "            \n",
    "        with open(file_input_path, 'r') as file:\n",
    "            for line in file:\n",
    "                # term sarà qualcosa tipo \"ciao\", invece la postings list sarà 3:2 3:3 ecc\n",
    "                elements = line.split()\n",
    "                term = elements[0]          \n",
    "                postings_list = ' '.join(elements[1:])\n",
    "                \n",
    "                # il dft si trova facendo la split su spazi e punti e virgola di tutta la posting list\n",
    "                dft = len(postings_list.split())\n",
    "\n",
    "                # la term frequency massima\n",
    "                max_tf = compute_max_term_frequency(postings_list)\n",
    "\n",
    "                if (sys.getsizeof(lexicon.get_structure()) > block_size):  #Free memory available\n",
    "                    write_to_block(DIR_FOLDER + file_output_path + str(nr_block) + file_extension, lexicon.get_structure())\n",
    "                    lexicon.clear_structure()\n",
    "                    nr_block=nr_block + 1 \n",
    "\n",
    "                lexicon.add_term(term, dft, document_index, max_tf)\n",
    "\n",
    "            #Finally, saving the last remaing block.       \n",
    "            if (not lexicon.is_empty()):   \n",
    "                write_to_block(DIR_FOLDER + file_output_path + str(nr_block) + file_extension, lexicon.get_structure())\n",
    "\n",
    "            return 0                \n",
    "    except IOError as e:\n",
    "        print(f\"Error reading from {file_input_path}: {e}\")\n",
    "        return -1\n",
    "        \n",
    "    \n",
    "class LexiconRow:\n",
    "    \n",
    "    STR_SIZE_LEXICON_ROW='30s 10i'\n",
    "    SIZE_LEXICON_ROW=struct.calcsize(STR_SIZE_LEXICON_ROW)\n",
    "    \n",
    "    def __init__(self, term: str, dft: int, max_tf: int=0, bm25dl:int=0, BM25Tf:int=0,\n",
    "                 docidOffset:int=0,frequencyOffset:int=0,docidSize:int=0,frequencySize:int=0,numBlocks:int=1,blockOffset:int=0):\n",
    "        \n",
    "        self.term = term if (len(term)<30) else term[:30]\n",
    "        self.term = self.term.ljust(30)\n",
    "        \n",
    "        # Document frequency of the term\n",
    "        self.dft = dft\n",
    "\n",
    "        # Inverse of document frequency of the term.              \n",
    "        self.idft = 0\n",
    "        #compute_IDFT(doc_index.number_of_documents, dft)\n",
    "\n",
    "        # Max term frequency\n",
    "        self.max_tf = max_tf\n",
    "\n",
    "        # Max tfidf\n",
    "        self.maxTFIDF = 0\n",
    "        #compute_TFIDF(max_tf, self.idft)\n",
    "    \n",
    "        \n",
    "        self.docidOffset=docidOffset\n",
    "        self.frequencyOffset=frequencyOffset\n",
    "        self.docidSize=0\n",
    "        self.frequencySize=0\n",
    "        self.numBlocks=0\n",
    "        self.blockOffset=0\n",
    "\n",
    "#     def __init__(self, term: str, dft: int, doc_index: DocumentIndex, max_tf: int):\n",
    "#         if not isinstance(term, str) or not isinstance(doc_index, DocumentIndex):\n",
    "#             raise ValueError(\"term must be a string and doc_index a DocumentIndex.\")\n",
    "#         if not isinstance(dft, int) or not isinstance(max_tf, int):\n",
    "#             raise ValueError(\"dft and max_tf must be integers.\")\n",
    "\n",
    "#         self.term = term\n",
    "\n",
    "#         # Document frequency of the term\n",
    "#         self.dft = dft\n",
    "\n",
    "#         # Inverse of document frequency of the term.              \n",
    "#         self.idft = compute_IDFT(doc_index.number_of_documents, dft)\n",
    "\n",
    "#         # Max term frequency\n",
    "#         self.max_tf = max_tf\n",
    "\n",
    "#         # Max tfidf\n",
    "#         self.maxTFIDF = compute_TFIDF(max_tf, self.idft)\n",
    "\n",
    "    def to_string(self):\n",
    "        string = ' '.join([str(self.term) , str(self.dft) , str(self.idft), str(self.max_tf), str(self.maxTFIDF)])\n",
    "        return string \n",
    "    \n",
    "    \n",
    "    def to_string(self):\n",
    "        \"\"\"This function returns a string representation of a LexiconRow.\n",
    "        \n",
    "        Returns:\n",
    "            a human readable string representation of the Lexicon Row\n",
    "        \"\"\"\n",
    "        string = ' '.join([str(self.term) , str(self.dft) , str(self.idft), str(self.max_tf), str(self.maxTFIDF)])\n",
    "        return string   \n",
    "    \n",
    "    def write_lexicon_row_on_disk_to_opened_file(self,file:BinaryIO,offset:int=0):\n",
    "        \"\"\"This function writes on a specific position of an opened file a lexicon row information.\n",
    "           \n",
    "           Args:\n",
    "               file: the file to store the lexicon row\n",
    "               offset: the position inside the file to store the lexicon row\n",
    "           Returns:\n",
    "               the new offset free position after writing on the file\n",
    "        \"\"\"\n",
    "        \n",
    "        file.seek(offset)\n",
    "       \n",
    "        binary_data = struct.pack(self.STR_SIZE_LEXICON_ROW, self.term.encode('utf-8'),self.dft, \n",
    "                                      self.idft,self.max_tf, self.maxTFIDF, self.docidOffset,\n",
    "                                      self.frequencyOffset,self.docidSize,self.frequencySize,\n",
    "                                      self.numBlocks,self.blockOffset)\n",
    "        file.write(binary_data)\n",
    "            \n",
    "        return self.SIZE_LEXICON_ROW+offset\n",
    "        \n",
    "    def read_lexicon_row_on_disk_from_opened_file(self,file:BinaryIO,offset:int):\n",
    "        \"\"\"This function reads a lexicon row informations in a specific position from an opened file.\n",
    "        \n",
    "        Args:\n",
    "            file: the file to read a lexicon row\n",
    "            offset: the position inside the file to read the lexicon row\n",
    "        \n",
    "        Returns:\n",
    "            the offset position after reading\n",
    "            \n",
    "        \"\"\"\n",
    "        file.seek(offset)  \n",
    "        bytesLetti = file.read(self.SIZE_LEXICON_ROW)\n",
    "        \n",
    "        if(not bytesLetti):\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            term,dft,idft,max_tf,maxTFIDF,docidOffset,frequencyOffset,docidSize,frequencySize,numBlocks,blockOffset = struct.unpack(self.STR_SIZE_LEXICON_ROW, bytesLetti)\n",
    "            termine=term.decode('utf-8')\n",
    "            #oggetto = LexiconRow(termine,dft,idft,max_tf,maxTFIDF,docidOffset,frequencyOffset,docidSize,frequencySize,numBlocks,blockOffset)\n",
    "            \n",
    "            self.term=termine\n",
    "            self.dft=dft\n",
    "            self.idft=idft\n",
    "            self.max_tf=max_tf\n",
    "            self.maxTFIDF=maxTFIDF\n",
    "            self.docidOffset=docidOffset\n",
    "            self.frequencyOffset=frequencyOffset\n",
    "            self.docidSize=docidSize\n",
    "            self.frequencySize=frequencySize\n",
    "            self.numBlocks=numBlocks\n",
    "            self.blockOffset=blockOffset\n",
    "            \n",
    "            \n",
    "        except struct.error as e:\n",
    "            print(f\"Error unpacking data: {e}\")\n",
    "            \n",
    "        return offset+self.SIZE_LEXICON_ROW\n",
    "    \n",
    "   \n",
    "    #USED FOR DEBUGGING\n",
    "    \n",
    "    def write_lexicon_row_on_disk(self,file_path:str,offset:int=0):\n",
    "        \"\"\"This function opens a file and writes on a specific position a lexicon row information.\n",
    "            This is used for debug and tests.\n",
    "        \n",
    "            Args:\n",
    "               file_path: the file to store the lexicon row\n",
    "               offset: the position inside the file to store the lexicon row\n",
    "            Returns:\n",
    "                the new offset free position after writing\n",
    "               \n",
    "        \"\"\"\n",
    "        with open(file_path, 'ab') as file:\n",
    "            return self.write_lexicon_row_on_disk_to_opened_file(file,offset)\n",
    "            \n",
    "    def read_lexicon_row_on_disk(self,file_path:str,offset:int):\n",
    "        \"\"\"This function opens a file and reads in a specific position a lexicon row information.\n",
    "            This is used for debug and tests.\n",
    "        \n",
    "            Args:\n",
    "               file_path: the file to read a lexicon row\n",
    "               offset: the position inside the file to read the lexicon row\n",
    "            Returns:\n",
    "                the offset position after reading\n",
    "        \"\"\"\n",
    "        with open(file_path, 'rb') as file:\n",
    "            return read_lexicon_row_on_disk_from_opened_file(file_path,offset)\n",
    "    \n",
    "     \n",
    "\n",
    "class Lexicon(Singleton):\n",
    "    def __init__(self):\n",
    "        self._vocabulary = defaultdict(LexiconRow) # oppure \"dictionary\"??\n",
    "\n",
    "    def add_term(self, term: str, dft: int, document_index: DocumentIndex, maxTf: int) -> None:\n",
    "        \"\"\"Adds a document to the lexicon.\"\"\"\n",
    "        if not isinstance(term, str) or not isinstance(dft, int) or not isinstance(document_index, DocumentIndex) or not isinstance(maxTf, int):\n",
    "            raise ValueError(\"There's an error in parameter's type.\")\n",
    "            \n",
    "        # Append new row to the lexicon\n",
    "        if (self.get_terms(term)==None):\n",
    "            self._vocabulary[term]=[]\n",
    "        self._vocabulary[term] = LexiconRow(term, dft, document_index, maxTf)\n",
    "             \n",
    "    def get_terms(self, term: str) -> LexiconRow:\n",
    "        \"\"\"Fetches a row to the lexicon\"\"\"\n",
    "        if not isinstance(term, str):\n",
    "            raise ValueError(\"Term must be a string.\")\n",
    "            \n",
    "        if (term in self._vocabulary):\n",
    "            return self._vocabulary[term]\n",
    "        return None\n",
    "    \n",
    "    def is_empty(self)->bool:\n",
    "        \"\"\"Check if there is no term in the lexicon.\"\"\"\n",
    "        return len(self.get_term())==0\n",
    "    \n",
    "    def get_term(self) -> List[str]:\n",
    "        \"\"\"Returns all unique terms in the lexicon.\"\"\"\n",
    "        return self._vocabulary.keys() \n",
    "    \n",
    "    def clear_structure(self):\n",
    "        \"\"\" It clears the lexicon data structure.\"\"\"\n",
    "        self._vocabulary.clear()\n",
    "    \n",
    "    def get_structure(self):\n",
    "        \"\"\"Returns the lexicon data structure.\"\"\"\n",
    "        return self._vocabulary "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
