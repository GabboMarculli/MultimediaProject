{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0736fda9-8217-4c09-93f7-d7da2991db5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\query_processing\\..\\structures\\DocumentIndex.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\query_processing\\..\\utilities\\General_Utilities.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\query_processing\\..\\structures\\DocumentIndexRow.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\query_processing\\..\\structures\\Lexicon.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\query_processing\\..\\structures\\LexiconRow.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\query_processing\\..\\building_data_structures\\CollectionStatistics.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\query_processing\\..\\structures\\PostingListHandler.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\query_processing\\..\\structures\\InvertedIndex.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\query_processing\\..\\utilities\\Compression.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\query_processing\\..\\structures\\BlockDescriptor.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\query_processing\\..\\query_processing\\Scoring.ipynb\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "from typing import List, Tuple\n",
    "from io import BufferedReader\n",
    "\n",
    "import import_ipynb\n",
    "import sys\n",
    "sys.path.append('../')  # Go up two folders to the project root\n",
    "\n",
    "from structures.DocumentIndex import DocumentIndex\n",
    "from structures.Lexicon import Lexicon\n",
    "from structures.PostingListHandler import Posting_List_Reader\n",
    "from query_processing.Scoring import Scoring\n",
    "from building_data_structures.CollectionStatistics import Collection_statistics\n",
    "from structures.InvertedIndex import Posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01c44daf-7d90-4fc4-a557-2c299d9afef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_INVERTED_INDEX=\"../building_data_structures/INV_INDEX\"\n",
    "PATH_FINAL_DOC_IDS=\"doc_ids.bin\"\n",
    "PATH_FINAL_FREQ=\"freq.bin\"\n",
    "PATH_FINAL_BLOCK_DESCRIPTOR=\"block_descriptors.bin\"\n",
    "DIR_LEXICON=\"../building_data_structures/LEXICON\"\n",
    "PATH_FINAL_LEXICON=\"lexicon.bin\"\n",
    "\n",
    "DIR_DOC_INDEX=\"../building_data_structures/DOC_INDEX\"\n",
    "PATH_COLLECTION_STATISTICS=\"collection_statistics.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be38f306-76bc-4967-8009-b575f5f29bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############################### CORREGGI: IL K VA DATO COME ATTRIBUTO ALLA CLASSE, NON PASSATO COME PARAMETRO ALLE FUNZIONI\n",
    "\n",
    "class DAAT():\n",
    "    file_DocIds: BufferedReader\n",
    "    file_Freq: BufferedReader\n",
    "    file_blocks: BufferedReader\n",
    "    posting_readers: List[Posting_List_Reader]\n",
    "    top_k_documents: List[Tuple[float, int]]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.lexicon = Lexicon()\n",
    "        self.collection_statistics = Collection_statistics(DIR_DOC_INDEX+\"/\"+PATH_COLLECTION_STATISTICS)\n",
    "        self.collection_statistics.read_binary_mode()\n",
    "        self.scorer = Scoring(self.collection_statistics)\n",
    "        # ############### VALUTARE SE APRIRE TUTTE LE POSTING UNA SOLA VOLTA NEL COSTRUTTORE E CHIUDERLE A FINE PROGRAMMA PER RISPARMIARE TEMPO\n",
    "        # self.open_all_posting_lists()\n",
    "    \n",
    "    def open_all_posting_lists(self) -> None: \n",
    "        self.file_DocIds = open(DIR_INVERTED_INDEX+\"/\"+PATH_FINAL_DOC_IDS, 'rb') \n",
    "        self.file_Freq = open(DIR_INVERTED_INDEX+\"/\"+PATH_FINAL_FREQ, 'rb') \n",
    "        self.file_blocks = open(DIR_INVERTED_INDEX+\"/\"+PATH_FINAL_BLOCK_DESCRIPTOR, 'rb')\n",
    "        self.file_lexicon = open(DIR_LEXICON+\"/\"+PATH_FINAL_LEXICON, 'rb') \n",
    "        \n",
    "        self.scorer.open_files()\n",
    "\n",
    "    def reset_lists(self) -> None:\n",
    "        # This list will contain pointer to the posting lists of all terms \n",
    "        self.posting_readers = []\n",
    "        # This list will contain the k most relevant document\n",
    "        self.top_k_documents = []\n",
    "\n",
    "    def close_all_posting_lists(self):\n",
    "        for file in [self.file_DocIds, self.file_Freq, self.file_blocks, self.file_lexicon]:\n",
    "            file.close()      \n",
    "\n",
    "    def scoreQuery(self, k: int, choice_function: str, tokens: List[str], isConjunctive: bool) -> List[Tuple[float, int]]:\n",
    "        \"\"\"\n",
    "        Scores a query and returns the top-k documents.\n",
    "\n",
    "        Args:\n",
    "            k (int): The number of top documents to retrieve.\n",
    "            choice_function (str): The scoring function to use.\n",
    "            tokens (List[str]): List of query tokens.\n",
    "            isConjunctive (bool): Whether the query is conjunctive.\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[float, int]]: List of top-k documents with their scores.\n",
    "        \"\"\"\n",
    "        self.open_all_posting_lists()\n",
    "        self.reset_lists()\n",
    "        self.initialize_posting_lists(tokens)\n",
    "\n",
    "        old_doc_id = -1 # used for the last posting list\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                # Retrieve the minimum doc_id, the next to process\n",
    "                docToProcess, term_freq = self.min_doc()\n",
    "\n",
    "                # Check if there are no other doc to process\n",
    "                if docToProcess == -1:\n",
    "                    break\n",
    "\n",
    "                # If i have read a new doc_id\n",
    "                if docToProcess != old_doc_id:\n",
    "                    term_freq = 0 # reset term_freq\n",
    "                    old_doc_id = docToProcess # update old doc_id\n",
    "\n",
    "                if isConjunctive:\n",
    "                    current_docs = [reader for reader in self.posting_readers if reader.get_current_posting() is not None]\n",
    "\n",
    "                    # Check if the document is present in all posting lists\n",
    "                    if any(post.get_current_posting().doc_id != docToProcess for post in current_docs):\n",
    "                        for reader in current_docs: \n",
    "                            if reader.get_current_posting().doc_id == docToProcess: # next if doc_id equal to min_doc_id\n",
    "                                next(reader)\n",
    "                        continue\n",
    "                        \n",
    "                for reader in self.posting_readers:\n",
    "                    if reader.get_current_posting() is not None and reader.get_current_posting().doc_id == docToProcess:\n",
    "                        term_freq += reader.get_current_posting().frequency\n",
    "                        next(reader)\n",
    "\n",
    "                self.update_heap(choice_function, docToProcess, term_freq, k)\n",
    "\n",
    "            except StopIteration:\n",
    "                    end, _ = self.all_lists_exhausted()\n",
    "                \n",
    "                    if end == True:\n",
    "                        self.update_heap(choice_function, docToProcess, term_freq, k)\n",
    "                        break  \n",
    "                    else:\n",
    "                        continue  \n",
    "            except Exception as e:\n",
    "                print(f\"Error during execution: {e}\")\n",
    "                break\n",
    "                \n",
    "            finally:\n",
    "                self.close_all_posting_lists()\n",
    "        \n",
    "        self.close_all_posting_lists()\n",
    "        self.scorer.close_files()\n",
    "\n",
    "        return self.top_k_documents\n",
    "\n",
    "    def initialize_posting_lists(self,tokens: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Initializes posting lists for the given tokens.\n",
    "\n",
    "        Args:\n",
    "            tokens (List[str]): List of query tokens.\n",
    "        \"\"\"\n",
    "        for token in tokens:\n",
    "            term_lexicon_row = self.lexicon.get_entry(token)\n",
    "        \n",
    "            if term_lexicon_row is not None:                \n",
    "                self.posting_readers.append(Posting_List_Reader(term_lexicon_row, False, self.file_DocIds, self.file_Freq, self.file_blocks))\n",
    "\n",
    "                # print(\"Per il termine: \", term_lexicon_row.term)\n",
    "                # for obj in posting_reader:\n",
    "                #     print (obj)\n",
    "                \n",
    "        for reader in self.posting_readers:\n",
    "            next(reader)\n",
    "\n",
    "    def update_heap(self,choice_function: str, docToProcess: int, term_freq: int, k: int) -> None:\n",
    "        \"\"\"\n",
    "        Updates the priority queue (heap) with the latest document score.\n",
    "\n",
    "        Args:\n",
    "            choice_function (str): The scoring function to use.\n",
    "            docToProcess (int): Document ID to process.\n",
    "            term_freq (int): Term frequency in the document.\n",
    "            k (int): The number of top documents to retrieve.\n",
    "        \"\"\"\n",
    "        # print(\"Sto per processare docToProcess, term_freq: \", docToProcess, term_freq)\n",
    "        doc_score = self.scorer.choose_scoring_function(choice_function, docToProcess, term_freq)\n",
    "        # print(\"Il doc_score Ã¨ venuto: \", doc_score)\n",
    "        \n",
    "        # Add the element to the priority queue\n",
    "        heapq.heappush(self.top_k_documents, (doc_score, docToProcess)) \n",
    "    \n",
    "        # Keep the priority queue of size k.\n",
    "        if len(self.top_k_documents) > k:\n",
    "            heapq.heappop(self.top_k_documents) \n",
    "\n",
    "    def all_lists_exhausted(self) -> Tuple[bool, List[Posting]]:\n",
    "        \"\"\"\n",
    "        Checks if all posting lists are exhausted.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[bool, List[[Posting]]: Tuple containing a boolean indicating whether all lists are exhausted\n",
    "            and a list of the current documents in each posting list.\n",
    "        \"\"\"\n",
    "        # Read the next document from each posting list\n",
    "        current_docs = [reader.get_current_posting() for reader in self.posting_readers]\n",
    "        \n",
    "        # Check if all readers have reached the end of the list\n",
    "        return all(doc is None for doc in current_docs), current_docs\n",
    "\n",
    "    def min_doc(self) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Retrieves the minimum document ID and its frequency among the current documents in all posting lists.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[int, int]: Tuple containing the minimum document ID and its frequency.\n",
    "        \"\"\"\n",
    "        end, current_docs = self.all_lists_exhausted()\n",
    "\n",
    "        # Check if all readers have reached the end of the list\n",
    "        if end == True:\n",
    "            return -1,-1\n",
    "\n",
    "        # Fetch only not null documents\n",
    "        valid_docs = [doc for doc in current_docs if doc is not None]\n",
    "\n",
    "        # Retrieve the documents with min doc_id\n",
    "        min_doc = min(valid_docs, key=lambda x: x.doc_id)\n",
    "\n",
    "        # Return the minimum doc_id and its frequency\n",
    "        return min_doc.doc_id, min_doc.frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06608c5f-1b7c-490f-ab78-792fcee5be52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.1948436837130192, 21), (1.1948436837130192, 51)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# daat = DAAT()\n",
    "# my_list = [\"and\", \"cat\"]\n",
    "# daat.scoreQuery(3, \"bm25\", my_list , True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b74b4305-2f68-4a1b-8b8b-c7755dbf12f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# VALUTARE SE UNA CLASSE SIMILE RENDEREBBE PIU' LENTA/VELOCE L'ESECUZIONE\n",
    "# Al posto di avere la lista top_k_documents e un parametro k \n",
    "####################################################\n",
    "\n",
    "# class MinHeap:\n",
    "#     def __init__(self, k: int):\n",
    "#         self.heap = []\n",
    "#         self.k = k\n",
    "\n",
    "#     def push(self, item: Tuple[float, int]) -> None:\n",
    "#         heapq.heappush(self.heap, item)\n",
    "#         if len(self.heap) > self.k:\n",
    "#             heapq.heappop(self.heap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bc4a102-32c8-4bf1-b00b-898690307cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
