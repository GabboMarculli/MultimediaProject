{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22672c0c-6c86-472c-a763-2c35a771e958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\query_processing\\..\\pre_processing\\TextProcessor.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\query_processing\\..\\utilities\\General_Utilities.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\query_processing\\..\\query_processing\\DAAT.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\query_processing\\..\\structures\\DocumentIndex.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\query_processing\\..\\structures\\DocumentIndexRow.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\query_processing\\..\\structures\\Lexicon.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\query_processing\\..\\structures\\LexiconRow.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\query_processing\\..\\building_data_structures\\CollectionStatistics.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\query_processing\\..\\structures\\PostingListHandler.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\query_processing\\..\\structures\\InvertedIndex.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\query_processing\\..\\utilities\\Compression.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\query_processing\\..\\structures\\BlockDescriptor.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\query_processing\\..\\query_processing\\Scoring.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\query_processing\\..\\query_processing\\MaxScore.ipynb\n",
      "Leggo da disco effettivamente\n",
      "Leggo da disco effettivamente\n",
      "Leggo da disco effettivamente\n",
      "Leggo da disco effettivamente\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import import_ipynb\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')  # Go up two folders to the project root\n",
    "from pre_processing.TextProcessor import TextProcessor\n",
    "from query_processing.DAAT import DAAT\n",
    "from query_processing.MaxScore import Max_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cc8e5f0-d305-464e-9478-53cc6b551a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_index_path = \"../building_data_structures/INV_INDEX\"\n",
    "lexicon_path = \"../building_data_structures/LEXICON\"\n",
    "doc_index_path = \"../building_data_structures/DOC_INDEX\"\n",
    "\n",
    "# ################################################################# sposta questo check dentro \"util\"\n",
    "\n",
    "def check_files_for_query_processing() -> bool:\n",
    "    \"\"\"\n",
    "        Check the existence and non-emptiness of directories 'INV_INDEX', 'LEXICON', and 'DOCUMENT_INDEX' and its files.\n",
    "    \n",
    "    Returns:\n",
    "        bool\n",
    "    \"\"\"\n",
    "    if os.path.exists(doc_index_path + \"/\" + \"document_index.bin\") and os.path.exists(doc_index_path + \"/\" + \"collection_statistics.bin\"):\n",
    "        if os.path.exists(inv_index_path + \"/\" + \"doc_ids.bin\") and os.path.exists(inv_index_path + \"/\" + \"freq.bin\"):\n",
    "            if os.path.exists(lexicon_path + \"/\" + \"lexicon.bin\"):\n",
    "                return True\n",
    "                \n",
    "    return False\n",
    "        \n",
    "def process_query(query:str, flag: bool, scoring_function: str, alg, k: int, isConjunctive:bool):\n",
    "    if not check_files_for_query_processing(): # serve?\n",
    "        return \n",
    "        \n",
    "    if query == \"\":\n",
    "       return []\n",
    "    \n",
    "    # process query\n",
    "    text_processor = TextProcessor(flag)\n",
    "    tokens = text_processor.process_text(query, True)\n",
    "\n",
    "    # elimino i duplicati\n",
    "    tokens = list(set(tokens))\n",
    "\n",
    "    if alg == \"daat\":\n",
    "        daat = DAAT()\n",
    "        result = daat.scoreQuery(k, scoring_function, tokens, isConjunctive)\n",
    "    else:\n",
    "        max_score = MaxScore()\n",
    "        result = max_score.scoreQuery(k, scoring_function, tokens, isConjunctive)\n",
    "\n",
    "    return [item[1] for item in result]    # torna solo i documenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6d5847c-3ace-480d-924e-49c0b4a4bb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_query(\"my name is Nicola\", False, \"bm25\", \"daat\", 7, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
