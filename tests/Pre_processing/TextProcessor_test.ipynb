{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1ded6f5-4fae-4f7b-9df8-faa13a2e0fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\tests\\Pre_processing\\../..\\pre_processing\\TextProcessor.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\tests\\Pre_processing\\../..\\utilities\\General_Utilities.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../')  # Go up two folders to the project root\n",
    "\n",
    "from pre_processing.TextProcessor import TextProcessor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563dc4e4-cbfd-4618-bad2-27f0f08d75b6",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7214437a-ad76-40a7-99c1-b37be0f85aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import ipytest\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "091567c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopword file english_stop_words.txt not imported\n",
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                         [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "# ######################################################## test remove stop words ##########################################################\n",
    "txt_proc = TextProcessor(True)\n",
    "\n",
    "first_list = [\n",
    "    \"the\", \"and\", \"is\", \"in\", \"it\", \"you\", \"are\", \"to\", \"of\", \"that\",\n",
    "    \"I\", \"we\", \"he\", \"she\", \"they\", \"me\", \"him\", \"her\", \"us\", \"them\",\n",
    "    \"this\", \"a\", \"an\", \"for\", \"on\", \"with\", \"at\", \"by\", \"from\", \"about\",\n",
    "    \"as\", \"but\", \"not\", \"or\", \"if\", \"when\", \"where\", \"how\", \"which\", \"what\",\n",
    "    \"there\", \"here\", \"so\", \"just\", \"then\", \"now\", \"well\", \"also\", \"very\", \"really\",\n",
    "    \"but\", \"however\", \"although\", \"because\", \"since\", \"if\", \"unless\", \"until\", \"while\", \"after\",\n",
    "    \"before\", \"during\", \"since\", \"when\", \"whereas\", \"though\", \"so\", \"because\", \"as\", \"if\",\n",
    "    \"said\", \"was\", \"were\", \"have\", \"has\", \"had\", \"do\", \"does\", \"did\", \"say\",\n",
    "    \"get\", \"got\", \"getting\", \"gets\", \"make\", \"made\", \"making\", \"makes\", \"go\", \"went\",\n",
    "    \"going\", \"goes\", \"come\", \"came\", \"coming\", \"comes\", \"see\", \"saw\", \"seeing\", \"sees\",\n",
    "    \"think\", \"thought\", \"thinking\", \"thinks\", \"know\", \"knew\", \"knowing\", \"knows\", \"want\", \"wanted\",\n",
    "    \"wanting\", \"wants\", \"use\", \"used\", \"using\", \"uses\", \"find\", \"found\", \"finding\", \"finds\",\n",
    "    \"tell\", \"told\", \"telling\", \"tells\", \"ask\", \"asked\", \"asking\", \"asks\", \"work\", \"worked\",\n",
    "    \"working\", \"works\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"like\", \"liked\", \"liking\", \"likes\",\n",
    "    \"call\", \"called\", \"calling\", \"calls\", \"try\", \"tried\", \"trying\", \"tries\", \"need\", \"needed\",\n",
    "    \"needing\", \"needs\", \"become\", \"became\", \"becoming\", \"becomes\", \"leave\", \"left\", \"leaving\", \"leaves\",\n",
    "    \"put\", \"puts\", \"getting\", \"got\", \"putting\", \"puts\", \"mean\", \"meant\", \"meaning\", \"means\",\n",
    "    \"keep\", \"kept\", \"keeping\", \"keeps\", \"start\", \"started\", \"starting\", \"starts\", \"show\", \"showed\",\n",
    "    \"showing\", \"shows\", \"hear\", \"heard\", \"hearing\", \"hears\", \"move\", \"moved\", \"moving\", \"moves\",\n",
    "    \"live\", \"lived\", \"living\", \"lives\", \"bring\", \"brought\", \"bringing\", \"brings\", \"happen\", \"happened\",\n",
    "    \"happening\", \"happens\", \"write\", \"wrote\", \"writing\", \"writes\", \"provide\", \"provided\", \"providing\", \"provides\",\n",
    "    \"sit\", \"sat\", \"sitting\", \"sits\", \"stand\", \"stood\", \"standing\", \"stands\", \"include\", \"included\",\n",
    "    \"including\", \"includes\", \"continue\", \"continued\", \"continuing\", \"continues\", \"change\", \"changed\", \"changing\", \"changes\"\n",
    "]\n",
    "\n",
    "first_list_removed_stopwords = txt_proc.remove_stopwords(first_list)\n",
    "\n",
    "expected_first_list_stopwords = ['I', 'us', 'well', 'also', 'really', 'however', 'although', 'since', 'unless', 'since', 'whereas',\n",
    "                       'though', 'said', 'say', 'get', 'got', 'getting', 'gets', 'make', 'made', 'making', 'makes', 'go', \n",
    "                       'went', 'going', 'goes', 'come', 'came', 'coming', 'comes', 'see', 'saw', 'seeing', 'sees', 'think',\n",
    "                       'thought', 'thinking', 'thinks', 'know', 'knew', 'knowing', 'knows', 'want', 'wanted', 'wanting', 'wants',\n",
    "                       'use', 'used', 'using', 'uses', 'find', 'found', 'finding', 'finds', 'tell', 'told', 'telling', 'tells', 'ask',\n",
    "                       'asked', 'asking', 'asks', 'work', 'worked', 'working', 'works', 'seem', 'seemed', 'seeming', 'seems',\n",
    "                       'like', 'liked', 'liking', 'likes', 'call', 'called', 'calling', 'calls', 'try', 'tried', 'trying', 'tries',\n",
    "                       'need', 'needed', 'needing', 'needs', 'become', 'became', 'becoming', 'becomes', 'leave', 'left', 'leaving',\n",
    "                       'leaves', 'put', 'puts', 'getting', 'got', 'putting', 'puts', 'mean', 'meant', 'meaning', 'means', 'keep',\n",
    "                       'kept', 'keeping', 'keeps', 'start', 'started', 'starting', 'starts', 'show', 'showed', 'showing', 'shows',\n",
    "                       'hear', 'heard', 'hearing', 'hears', 'move', 'moved', 'moving', 'moves', 'live', 'lived', 'living', 'lives',\n",
    "                       'bring', 'brought', 'bringing', 'brings', 'happen', 'happened', 'happening', 'happens', 'write', 'wrote', \n",
    "                       'writing', 'writes', 'provide', 'provided', 'providing', 'provides', 'sit', 'sat', 'sitting', 'sits', 'stand', \n",
    "                       'stood', 'standing', 'stands', 'include', 'included', 'including', 'includes', 'continue', 'continued', 'continuing',\n",
    "                       'continues', 'change', 'changed', 'changing', 'changes']\n",
    "\n",
    "def test_remove_stop_words():\n",
    "    assert len(first_list_removed_stopwords) == len(expected_first_list_stopwords)\n",
    "    \n",
    "    for i in range(0, len(first_list_removed_stopwords)):\n",
    "        assert first_list_removed_stopwords[i] == expected_first_list_stopwords[i]\n",
    "\n",
    "\n",
    "# ###################################################### test stem text #####################################################################\n",
    "\n",
    "first_list_stemmed = txt_proc.stem_text(first_list)\n",
    "\n",
    "expected_first_list_stemmed = ['the', 'and', 'is', 'in', 'it', 'you', 'are', 'to', 'of', 'that', 'i', 'we', 'he', 'she', 'they', 'me', 'him', 'her',\n",
    "                               'us', 'them', 'this', 'a', 'an', 'for', 'on', 'with', 'at', 'by', 'from', 'about', 'as', 'but', 'not', 'or', 'if',\n",
    "                               'when', 'where', 'how', 'which', 'what', 'there', 'here', 'so', 'just', 'then', 'now', 'well', 'also', 'veri', 'realli',\n",
    "                               'but', 'howev', 'although', 'becaus', 'sinc', 'if', 'unless', 'until', 'while', 'after', 'befor', 'dure', 'sinc',\n",
    "                               'when', 'wherea', 'though', 'so', 'becaus', 'as', 'if', 'said', 'was', 'were', 'have', 'has', 'had', 'do', 'doe',\n",
    "                               'did', 'say', 'get', 'got', 'get', 'get', 'make', 'made', 'make', 'make', 'go', 'went', 'go', 'goe', 'come', 'came', \n",
    "                               'come', 'come', 'see', 'saw', 'see', 'see', 'think', 'thought', 'think', 'think', 'know', 'knew', 'know', 'know',\n",
    "                               'want', 'want', 'want', 'want', 'use', 'use', 'use', 'use', 'find', 'found', 'find', 'find', 'tell', 'told', 'tell',\n",
    "                               'tell', 'ask', 'ask', 'ask', 'ask', 'work', 'work', 'work', 'work', 'seem', 'seem', 'seem', 'seem', 'like', 'like',\n",
    "                               'like', 'like', 'call', 'call', 'call', 'call', 'tri', 'tri', 'tri', 'tri', 'need', 'need', 'need', 'need', 'becom',\n",
    "                               'becam', 'becom', 'becom', 'leav', 'left', 'leav', 'leav', 'put', 'put', 'get', 'got', 'put', 'put', 'mean', 'meant',\n",
    "                               'mean', 'mean', 'keep', 'kept', 'keep', 'keep', 'start', 'start', 'start', 'start', 'show', 'show', 'show', 'show',\n",
    "                               'hear', 'heard', 'hear', 'hear', 'move', 'move', 'move', 'move', 'live', 'live', 'live', 'live', 'bring', 'brought', \n",
    "                               'bring', 'bring', 'happen', 'happen', 'happen', 'happen', 'write', 'wrote', 'write', 'write', 'provid', 'provid',\n",
    "                               'provid', 'provid', 'sit', 'sat', 'sit', 'sit', 'stand', 'stood', 'stand', 'stand', 'includ', 'includ', 'includ', \n",
    "                               'includ', 'continu', 'continu', 'continu', 'continu', 'chang', 'chang', 'chang', 'chang']\n",
    "\n",
    "def test_stemming():\n",
    "    assert len(first_list_stemmed) == len(expected_first_list_stemmed) == len(first_list)\n",
    "    \n",
    "    for i in range(0, len(first_list_stemmed)):\n",
    "        assert first_list_stemmed[i] == expected_first_list_stemmed[i]\n",
    "\n",
    "\n",
    "# ########################################## test clean text ###########################################################################\n",
    "\n",
    "def test_clean_text():\n",
    "    assert txt_proc.clean_text(\"HBHYjvbghbHnBghJGHjHnbnBhKJG\") == \"hbhyjvbghbhnbghjghjhnbnbhkjg\"\n",
    "    assert txt_proc.clean_text(\"There are special characters like @, #, $, %, and ^ in this sentence.\") == \"there are special characters like and in this sentence\"\n",
    "    assert txt_proc.clean_text(\"https://en.wikipedia.org/wiki/List_of_Unicode_characters hello\") == \"hello\"\n",
    "    assert txt_proc.clean_text(\"The first websiteâThe Manhattan Project:\") == \"the first website the manhattan project\"\n",
    "    assert txt_proc.clean_text(\"\") == \"\"\n",
    "    assert txt_proc.clean_text(\"      .................   !!!!!! @#&                    \") == \"\"\n",
    "    assert txt_proc.clean_text(\" !%£&/  ($£(/  £()\") == \"\"\n",
    "    assert txt_proc.clean_text(\"The $sun is shining! brightly                  ..... in the sky&&.\")  == \"the sun is shining brightly in the sky\"\n",
    "    assert txt_proc.clean_text(\"Each of these types of communitiesâ <html> hi </html>\") == \"each of these types of communities hi\"\n",
    "    assert txt_proc.clean_text(\"This is a sentence with multiple spaces  and punctuation!\") == \"this is a sentence with multiple spaces and punctuation\"\n",
    "\n",
    "import re\n",
    "\n",
    "def test_single_functionalities():\n",
    "    # Test for regexp\n",
    "    assert re.sub(txt_proc.reg_exp_html_tags, \"\", \"<html>This is an <b>example</b> text.</html>\") == \"This is an example text.\"\n",
    "    assert re.sub(txt_proc.reg_exp_usernames, \"\", \"Hello @user123, how are you?\") == \"Hello , how are you?\"\n",
    "    assert re.sub(txt_proc.reg_exp_web_link_pattern, \"\", \"Visit our website: https://www.example.com\") == \"Visit our website: \"\n",
    "    assert re.sub(txt_proc.reg_exp_punctuation, \"\", \"This is a sentence with @ special characters!\") == \"This is a sentence with  special characters\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b4f5cc-121b-4335-83f9-857ba5b15970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
