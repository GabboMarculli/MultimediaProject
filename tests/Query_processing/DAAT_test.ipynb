{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5991f227-2f23-4341-b41e-fb7fb8aed6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\tests\\Query_processing\\../..\\query_processing\\DAAT.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\tests\\Query_processing\\../..\\structures\\DocumentIndex.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\tests\\Query_processing\\../..\\utilities\\General_Utilities.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\tests\\Query_processing\\../..\\structures\\DocumentIndexRow.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\tests\\Query_processing\\../..\\structures\\Lexicon.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\tests\\Query_processing\\../..\\structures\\LexiconRow.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\tests\\Query_processing\\../..\\building_data_structures\\CollectionStatistics.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\tests\\Query_processing\\../..\\structures\\PostingListHandler.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\tests\\Query_processing\\../..\\structures\\InvertedIndex.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\tests\\Query_processing\\../..\\utilities\\Compression.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\tests\\Query_processing\\../..\\structures\\BlockDescriptor.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\tests\\Query_processing\\../..\\query_processing\\Scoring.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import sys\n",
    "sys.path.append('../../')  # Go up two folders to the project root\n",
    "\n",
    "from query_processing import DAAT\n",
    "\n",
    "import ipytest\n",
    "ipytest.autoconfig()\n",
    "\n",
    "import tempfile\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09815cda-7e0c-43cb-bb91-281ecbd5ccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "def check_if_inserted(doc_id, real_action, my_heap) -> bool:\n",
    "    \"\"\"\n",
    "        Pass to the function a doc_id and the list of most relevant document with score.\n",
    "        Real_action is True if the doc_id needs to be inserted.\n",
    "    \"\"\"\n",
    "    for index,elem in enumerate(my_heap):\n",
    "        if my_heap[index][1] == doc_id and real_action == False:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "def test_daat():\n",
    "    # Set up a temporary directory for testing\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        daat_instance = DAAT()\n",
    "        \n",
    "        # Test open_all_posting_lists\n",
    "        daat_instance.open_all_posting_lists()\n",
    "        assert daat_instance.file_DocIds.readable() is True\n",
    "        assert daat_instance.file_Freq.readable() is True\n",
    "        assert daat_instance.file_blocks.readable() is True\n",
    "        assert daat_instance.file_lexicon.readable() is True\n",
    "\n",
    "        # Test reset_lists\n",
    "        daat_instance.reset_lists()\n",
    "        assert len(daat_instance.posting_readers) == 0\n",
    "        assert len(daat_instance.top_k_documents) == 0\n",
    "\n",
    "        # Test close_all_posting_lists\n",
    "        daat_instance.close_all_posting_lists()\n",
    "        assert daat_instance.file_DocIds.closed is True\n",
    "        assert daat_instance.file_Freq.closed is True\n",
    "        assert daat_instance.file_blocks.closed is True\n",
    "        assert daat_instance.file_lexicon.closed is True\n",
    "\n",
    "        # Test initialize_posting_lists (assuming tokens is a list of strings)\n",
    "        daat_instance.open_all_posting_lists()\n",
    "        tokens = [\"happiness\", \"home\", \"between\"]\n",
    "        daat_instance.initialize_posting_lists(tokens)\n",
    "        assert len(daat_instance.posting_readers) == len(tokens)\n",
    "        daat_instance.close_all_posting_lists()\n",
    "\n",
    "        # Test update_heap\n",
    "        scoring_function = \"bm25\"\n",
    "        daat_instance.update_heap(scoring_function, 0, 3, 2) # check if insert works\n",
    "        assert len(daat_instance.top_k_documents) == 1\n",
    "        daat_instance.update_heap(scoring_function, 1, 4, 2)\n",
    "        assert len(daat_instance.top_k_documents) == 2\n",
    "        \n",
    "        for i in range(2,50): # generate 50 others doc_id\n",
    "            freq = random.randint(1, 10) # with frequency\n",
    "            score = daat_instance.scorer.choose_scoring_function(scoring_function, i, freq)\n",
    "            to_insert = (score >= daat_instance.top_k_documents[0][0] or score >= daat_instance.top_k_documents[1][0])\n",
    "            daat_instance.update_heap(scoring_function, i, freq, 2)\n",
    "\n",
    "            assert len(daat_instance.top_k_documents) == 2 # check if lenght remains k\n",
    "            assert check_if_inserted(i, to_insert, daat_instance.top_k_documents) == True # check if heap maintains only highest score\n",
    "\n",
    "        # test scoreQuery\n",
    "        daat = DAAT()\n",
    "        my_list = [\"happiness\", \"home\", \"between\"]\n",
    "        result = daat.scoreQuery(3, \"bm25\", my_list , False)\n",
    "        assert len(result) <= 3        \n",
    "        \n",
    "        # Test min_doc\n",
    "        min_doc_id, min_doc_freq = daat.min_doc()\n",
    "        assert min_doc_id == -1\n",
    "        assert min_doc_freq == -1\n",
    "        # TODO: FANNE ALTRI\n",
    "\n",
    "        # TODO: Test all_lists_exhausted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
