{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e28d175c-c1db-4793-aaec-5b20cd545673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\pre_processing\\..\\utilities\\General_Utilities.ipynb\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from typing import List\n",
    "import time\n",
    "import re\n",
    "\n",
    "import import_ipynb\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')  # Go up two folders to the project root\n",
    "from utilities.General_Utilities import read_words_from_file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "593bf381-71d4-42c1-b3eb-d09dfca539e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This class is used in the project to implement pre-processing text elaboration.\n",
    "    It contains several methods for text cleaning, stemming and stop word removal.\n",
    "\"\"\"\n",
    "class TextProcessor:\n",
    "    \n",
    "    def __init__(self, use_stemming_and_stop_words_removal:bool):\n",
    "        self.use_stemming_and_stop_words = use_stemming_and_stop_words_removal # flag to enable stemming and stop_words_removal\n",
    "        \n",
    "        #Regular expression used to clean a raw text.\n",
    "        self.reg_exp_punctuation = r'[^\\w\\s]'\n",
    "        self.reg_exp_html_tags=r'<[^>]+>'\n",
    "        self.reg_exp_hashtags = r'#\\w+' \n",
    "        self.reg_exp_usernames = r'@\\w+'\n",
    "        self.control_char_pattern=r'[^\\x00-\\x7F]+'\n",
    "        self.reg_exp_web_link_pattern=r'https*://\\S+|www.\\S+'\n",
    "        \n",
    "        #A faster an improved version of PorterStemmer\n",
    "        self.stemmer = SnowballStemmer(\"english\")\n",
    "        \n",
    "        #Initialize the list of all English stopwords\n",
    "        #Use the default NLTK stopword + additional stopwords find on the web and grouped in a single file.\n",
    "        stop_word_list=[]\n",
    "        try:\n",
    "            stop_word_list=read_words_from_file(\"../utilities/english_stop_words.txt\")\n",
    "        except Exception as e:   \n",
    "            print(\"Stopword file english_stop_words.txt not imported\")\n",
    "        self.stop_words=set(stopwords.words(\"english\")+stop_word_list)\n",
    "        \n",
    "        #print(self.stop_words)\n",
    "        #print(len(self.stop_words))\n",
    "            \n",
    "    def process_text(self, text:str, return_tokens = False)->str:\n",
    "        \"\"\" Process a text by cleaning, tokenizing, removing stopwords and stemming(optionally).\n",
    "            This is the main method called from outside, during the processing of all the documents.\n",
    "\n",
    "            Args:\n",
    "                text: The text to be processed.\n",
    "            Returns:\n",
    "                The processed text after cleaning, tokenizing, removing stopwords, and stemming.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Remove special characters\n",
    "        text = self.clean_text(text)\n",
    "        \n",
    "        # To handle case of empty string\n",
    "        if not text:\n",
    "            return text\n",
    "\n",
    "        # Transform the document in a list of tokens\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        word_tokens = tokenizer.tokenize(text)\n",
    "\n",
    "        # Remove stopwords and apply stemming.\n",
    "        if self.use_stemming_and_stop_words:\n",
    "            word_tokens = self.remove_stopwords(word_tokens)\n",
    "            word_tokens = self.stem_text(word_tokens)\n",
    "\n",
    "        # Transform and return the list of tokens in a document (unique string)\n",
    "        if return_tokens == True:\n",
    "            return word_tokens\n",
    "            \n",
    "        return ' '.join(word_tokens)\n",
    "\n",
    "    def stem_text(self, tokens:List[str])->List[str]:\n",
    "        \"\"\"Apply Stemming on a list of tokens using English language.\n",
    "        \n",
    "        Args:\n",
    "            tokens: List of tokens to be stemmed.\n",
    "        Returns:\n",
    "            The list of tokens after stemming based on the provided language. \n",
    "        \"\"\"\n",
    "        \n",
    "        # Do stem for each tokens in the list\n",
    "        return [self.stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    def remove_stopwords(self, tokens:List[str]):\n",
    "        \"\"\"Remove stopwords from a list of stop words tokens in English language.\n",
    "\n",
    "        Args:\n",
    "            tokens: List of tokens to be processed.\n",
    "            language: The language used to identify the stopwords.\n",
    "        Returns:\n",
    "            The list of tokens after removing the stopwords based on the provided language. \n",
    "            If the language is not supported, the function returns the original tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the tokens is in the list of stopwords, remove it\n",
    "        return [word for word in tokens if word not in self.stop_words]\n",
    "    \n",
    "    def clean_text(self,text:str)->str:\n",
    "        \"\"\"Clean the text by converting to lowercase, replacing special characters.\n",
    "\n",
    "        Args:\n",
    "            text: The text to be cleaned.\n",
    "        Returns:\n",
    "            The cleaned text after converting to lowercase, replacing special characters, and removing emojis.\n",
    "        \"\"\"\n",
    "        text = str(text).lower()\n",
    "\n",
    "        # Replace special characters \n",
    "        text = re.sub(self.control_char_pattern, \" \", text)\n",
    "        \n",
    "        combined_pattern = re.compile(self.reg_exp_html_tags+'|'+self.reg_exp_hashtags + '|' + self.reg_exp_punctuation + '|' + self.reg_exp_usernames + '|' + self.reg_exp_web_link_pattern)\n",
    "        text = re.sub(combined_pattern, \" \", text)\n",
    "\n",
    "        #Collapse white spaces between words.\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        return text.strip()\n",
    "    \n",
    "    '''\n",
    "    def lemmatize(self, tokens, language):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        lemmatized = [lemmatizer.lemmatize(w, language) \n",
    "                      for w in tokens]\n",
    "        \n",
    "        return lemmatized\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
