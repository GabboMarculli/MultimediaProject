{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7104e7ab-9d07-43de-82c0-5061dfb7f246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\MultimediaProject\\pre_processing\\..\\pre_processing\\TextProcessor.ipynb\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import io\n",
    "import tarfile\n",
    "from tarfile import TarInfo\n",
    "import threading\n",
    "\n",
    "import sys\n",
    "import import_ipynb\n",
    "\n",
    "from typing import TextIO, BinaryIO\n",
    "from typing import List\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import pre_processing.TextProcessor as text_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83a7c96d-5929-41df-8a87-a8ad83a85a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_compressed_tsv_line_by_line(compressed_file_path):\n",
    "    try:\n",
    "        with open(compressed_file_path, 'rb') as f:\n",
    "            text_processor = TextProcessor()\n",
    "            with gzip.GzipFile(fileobj=f) as gz: \n",
    "                buffer = io.TextIOWrapper(gz, encoding='utf-8')\n",
    "\n",
    "                # C'è bisogno di leggere la prima riga a parte perchè essa inizia con i seguenti metadati \"# collection.tsv0000777000175000017502663675055413400073633015704 0ustar  spacemanidolspacemanidol0 \"\n",
    "                # Quindi prima di entrare nel ciclo che legge tutte le righe, elimino questo prefisso\n",
    "                first_line = next(buffer)  # Leggi la prima riga\n",
    "                cleaned_first_line = ' '.join(first_line.split()[3:])  # Rimuovi la parte iniziale indesiderata\n",
    "                text_processor.process_text(text)(0, cleaned_first_line)\n",
    "\n",
    "                for line in buffer:\n",
    "                    pid, text = line.strip().split('\\t')\n",
    "                    text_processor.process_text(text)(pid, text)\n",
    "                    print(pid,text)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {compressed_file_path} non trovato.\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"Si è verificato un errore durante l'analisi del file {compressed_file_path}: {e}\")\n",
    "\n",
    "# Esempio di utilizzo\n",
    "#parse_compressed_tsv_line_by_line('collection.tar.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be6a68c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompress_strings(compressed_file_path):\n",
    "    try:\n",
    "        with open(compressed_file_path, 'rb') as f:\n",
    "            text_processor = TextProcessor()\n",
    "            with gzip.GzipFile(fileobj=f) as gz: \n",
    "                buffer = io.TextIOWrapper(gz, encoding='utf-8')\n",
    "                print(buffer)\n",
    "                # C'è bisogno di leggere la prima riga a parte perchè essa inizia con i seguenti metadati \"# collection.tsv0000777000175000017502663675055413400073633015704 0ustar  spacemanidolspacemanidol0 \"\n",
    "                # Quindi prima di entrare nel ciclo che legge tutte le righe, elimino questo prefisso\n",
    "                first_line = next(buffer)  # Leggi la prima riga\n",
    "                cleaned_first_line = ' '.join(first_line.split()[3:])  # Rimuovi la parte iniziale indesiderata\n",
    "                text_processor.process_text(text)(0, cleaned_first_line)\n",
    "\n",
    "                for line in buffer:\n",
    "                    pid, text = line.strip().split('\\t')\n",
    "                    text_processor.process_text(text)(pid, text)\n",
    "                    #print(pid,text)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {compressed_file_path} non trovato.\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"Si è verificato un errore durante l'analisi del file {compressed_file_path}: {e}\")\n",
    "    \n",
    "    return decompressed_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2b64dde",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'collection_cleaned.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Specifica il percorso del file TSV\u001b[39;00m\n\u001b[0;32m      2\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollection_cleaned.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      4\u001b[0m     count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'collection_cleaned.tsv'"
     ]
    }
   ],
   "source": [
    "# Specifica il percorso del file TSV\n",
    "file_path = \"collection_cleaned.tsv\"\n",
    "with open(file_path, 'rb') as f:\n",
    "    count = 0\n",
    "    for line in f:\n",
    "        print(line)\n",
    "        count = count + 1\n",
    "        if count == 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d19311-9b6f-48dc-805f-eb1ac7997285",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_compressed_tsv_line_by_line(\"C:/Users/Davide/IR/collection.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acc5e51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collection_Reader:\n",
    "    collection_file: tarfile.TarFile\n",
    "        \n",
    "    MEMORY_BASED_BUFFER_SIZE:int\n",
    "    memory_based_buffer:List[str]\n",
    "        \n",
    "    text_processor:text_proc.TextProcessor\n",
    "    file_position: int\n",
    "    file_members:List[TarInfo]\n",
    "    \n",
    "    def __init__(self,path_collection_file:str,memory_based_buffer_size:int):        \n",
    "        self.collection_file = tarfile.open(path_collection_file, 'r:gz')\n",
    "        self.MEMORY_BASED_BUFFER_SIZE = memory_based_buffer_size\n",
    "        self.text_processor = text_proc.TextProcessor(True, True)\n",
    "        self.file_position = 0\n",
    "        self.memory_based_buffer = []\n",
    "        self.file_member = self.collection_file.getmembers()[0]\n",
    "\n",
    "    def read_collection(self):\n",
    "        with self.collection_file.extractfile(self.file_member) as file_handle:\n",
    "            file_handle.seek(self.file_position)\n",
    "            \n",
    "            for _ in range(self.MEMORY_BASED_BUFFER_SIZE):\n",
    "                line = file_handle.readline().decode('utf-8')\n",
    "    \n",
    "                # end of file\n",
    "                if not line:\n",
    "                    break \n",
    "                    \n",
    "                line_processed = self.text_processor.process_text(line)\n",
    "                self.memory_based_buffer.append(line_processed)\n",
    "\n",
    "                self.file_position = file_handle.tell()\n",
    "    \n",
    "    def get_documents(self):\n",
    "        \"\"\" This is the only function to be called from outside to have a list of documents ready to be processed.\n",
    "        \n",
    "        Returns:\n",
    "           the memory_based_buffer list of strings\n",
    "            \n",
    "        \"\"\"\n",
    "        self.memory_based_buffer.clear()\n",
    "        self.read_collection()\n",
    "        \n",
    "        return self.memory_based_buffer\n",
    "    \n",
    "    \n",
    "    \n",
    "    def close_file_collection(self):\n",
    "        collection_file.close()\n",
    "         \n",
    "#Qua potrebbe essere utile pensare di farlo con 2 thread.\n",
    "#Quando viene chiamata la funzione get_documents, si restituisce al chiamante il blocco di documenti richiesti\n",
    "#e nel mentre si potrebbe attivare un thread che parallelamente carica il successivo blocco di documenti e lo\n",
    "#salva localmente nella struttura dati, quindi alla prossima chiamata get_documents ho già pronto il blocco e non\n",
    "# devo stare ad aspettare tutta la lettura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "156da327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0 presenc communic amid scientif mind equal import success manhattan project scientif intellect cloud hang impress achiev atom research engin success truli meant hundr thousand innoc live obliter', '1 manhattan project atom bomb help bring end world war ii legaci peac use atom energi continu impact histori scienc', '2 essay manhattan project manhattan project manhattan project see make atom bomb possibl success project would forev chang world forev make known someth power manmad', '3 manhattan project name project conduct world war ii develop first atom bomb refer specif period project 194 2 1946 control u armi corp engin administr general lesli r grove', '4 version volum well complementari websit first websit manhattan project interact histori avail offic histori heritag resourc websit doe gov me70 histori offic histori heritag resourc nation nuclear secur']\n",
      "['5 manhattan project classifi photograph featur first atom bomb weapon atom scientist nicknam gadget nuclear age began juli 16 1945 deton new mexico desert', '6 attempt substitut extraordinarili rich literatur atom bomb end world war ii collect attempt document origin develop manhattan project', '7 manhattan project manhattan project research develop undertak world war ii produc first nuclear weapon led unit state support unit kingdom canada 1942 1946 project direct major general lesli grove u armi corp engin nuclear physicist robert oppenheim director los alamo laboratori design actual bomb armi compon project design', '8 june 1942 unit state armi corp engineersbegan manhattan project secret name 2 atom bomb', '9 one main reason hanford select site manhattan project b reactor proxim columbia river largest river flow pacif ocean north american coast']\n"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/gabri/Desktop/Materie magistrale/Multimedia/progetto/collection.tar.gz\"\n",
    "reader = Collection_Reader(path, 5)\n",
    "# print(\"Primo blocco: \\n\")\n",
    "block_0 = reader.get_documents()\n",
    "\n",
    "assert len(block_0) == reader.MEMORY_BASED_BUFFER_SIZE\n",
    "assert block_0[2] == '2 essay manhattan project manhattan project manhattan project see make atom bomb possibl success project would forev chang world forev make known someth power manmad'\n",
    "print(block_0)\n",
    "\n",
    "# print(\"Secondo blocco: \\n\")\n",
    "block_2 = reader.get_documents()\n",
    "\n",
    "for i, document in enumerate(block_2, start=5):\n",
    "    expected_prefix = str(i)\n",
    "    assert document.startswith(expected_prefix)\n",
    "    \n",
    "print(block_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074fb8a2-7980-4012-85a7-32212e8441dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
