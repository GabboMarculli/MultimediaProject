{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5991f227-2f23-4341-b41e-fb7fb8aed6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\tests\\Query_processing\\../..\\query_processing\\DAAT.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\tests\\Query_processing\\../..\\structures\\DocumentIndex.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\tests\\Query_processing\\../..\\utilities\\General_Utilities.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\tests\\Query_processing\\../..\\structures\\DocumentIndexRow.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\tests\\Query_processing\\../..\\structures\\Lexicon.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\tests\\Query_processing\\../..\\structures\\LexiconRow.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\tests\\Query_processing\\../..\\building_data_structures\\CollectionStatistics.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\tests\\Query_processing\\../..\\structures\\PostingListHandler.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\tests\\Query_processing\\../..\\structures\\InvertedIndex.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\tests\\Query_processing\\../..\\utilities\\Compression.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\tests\\Query_processing\\../..\\structures\\BlockDescriptor.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\gabri\\Documents\\GitHub\\tests\\Query_processing\\../..\\query_processing\\Scoring.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import sys\n",
    "sys.path.append('../../')  # Go up two folders to the project root\n",
    "\n",
    "from query_processing.DAAT import DAAT\n",
    "from structures.LexiconRow import LexiconRow\n",
    "from structures.PostingListHandler import Posting_List_Reader\n",
    "from structures.InvertedIndex import Posting\n",
    "\n",
    "import ipytest\n",
    "ipytest.autoconfig()\n",
    "\n",
    "import tempfile\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09815cda-7e0c-43cb-bb91-281ecbd5ccb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "def check_if_inserted(doc_id, real_action, my_heap) -> bool:\n",
    "    \"\"\"\n",
    "        Pass to the function a doc_id and the list of most relevant document with score.\n",
    "        Real_action is True if the doc_id needs to be inserted.\n",
    "    \"\"\"\n",
    "    for index,elem in enumerate(my_heap):\n",
    "        if my_heap[index][1] == doc_id and real_action == False:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "def test_daat():\n",
    "    # Set up a temporary directory for testing\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        daat_instance = DAAT()\n",
    "        \n",
    "        # Test open_all_posting_lists\n",
    "        daat_instance.open_all_posting_lists()\n",
    "        assert daat_instance.file_DocIds.readable() is True\n",
    "        assert daat_instance.file_Freq.readable() is True\n",
    "        assert daat_instance.file_blocks.readable() is True\n",
    "        assert daat_instance.file_lexicon.readable() is True\n",
    "\n",
    "        # Test reset_lists\n",
    "        daat_instance.reset_lists()\n",
    "        assert len(daat_instance.posting_readers) == 0\n",
    "        assert len(daat_instance.top_k_documents) == 0\n",
    "\n",
    "        # Test close_all_posting_lists\n",
    "        daat_instance.close_all_posting_lists()\n",
    "        assert daat_instance.file_DocIds.closed is True\n",
    "        assert daat_instance.file_Freq.closed is True\n",
    "        assert daat_instance.file_blocks.closed is True\n",
    "        assert daat_instance.file_lexicon.closed is True\n",
    "\n",
    "        # Test initialize_posting_lists (assuming tokens is a list of strings)\n",
    "        daat_instance.open_all_posting_lists()\n",
    "        tokens = [\"happiness\", \"home\", \"between\"]\n",
    "        daat_instance.initialize_posting_lists(tokens)\n",
    "        assert len(daat_instance.posting_readers) == len(tokens)\n",
    "        daat_instance.close_all_posting_lists()\n",
    "\n",
    "        # Test update_heap\n",
    "        scoring_function = \"bm25\"\n",
    "        daat_instance.update_heap(scoring_function, 0, 3, 2) # check if insert works\n",
    "        assert len(daat_instance.top_k_documents) == 1\n",
    "        daat_instance.update_heap(scoring_function, 1, 4, 2)\n",
    "        assert len(daat_instance.top_k_documents) == 2\n",
    "        \n",
    "        for i in range(2,50): # generate 50 others doc_id\n",
    "            freq = random.randint(1, 10) # with frequency\n",
    "            score = daat_instance.scorer.choose_scoring_function(scoring_function, i, freq)\n",
    "            to_insert = (score >= daat_instance.top_k_documents[0][0] or score >= daat_instance.top_k_documents[1][0])\n",
    "            daat_instance.update_heap(scoring_function, i, freq, 2)\n",
    "\n",
    "            assert len(daat_instance.top_k_documents) == 2 # check if lenght remains k\n",
    "            assert check_if_inserted(i, to_insert, daat_instance.top_k_documents) == True # check if heap maintains only highest score\n",
    "\n",
    "        # test scoreQuery\n",
    "        daat = DAAT()\n",
    "        my_list = [\"happiness\", \"home\", \"between\"]\n",
    "        result = daat.scoreQuery(3, \"bm25\", my_list , False)\n",
    "        assert len(result) <= 3        \n",
    "        \n",
    "        # Test min_doc\n",
    "        daat_instance.reset_lists()\n",
    "        assert daat_instance.min_doc() == (-1,-1)\n",
    "\n",
    "        posting_reader = Posting_List_Reader(LexiconRow(\"a\"), False, None, None, None)\n",
    "        posting_reader.get_current_posting = lambda:Posting(50, 3)\n",
    "        daat_instance.posting_readers.append(posting_reader)\n",
    "        min_doc, _ = daat_instance.min_doc()\n",
    "        assert min_doc == 50\n",
    "\n",
    "        posting_reader = Posting_List_Reader(LexiconRow(\"b\"), False, None, None, None)\n",
    "        posting_reader.get_current_posting = lambda:Posting(500, 3)\n",
    "        daat_instance.posting_readers.append(posting_reader)\n",
    "        min_doc, _ = daat_instance.min_doc()\n",
    "        assert min_doc == 50\n",
    "\n",
    "        posting_reader = Posting_List_Reader(LexiconRow(\"c\"), False, None, None, None)\n",
    "        posting_reader.get_current_posting = lambda:Posting(5, 3)\n",
    "        daat_instance.posting_readers.append(posting_reader)\n",
    "        min_doc, _ = daat_instance.min_doc()\n",
    "        assert min_doc == 5\n",
    "\n",
    "        posting_reader = Posting_List_Reader(LexiconRow(\"d\"), False, None, None, None)\n",
    "        posting_reader.get_current_posting = lambda:Posting(5000, 3)\n",
    "        daat_instance.posting_readers.append(posting_reader)\n",
    "        min_doc, _ = daat_instance.min_doc()\n",
    "        assert min_doc == 5\n",
    "\n",
    "        posting_reader = Posting_List_Reader(LexiconRow(\"e\"), False, None, None, None)\n",
    "        posting_reader.get_current_posting = lambda:Posting(23, 3)\n",
    "        daat_instance.posting_readers.append(posting_reader)\n",
    "        min_doc, _ = daat_instance.min_doc()\n",
    "        assert min_doc == 5\n",
    "\n",
    "        posting_reader = Posting_List_Reader(LexiconRow(\"f\"), False, None, None, None)\n",
    "        posting_reader.get_current_posting = lambda:Posting(789, 3)\n",
    "        daat_instance.posting_readers.append(posting_reader)\n",
    "        min_doc, _ = daat_instance.min_doc()\n",
    "        assert min_doc == 5\n",
    "\n",
    "        # Test all_lists_exhausted\n",
    "        daat_instance.reset_lists()\n",
    "        exhausted, current_docs = daat_instance.all_lists_exhausted()\n",
    "        assert exhausted is True\n",
    "        assert all(doc is None for doc in current_docs)\n",
    "    \n",
    "        # Caso in cui almeno una lista non Ã¨ esaurita    \n",
    "        posting_reader = Posting_List_Reader(LexiconRow(\"x\"), False, None, None, None)\n",
    "        posting_reader.get_current_posting = lambda:Posting(31, 9)\n",
    "        daat_instance.posting_readers.append(posting_reader)\n",
    "        posting_reader = Posting_List_Reader(LexiconRow(\"y\"), False, None, None, None)\n",
    "        posting_reader.get_current_posting = lambda:Posting(29, 18)\n",
    "        daat_instance.posting_readers.append(posting_reader)\n",
    "    \n",
    "        exhausted, current_docs = daat_instance.all_lists_exhausted()\n",
    "        assert exhausted is False\n",
    "        assert all(doc is not None for doc in current_docs)\n",
    "        assert len(current_docs) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f06653-91ca-4ad4-90e1-084c959e9fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
