{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55d89b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../')  # Go up two folders to the project root\n",
    "\n",
    "from structures.InvertedIndex import Posting,InvertedIndex\n",
    "\n",
    "from structures.BlockDescriptor import BlockDescriptor\n",
    "from structures.LexiconRow import LexiconRow\n",
    "from structures.PostingListHandler import Posting_List_Reader\n",
    "from building_data_structures.IndexBuilder import IndexBuilder \n",
    "from pre_processing.Decompress_collection import Collection_Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d863cc",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6869bdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f68dd19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Costants\n",
    "\n",
    "DIR_TEMP_FOLDER=\"TEMP\"\n",
    "DIR_TEMP_DOC_ID=\"DOC_ID_TEMP\"\n",
    "DIR_TEMP_FREQ=\"FREQ_TEMP\"\n",
    "DIR_TEMP_LEXICON=\"LEXICON_TEMP\"\n",
    "\n",
    "DIR_LEXICON=\"LEXICON\"\n",
    "DIR_DOC_INDEX=\"DOC_INDEX\"\n",
    "DIR_INVERTED_INDEX=\"INV_INDEX\"\n",
    "\n",
    "PATH_FINAL_LEXICON=\"lexicon.bin\"\n",
    "PATH_FINAL_DOC_IDS=\"doc_ids.bin\"\n",
    "PATH_FINAL_FREQ=\"freq.bin\"\n",
    "PATH_FINAL_BLOCK_DESCRIPTOR=\"block_descriptors.bin\"\n",
    "PATH_FINAL_DOCUMENT_INDEX=\"document_index.bin\"\n",
    "\n",
    "\n",
    "PATH_COLLECTION_STATISTICS=\"collection_statistics.bin\"\n",
    "PATH_COLLECTION_STATISTICS_DEBUG=\"collection_statistics.txt\"\n",
    "\n",
    "PATH_FINAL_INVERTED_INDEX_DEBUG=\"inverted_index.txt\"\n",
    "PATH_FINAL_LEXICON_DEBUG=\"lexicon.txt\"\n",
    "PATH_FINAL_DOCUMENT_INDEX_DEBUG=\"document_index.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e75f576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection_Reader Costructor\n",
      "Using: \n",
      "Testing Mode : True\n",
      "No. of documents in the test collection: 10000\n",
      "No. of parallel processes=1, you can execute it also inside a jupyter notebook.\n",
      "\n",
      "\n",
      "Index Builder Costructor\n",
      "Using: \n",
      "Debug Mode :True\n",
      "Compression Mode :False\n",
      "Nr of posting in each block descriptor: 512\n",
      "\n",
      "\n",
      "\n",
      "SPMI doc processed: 0 Time spent :0.0\n",
      "Merging term processed: aaaaa                          Time spent :0.011991739273071289\n",
      "Merging term processed: bbbbb                          Time spent :0.012943267822265625\n",
      "Merging term processed: dddd                           Time spent :0.010970354080200195\n",
      "Merging term processed: ttt                            Time spent :0.009971857070922852\n",
      "END METHOD!\n"
     ]
    }
   ],
   "source": [
    "test_documents=[]\n",
    "for i in range(0,10000):\n",
    "    if (i%2==0):\n",
    "        test_documents.append(\"doc_\"+str(i)+\"\\t\"+\" aaaaa ttt\")\n",
    "    else:\n",
    "        test_documents.append(\"doc_\"+str(i)+\"\\t\"+\" bbbbb dddd\")\n",
    "        \n",
    "indexBuilder=IndexBuilder(True,False,Collection_Reader(\"\",-1,-1,False,False,test_documents))\n",
    "indexBuilder.single_pass_in_memory_indexing(15000000)\n",
    "indexBuilder.index_merging()\n",
    "\n",
    "\n",
    "#Decomment for doing a manual testing.\n",
    "# lexicon_file_path=os.path.join(DIR_LEXICON, PATH_FINAL_LEXICON)\n",
    "# doc_id_file_path=os.path.join(DIR_INVERTED_INDEX, PATH_FINAL_DOC_IDS)\n",
    "# freq_file_path=os.path.join(DIR_INVERTED_INDEX, PATH_FINAL_FREQ)\n",
    "# block_file_path=os.path.join(DIR_INVERTED_INDEX, PATH_FINAL_BLOCK_DESCRIPTOR)\n",
    "\n",
    "# lexicon_file=open(lexicon_file_path,\"rb\")\n",
    "# doc_id_file=open(doc_id_file_path,\"rb\")\n",
    "# freq_file=open(freq_file_path,\"rb\")\n",
    "# block_file=open(block_file_path,\"rb\")\n",
    "\n",
    "\n",
    "# term_bbb=LexiconRow(\"aaaaa\",0)           \n",
    "# term_bbb.read_lexicon_row_on_disk_from_opened_file(lexicon_file,term_bbb.SIZE_LEXICON_ROW)\n",
    "\n",
    "# #bd=BlockDescriptor()\n",
    "# #block_desc=bd.read_block_descriptor_on_disk_from_opened_file(block_file,0)\n",
    "\n",
    "\n",
    "\n",
    "# pl_reader3=Posting_List_Reader(term_bbb,False,doc_id_file,freq_file,block_file)\n",
    "# for elem in pl_reader3:\n",
    "#     print(elem)\n",
    "\n",
    "# lexicon_file.close()\n",
    "# doc_id_file.close()\n",
    "# freq_file.close()\n",
    "# block_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08be78d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_Posting_List_Reader():\n",
    "    \n",
    "\n",
    "    lexicon_file_path=os.path.join(DIR_LEXICON, PATH_FINAL_LEXICON)\n",
    "    doc_id_file_path=os.path.join(DIR_INVERTED_INDEX, PATH_FINAL_DOC_IDS)\n",
    "    freq_file_path=os.path.join(DIR_INVERTED_INDEX, PATH_FINAL_FREQ)\n",
    "    block_file_path=os.path.join(DIR_INVERTED_INDEX, PATH_FINAL_BLOCK_DESCRIPTOR)\n",
    "    \n",
    "    \n",
    "    lexicon_file=open(lexicon_file_path,\"rb\")\n",
    "    doc_id_file=open(doc_id_file_path,\"rb\")\n",
    "    freq_file=open(freq_file_path,\"rb\")\n",
    "    block_file=open(block_file_path,\"rb\")\n",
    "    \n",
    "    \n",
    "    term_aaa=LexiconRow(\"aaaaa\",0)           \n",
    "    term_aaa.read_lexicon_row_on_disk_from_opened_file(lexicon_file,0)\n",
    "\n",
    "    \n",
    "    pl_reader=Posting_List_Reader(term_aaa,False,doc_id_file,freq_file,block_file)\n",
    "    \n",
    "    assert pl_reader.get_total_blocks()==71\n",
    "    assert pl_reader.get_current_block()==None\n",
    "    assert pl_reader.get_current_posting()==None\n",
    "    \n",
    "    i=0\n",
    "    for elem in pl_reader:\n",
    "        i+=1\n",
    "    \n",
    "    #Check the posting list is traversed at blocks so methods\n",
    "    # __next__ and __update_posting_list__ are working properly.\n",
    "    assert i==5000\n",
    "    \n",
    "    try:\n",
    "        next(pl_reader)\n",
    "        assert 1==0\n",
    "    except Exception as e: \n",
    "        assert 1==1\n",
    "        \n",
    "    pl_reader2=Posting_List_Reader(term_aaa,False,doc_id_file,freq_file,block_file)\n",
    "    \n",
    "    posting=next(pl_reader2)\n",
    "    assert posting.doc_id==0\n",
    "    assert posting.frequency==1\n",
    "    \n",
    "    posting=next(pl_reader2)\n",
    "    \n",
    "    assert posting.doc_id==2\n",
    "    assert posting.frequency==1\n",
    "    \n",
    "    posting=next(pl_reader2)\n",
    "    \n",
    "    assert posting.doc_id==4\n",
    "    assert posting.frequency==1\n",
    "    \n",
    "    \n",
    "    pl_reader3=Posting_List_Reader(term_aaa,False,doc_id_file,freq_file,block_file)\n",
    "    \n",
    "    for i in range (1,80):\n",
    "        posting=next(pl_reader3)\n",
    "        \n",
    "    assert pl_reader3.get_current_block().nr_postings==71\n",
    "    assert pl_reader3.get_current_posting().doc_id==posting.doc_id\n",
    "    assert pl_reader3.get_current_posting().frequency==posting.frequency\n",
    "    \n",
    "    assert pl_reader3.get_current_block().min_doc_id==142\n",
    "    assert pl_reader3.get_current_block().max_doc_id==282\n",
    "    \n",
    "    \n",
    "    #Do the same tests on other terms.\n",
    "    \n",
    "    term_bbb=LexiconRow(\"bbbbb\",0)           \n",
    "    term_bbb.read_lexicon_row_on_disk_from_opened_file(lexicon_file,term_bbb.SIZE_LEXICON_ROW)\n",
    "    \n",
    "    pl_reader=Posting_List_Reader(term_bbb,False,doc_id_file,freq_file,block_file)\n",
    "    \n",
    "    assert pl_reader.get_total_blocks()==71\n",
    "    assert pl_reader.get_current_block()==None\n",
    "    assert pl_reader.get_current_posting()==None\n",
    "    \n",
    "    \n",
    "    \n",
    "    i=0\n",
    "    for elem in pl_reader:\n",
    "        i+=1\n",
    "    \n",
    "    #Check the posting list is traversed at blocks so methods\n",
    "    # __next__ and __update_posting_list__ are working properly.\n",
    "    assert i==5000\n",
    "    \n",
    "    try:\n",
    "        next(pl_reader)\n",
    "        assert 1==0\n",
    "    except Exception as e: \n",
    "        assert 1==1\n",
    "        \n",
    "    pl_reader2=Posting_List_Reader(term_bbb,False,doc_id_file,freq_file,block_file)\n",
    "    \n",
    "    posting=next(pl_reader2)\n",
    "    assert posting.doc_id==1\n",
    "    assert posting.frequency==1\n",
    "    \n",
    "    posting=next(pl_reader2)\n",
    "    \n",
    "    assert posting.doc_id==3\n",
    "    assert posting.frequency==1\n",
    "    \n",
    "    posting=next(pl_reader2)\n",
    "    \n",
    "    assert posting.doc_id==5\n",
    "    assert posting.frequency==1\n",
    "    \n",
    "    \n",
    "    pl_reader3=Posting_List_Reader(term_bbb,False,doc_id_file,freq_file,block_file)\n",
    "    \n",
    "    for i in range (1,80):\n",
    "        posting=next(pl_reader3)\n",
    "        \n",
    "    assert pl_reader3.get_current_block().nr_postings==71\n",
    "    assert pl_reader3.get_current_posting().doc_id==posting.doc_id\n",
    "    assert pl_reader3.get_current_posting().frequency==posting.frequency\n",
    "    \n",
    "    assert pl_reader3.get_current_block().min_doc_id==143\n",
    "    assert pl_reader3.get_current_block().max_doc_id==283\n",
    "    \n",
    "    \n",
    "    lexicon_file.close()\n",
    "    doc_id_file.close()\n",
    "    freq_file.close()\n",
    "    block_file.close()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0f0a08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "def test_Posting_List_Reader_NEXT_GEQ():\n",
    "    \n",
    "    \n",
    "    lexicon_file_path=os.path.join(DIR_LEXICON, PATH_FINAL_LEXICON)\n",
    "    doc_id_file_path=os.path.join(DIR_INVERTED_INDEX, PATH_FINAL_DOC_IDS)\n",
    "    freq_file_path=os.path.join(DIR_INVERTED_INDEX, PATH_FINAL_FREQ)\n",
    "    block_file_path=os.path.join(DIR_INVERTED_INDEX, PATH_FINAL_BLOCK_DESCRIPTOR)\n",
    "    \n",
    "    lexicon_file=open(lexicon_file_path,\"rb\")\n",
    "    doc_id_file=open(doc_id_file_path,\"rb\")\n",
    "    freq_file=open(freq_file_path,\"rb\")\n",
    "    block_file=open(block_file_path,\"rb\")\n",
    "    \n",
    "    \n",
    "    term_aaa=LexiconRow(\"aaaaa\",0)           \n",
    "    term_aaa.read_lexicon_row_on_disk_from_opened_file(lexicon_file,0)\n",
    "\n",
    "    pl_reader=Posting_List_Reader(term_aaa,False,doc_id_file,freq_file,block_file)\n",
    "    \n",
    "    assert pl_reader.get_total_blocks()==71\n",
    "    assert pl_reader.get_current_block()==None\n",
    "    assert pl_reader.get_current_posting()==None\n",
    "    \n",
    "    \n",
    "    current_posting=pl_reader.nextGEQ(80)\n",
    "    \n",
    "    assert current_posting!=None\n",
    "    assert current_posting.doc_id==80\n",
    "    assert current_posting.frequency==1\n",
    "    \n",
    "    #Test that if I pass a lower number of the current_one returns the current_one\n",
    "    current_posting=pl_reader.nextGEQ(4)\n",
    "    \n",
    "    assert current_posting!=None\n",
    "    assert current_posting.doc_id==80\n",
    "    assert current_posting.frequency==1\n",
    "    \n",
    "    current_posting=pl_reader.nextGEQ(81)\n",
    "    \n",
    "    assert current_posting!=None\n",
    "    assert current_posting.doc_id==82\n",
    "    assert current_posting.frequency==1\n",
    "    \n",
    "    \n",
    "    current_posting=pl_reader.nextGEQ(1850)\n",
    "    assert current_posting!=None\n",
    "    assert current_posting.doc_id==1850\n",
    "    assert current_posting.frequency==1\n",
    "    \n",
    "    current_posting=next(pl_reader)\n",
    "    assert current_posting!=None\n",
    "    assert current_posting.doc_id==1852\n",
    "    assert current_posting.frequency==1\n",
    "    \n",
    "    #Exceed the max number in the posting list but still remains on current_value not_replacing it.\n",
    "    current_posting=pl_reader.nextGEQ(100000)\n",
    "    assert current_posting==None\n",
    "    \n",
    "    current_posting=next(pl_reader)\n",
    "    assert current_posting.doc_id==1854\n",
    "    assert current_posting.frequency==1\n",
    "    \n",
    "    i=0\n",
    "    for elem in pl_reader:\n",
    "        i+=1\n",
    "        \n",
    "    current_posting=next(pl_reader)\n",
    "    assert current_posting==None\n",
    "    \n",
    "    \n",
    "    current_posting=pl_reader.nextGEQ(54)\n",
    "    assert current_posting==None\n",
    "    \n",
    "    current_posting=pl_reader.nextGEQ(100000)\n",
    "    assert current_posting==None\n",
    "    \n",
    "    \n",
    "    lexicon_file.close()\n",
    "    doc_id_file.close()\n",
    "    freq_file.close()\n",
    "    block_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8110026c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "def test_fake_test_to_delete_folders():\n",
    "    \n",
    "    if os.path.exists(DIR_LEXICON):\n",
    "        shutil.rmtree(DIR_LEXICON)\n",
    "                \n",
    "    if os.path.exists(DIR_INVERTED_INDEX):\n",
    "        shutil.rmtree(DIR_INVERTED_INDEX)\n",
    "    \n",
    "    if os.path.exists(DIR_DOC_INDEX):\n",
    "        shutil.rmtree(DIR_DOC_INDEX)\n",
    "    \n",
    "    if os.path.exists(DIR_TEMP_FOLDER):\n",
    "        shutil.rmtree(DIR_TEMP_FOLDER)\n",
    "        \n",
    "    assert 1==1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
