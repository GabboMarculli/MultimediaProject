{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "594a7f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55d89b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\tests\\Structures\\../..\\structures\\InvertedIndex.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\tests\\Structures\\../..\\structures\\LexiconRow.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\tests\\Structures\\../..\\structures\\DocumentIndex.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\tests\\Structures\\../..\\utilities\\General_Utilities.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\tests\\Structures\\../..\\structures\\DocumentIndexRow.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\tests\\Structures\\../..\\utilities\\Compression.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\tests\\Structures\\../..\\structures\\BlockDescriptor.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\tests\\Structures\\../..\\structures\\PostingListHandler.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\tests\\Structures\\../..\\building_data_structures\\IndexBuilder.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\tests\\Structures\\../..\\structures\\Lexicon.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\tests\\Structures\\../..\\building_data_structures\\CollectionStatistics.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\tests\\Structures\\../..\\pre_processing\\Decompress_collection.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\tests\\Structures\\../..\\pre_processing\\TextProcessor.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\tests\\Structures\\../..\\query_processing\\Scoring.ipynb\n"
     ]
    }
   ],
   "source": [
    "%%cython\n",
    "import import_ipynb\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../')  # Go up two folders to the project root\n",
    "\n",
    "from structures.InvertedIndex import Posting,InvertedIndex\n",
    "\n",
    "from structures.BlockDescriptor import BlockDescriptor\n",
    "from structures.LexiconRow import LexiconRow\n",
    "from structures.PostingListHandler import Posting_List_Reader\n",
    "from building_data_structures.IndexBuilder import IndexBuilder \n",
    "from pre_processing.Decompress_collection import Collection_Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d863cc",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6869bdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f68dd19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Costants\n",
    "\n",
    "DIR_TEMP_FOLDER=\"TEMP\"\n",
    "DIR_TEMP_DOC_ID=\"DOC_ID_TEMP\"\n",
    "DIR_TEMP_FREQ=\"FREQ_TEMP\"\n",
    "DIR_TEMP_LEXICON=\"LEXICON_TEMP\"\n",
    "\n",
    "DIR_LEXICON=\"LEXICON\"\n",
    "DIR_DOC_INDEX=\"DOC_INDEX\"\n",
    "DIR_INVERTED_INDEX=\"INV_INDEX\"\n",
    "\n",
    "PATH_FINAL_LEXICON=\"lexicon.bin\"\n",
    "PATH_FINAL_DOC_IDS=\"doc_ids.bin\"\n",
    "PATH_FINAL_FREQ=\"freq.bin\"\n",
    "PATH_FINAL_BLOCK_DESCRIPTOR=\"block_descriptors.bin\"\n",
    "PATH_FINAL_DOCUMENT_INDEX=\"document_index.bin\"\n",
    "\n",
    "\n",
    "PATH_COLLECTION_STATISTICS=\"collection_statistics.bin\"\n",
    "PATH_COLLECTION_STATISTICS_DEBUG=\"collection_statistics.txt\"\n",
    "\n",
    "PATH_FINAL_INVERTED_INDEX_DEBUG=\"inverted_index.txt\"\n",
    "PATH_FINAL_LEXICON_DEBUG=\"lexicon.txt\"\n",
    "PATH_FINAL_DOCUMENT_INDEX_DEBUG=\"document_index.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e75f576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection_Reader Costructor\n",
      "Using: \n",
      "Testing Mode : True\n",
      "No. of documents in the test collection: 10000\n",
      "No. of parallel processes=1, you can execute it also inside a jupyter notebook.\n",
      "\n",
      "\n",
      "Index Builder Costructor\n",
      "Using: \n",
      "Debug Mode :True\n",
      "Compression Mode :False\n",
      "Nr of posting in each block descriptor: 512\n",
      "\n",
      "\n",
      "\n",
      "SPMI doc processed: 0 Time spent :0.0\n",
      "Merging term processed: aaaaa                          Time spent :0.011002779006958008\n",
      "Merging term processed: bbbbb                          Time spent :0.010966300964355469\n",
      "Merging term processed: dddd                           Time spent :0.009975910186767578\n",
      "Merging term processed: ttt                            Time spent :0.009970664978027344\n",
      "END METHOD!\n"
     ]
    }
   ],
   "source": [
    "test_documents=[]\n",
    "for i in range(0,10000):\n",
    "    if (i%2==0):\n",
    "        test_documents.append(\"doc\"+str(i)+\"\\t\"+\" aaaaa ttt\")\n",
    "    else:\n",
    "        test_documents.append(\"doc\"+str(i)+\"\\t\"+\" bbbbb dddd\")\n",
    "        \n",
    "indexBuilder=IndexBuilder(True,False,Collection_Reader(\"\",-1,-1,False,False,test_documents))\n",
    "indexBuilder.single_pass_in_memory_indexing(15000000)\n",
    "indexBuilder.index_merging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4347f4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_file_path=os.path.join(DIR_LEXICON, PATH_FINAL_LEXICON)\n",
    "doc_id_file_path=os.path.join(DIR_INVERTED_INDEX, PATH_FINAL_DOC_IDS)\n",
    "freq_file_path=os.path.join(DIR_INVERTED_INDEX, PATH_FINAL_FREQ)\n",
    "block_file_path=os.path.join(DIR_INVERTED_INDEX, PATH_FINAL_BLOCK_DESCRIPTOR)\n",
    "\n",
    "lexicon_file=open(lexicon_file_path,\"rb\")\n",
    "doc_id_file=open(doc_id_file_path,\"rb\")\n",
    "freq_file=open(freq_file_path,\"rb\")\n",
    "block_file=open(block_file_path,\"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "141acb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Error compiling Cython file:\n",
      "------------------------------------------------------------\n",
      "...\n",
      "def funzione():\n",
      "    \n",
      "    term_bbb=LexiconRow(\"the\",0)           \n",
      "            ^\n",
      "------------------------------------------------------------\n",
      "\n",
      "C:\\Users\\Davide\\.ipython\\cython\\_cython_magic_6b024229a97d4b8d8d828342bf0f8cc2.pyx:3:13: undeclared name not builtin: LexiconRow\n",
      "\n",
      "Error compiling Cython file:\n",
      "------------------------------------------------------------\n",
      "...\n",
      "def funzione():\n",
      "    \n",
      "    term_bbb=LexiconRow(\"the\",0)           \n",
      "    term_bbb.read_lexicon_row_on_disk_from_opened_file(lexicon_file,term_bbb.SIZE_LEXICON_ROW*1268662)\n",
      "                                                      ^\n",
      "------------------------------------------------------------\n",
      "\n",
      "C:\\Users\\Davide\\.ipython\\cython\\_cython_magic_6b024229a97d4b8d8d828342bf0f8cc2.pyx:4:55: undeclared name not builtin: lexicon_file\n",
      "\n",
      "Error compiling Cython file:\n",
      "------------------------------------------------------------\n",
      "...\n",
      "    #print(term_bbb.dft)\n",
      "    #bd=BlockDescriptor()\n",
      "    #block_desc=bd.read_block_descriptor_on_disk_from_opened_file(block_file,0)\n",
      "\n",
      "\n",
      "    pl_reader3=Posting_List_Reader(term_bbb,False,doc_id_file,freq_file,block_file)\n",
      "              ^\n",
      "------------------------------------------------------------\n",
      "\n",
      "C:\\Users\\Davide\\.ipython\\cython\\_cython_magic_6b024229a97d4b8d8d828342bf0f8cc2.pyx:11:15: undeclared name not builtin: Posting_List_Reader\n",
      "\n",
      "Error compiling Cython file:\n",
      "------------------------------------------------------------\n",
      "...\n",
      "    #print(term_bbb.dft)\n",
      "    #bd=BlockDescriptor()\n",
      "    #block_desc=bd.read_block_descriptor_on_disk_from_opened_file(block_file,0)\n",
      "\n",
      "\n",
      "    pl_reader3=Posting_List_Reader(term_bbb,False,doc_id_file,freq_file,block_file)\n",
      "                                                 ^\n",
      "------------------------------------------------------------\n",
      "\n",
      "C:\\Users\\Davide\\.ipython\\cython\\_cython_magic_6b024229a97d4b8d8d828342bf0f8cc2.pyx:11:50: undeclared name not builtin: doc_id_file\n",
      "\n",
      "Error compiling Cython file:\n",
      "------------------------------------------------------------\n",
      "...\n",
      "    #print(term_bbb.dft)\n",
      "    #bd=BlockDescriptor()\n",
      "    #block_desc=bd.read_block_descriptor_on_disk_from_opened_file(block_file,0)\n",
      "\n",
      "\n",
      "    pl_reader3=Posting_List_Reader(term_bbb,False,doc_id_file,freq_file,block_file)\n",
      "                                                             ^\n",
      "------------------------------------------------------------\n",
      "\n",
      "C:\\Users\\Davide\\.ipython\\cython\\_cython_magic_6b024229a97d4b8d8d828342bf0f8cc2.pyx:11:62: undeclared name not builtin: freq_file\n",
      "\n",
      "Error compiling Cython file:\n",
      "------------------------------------------------------------\n",
      "...\n",
      "    #print(term_bbb.dft)\n",
      "    #bd=BlockDescriptor()\n",
      "    #block_desc=bd.read_block_descriptor_on_disk_from_opened_file(block_file,0)\n",
      "\n",
      "\n",
      "    pl_reader3=Posting_List_Reader(term_bbb,False,doc_id_file,freq_file,block_file)\n",
      "                                                                       ^\n",
      "------------------------------------------------------------\n",
      "\n",
      "C:\\Users\\Davide\\.ipython\\cython\\_cython_magic_6b024229a97d4b8d8d828342bf0f8cc2.pyx:11:72: undeclared name not builtin: block_file\n"
     ]
    }
   ],
   "source": [
    "%%cython\n",
    "def funzione():\n",
    "    \n",
    "    term_bbb=LexiconRow(\"the\",0)           \n",
    "    term_bbb.read_lexicon_row_on_disk_from_opened_file(lexicon_file,term_bbb.SIZE_LEXICON_ROW*1268662)\n",
    "    #print(term_bbb.term)\n",
    "    #print(term_bbb.dft)\n",
    "    #bd=BlockDescriptor()\n",
    "    #block_desc=bd.read_block_descriptor_on_disk_from_opened_file(block_file,0)\n",
    "\n",
    "\n",
    "    pl_reader3=Posting_List_Reader(term_bbb,False,doc_id_file,freq_file,block_file)\n",
    "\n",
    "    #start=time.time()\n",
    "    i=0\n",
    "    for elem in pl_reader3:\n",
    "        i+=1\n",
    "        #print(elem)\n",
    "    #end=time.time()\n",
    "    #print(i)\n",
    "    #print (\"TOT TIME: \"+str(end-start))\n",
    "\n",
    "    lexicon_file.close()\n",
    "    doc_id_file.close()\n",
    "    freq_file.close()\n",
    "    block_file.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e3be026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.76 s ± 39.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit funzione()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c953def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the                           \n",
      "7714165\n",
      "7714165\n",
      "TOT TIME: 7.0486133098602295\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#Decomment for doing a manual testing.\n",
    "lexicon_file_path=os.path.join(DIR_LEXICON, PATH_FINAL_LEXICON)\n",
    "doc_id_file_path=os.path.join(DIR_INVERTED_INDEX, PATH_FINAL_DOC_IDS)\n",
    "freq_file_path=os.path.join(DIR_INVERTED_INDEX, PATH_FINAL_FREQ)\n",
    "block_file_path=os.path.join(DIR_INVERTED_INDEX, PATH_FINAL_BLOCK_DESCRIPTOR)\n",
    "\n",
    "lexicon_file=open(lexicon_file_path,\"rb\")\n",
    "doc_id_file=open(doc_id_file_path,\"rb\")\n",
    "freq_file=open(freq_file_path,\"rb\")\n",
    "block_file=open(block_file_path,\"rb\")\n",
    "\n",
    "\n",
    "term_bbb=LexiconRow(\"the\",0)           \n",
    "term_bbb.read_lexicon_row_on_disk_from_opened_file(lexicon_file,term_bbb.SIZE_LEXICON_ROW*1268662)\n",
    "print(term_bbb.term)\n",
    "print(term_bbb.dft)\n",
    "#bd=BlockDescriptor()\n",
    "#block_desc=bd.read_block_descriptor_on_disk_from_opened_file(block_file,0)\n",
    "\n",
    "\n",
    "\n",
    "pl_reader3=Posting_List_Reader(term_bbb,False,doc_id_file,freq_file,block_file)\n",
    "\n",
    "start=time.time()\n",
    "i=0\n",
    "for elem in pl_reader3:\n",
    "    i+=1\n",
    "    #print(elem)\n",
    "end=time.time()\n",
    "print(i)\n",
    "print (\"TOT TIME: \"+str(end-start))\n",
    "\n",
    "lexicon_file.close()\n",
    "doc_id_file.close()\n",
    "freq_file.close()\n",
    "block_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08be78d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_Posting_List_Reader():\n",
    "    \n",
    "\n",
    "    lexicon_file_path=os.path.join(DIR_LEXICON, PATH_FINAL_LEXICON)\n",
    "    doc_id_file_path=os.path.join(DIR_INVERTED_INDEX, PATH_FINAL_DOC_IDS)\n",
    "    freq_file_path=os.path.join(DIR_INVERTED_INDEX, PATH_FINAL_FREQ)\n",
    "    block_file_path=os.path.join(DIR_INVERTED_INDEX, PATH_FINAL_BLOCK_DESCRIPTOR)\n",
    "    \n",
    "    \n",
    "    lexicon_file=open(lexicon_file_path,\"rb\")\n",
    "    doc_id_file=open(doc_id_file_path,\"rb\")\n",
    "    freq_file=open(freq_file_path,\"rb\")\n",
    "    block_file=open(block_file_path,\"rb\")\n",
    "    \n",
    "    \n",
    "    term_aaa=LexiconRow(\"aaaaa\",0)           \n",
    "    term_aaa.read_lexicon_row_on_disk_from_opened_file(lexicon_file,0)\n",
    "\n",
    "    \n",
    "    pl_reader=Posting_List_Reader(term_aaa,False,doc_id_file,freq_file,block_file)\n",
    "    \n",
    "    assert pl_reader.get_total_blocks()==71\n",
    "    assert pl_reader.get_current_block()==None\n",
    "    assert pl_reader.get_current_posting()==None\n",
    "    \n",
    "    i=0\n",
    "    for elem in pl_reader:\n",
    "        i+=1\n",
    "    \n",
    "    #Check the posting list is traversed at blocks so methods\n",
    "    # __next__ and __update_posting_list__ are working properly.\n",
    "    assert i==5000\n",
    "    \n",
    "    try:\n",
    "        next(pl_reader)\n",
    "        assert 1==0\n",
    "    except Exception as e: \n",
    "        assert 1==1\n",
    "        \n",
    "    pl_reader2=Posting_List_Reader(term_aaa,False,doc_id_file,freq_file,block_file)\n",
    "    \n",
    "    posting=next(pl_reader2)\n",
    "    assert posting.doc_id==0\n",
    "    assert posting.frequency==1\n",
    "    \n",
    "    posting=next(pl_reader2)\n",
    "    \n",
    "    assert posting.doc_id==2\n",
    "    assert posting.frequency==1\n",
    "    \n",
    "    posting=next(pl_reader2)\n",
    "    \n",
    "    assert posting.doc_id==4\n",
    "    assert posting.frequency==1\n",
    "    \n",
    "    \n",
    "    pl_reader3=Posting_List_Reader(term_aaa,False,doc_id_file,freq_file,block_file)\n",
    "    \n",
    "    for i in range (1,80):\n",
    "        posting=next(pl_reader3)\n",
    "        \n",
    "    assert pl_reader3.get_current_block().nr_postings==71\n",
    "    assert pl_reader3.get_current_posting().doc_id==posting.doc_id\n",
    "    assert pl_reader3.get_current_posting().frequency==posting.frequency\n",
    "    \n",
    "    assert pl_reader3.get_current_block().min_doc_id==142\n",
    "    assert pl_reader3.get_current_block().max_doc_id==282\n",
    "    \n",
    "    \n",
    "    #Do the same tests on other terms.\n",
    "    \n",
    "    term_bbb=LexiconRow(\"bbbbb\",0)           \n",
    "    term_bbb.read_lexicon_row_on_disk_from_opened_file(lexicon_file,term_bbb.SIZE_LEXICON_ROW)\n",
    "    \n",
    "    pl_reader=Posting_List_Reader(term_bbb,False,doc_id_file,freq_file,block_file)\n",
    "    \n",
    "    assert pl_reader.get_total_blocks()==71\n",
    "    assert pl_reader.get_current_block()==None\n",
    "    assert pl_reader.get_current_posting()==None\n",
    "    \n",
    "    \n",
    "    \n",
    "    i=0\n",
    "    for elem in pl_reader:\n",
    "        i+=1\n",
    "    \n",
    "    #Check the posting list is traversed at blocks so methods\n",
    "    # __next__ and __update_posting_list__ are working properly.\n",
    "    assert i==5000\n",
    "    \n",
    "    try:\n",
    "        next(pl_reader)\n",
    "        assert 1==0\n",
    "    except Exception as e: \n",
    "        assert 1==1\n",
    "        \n",
    "    pl_reader2=Posting_List_Reader(term_bbb,False,doc_id_file,freq_file,block_file)\n",
    "    \n",
    "    posting=next(pl_reader2)\n",
    "    assert posting.doc_id==1\n",
    "    assert posting.frequency==1\n",
    "    \n",
    "    posting=next(pl_reader2)\n",
    "    \n",
    "    assert posting.doc_id==3\n",
    "    assert posting.frequency==1\n",
    "    \n",
    "    posting=next(pl_reader2)\n",
    "    \n",
    "    assert posting.doc_id==5\n",
    "    assert posting.frequency==1\n",
    "    \n",
    "    \n",
    "    pl_reader3=Posting_List_Reader(term_bbb,False,doc_id_file,freq_file,block_file)\n",
    "    \n",
    "    for i in range (1,80):\n",
    "        posting=next(pl_reader3)\n",
    "        \n",
    "    assert pl_reader3.get_current_block().nr_postings==71\n",
    "    assert pl_reader3.get_current_posting().doc_id==posting.doc_id\n",
    "    assert pl_reader3.get_current_posting().frequency==posting.frequency\n",
    "    \n",
    "    assert pl_reader3.get_current_block().min_doc_id==143\n",
    "    assert pl_reader3.get_current_block().max_doc_id==283\n",
    "    \n",
    "    \n",
    "    lexicon_file.close()\n",
    "    doc_id_file.close()\n",
    "    freq_file.close()\n",
    "    block_file.close()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0f0a08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "def test_Posting_List_Reader_NEXT_GEQ():\n",
    "    \n",
    "    \n",
    "    lexicon_file_path=os.path.join(DIR_LEXICON, PATH_FINAL_LEXICON)\n",
    "    doc_id_file_path=os.path.join(DIR_INVERTED_INDEX, PATH_FINAL_DOC_IDS)\n",
    "    freq_file_path=os.path.join(DIR_INVERTED_INDEX, PATH_FINAL_FREQ)\n",
    "    block_file_path=os.path.join(DIR_INVERTED_INDEX, PATH_FINAL_BLOCK_DESCRIPTOR)\n",
    "    \n",
    "    lexicon_file=open(lexicon_file_path,\"rb\")\n",
    "    doc_id_file=open(doc_id_file_path,\"rb\")\n",
    "    freq_file=open(freq_file_path,\"rb\")\n",
    "    block_file=open(block_file_path,\"rb\")\n",
    "    \n",
    "    \n",
    "    term_aaa=LexiconRow(\"aaaaa\",0)           \n",
    "    term_aaa.read_lexicon_row_on_disk_from_opened_file(lexicon_file,0)\n",
    "\n",
    "    pl_reader=Posting_List_Reader(term_aaa,False,doc_id_file,freq_file,block_file)\n",
    "    \n",
    "    assert pl_reader.get_total_blocks()==71\n",
    "    assert pl_reader.get_current_block()==None\n",
    "    assert pl_reader.get_current_posting()==None\n",
    "    \n",
    "    \n",
    "    current_posting=pl_reader.nextGEQ(80)\n",
    "    \n",
    "    assert current_posting!=None\n",
    "    assert current_posting.doc_id==80\n",
    "    assert current_posting.frequency==1\n",
    "    \n",
    "    #Test that if I pass a lower number of the current_one returns the current_one\n",
    "    current_posting=pl_reader.nextGEQ(4)\n",
    "    \n",
    "    assert current_posting!=None\n",
    "    assert current_posting.doc_id==80\n",
    "    assert current_posting.frequency==1\n",
    "    \n",
    "    current_posting=pl_reader.nextGEQ(81)\n",
    "    \n",
    "    assert current_posting!=None\n",
    "    assert current_posting.doc_id==82\n",
    "    assert current_posting.frequency==1\n",
    "    \n",
    "    \n",
    "    current_posting=pl_reader.nextGEQ(1850)\n",
    "    assert current_posting!=None\n",
    "    assert current_posting.doc_id==1850\n",
    "    assert current_posting.frequency==1\n",
    "    \n",
    "    current_posting=next(pl_reader)\n",
    "    assert current_posting!=None\n",
    "    assert current_posting.doc_id==1852\n",
    "    assert current_posting.frequency==1\n",
    "    \n",
    "    #Exceed the max number in the posting list but still remains on current_value not_replacing it.\n",
    "    current_posting=pl_reader.nextGEQ(100000)\n",
    "    assert current_posting==None\n",
    "    \n",
    "    current_posting=next(pl_reader)\n",
    "    assert current_posting.doc_id==1854\n",
    "    assert current_posting.frequency==1\n",
    "    \n",
    "    i=0\n",
    "    for elem in pl_reader:\n",
    "        i+=1\n",
    "        \n",
    "    current_posting=next(pl_reader)\n",
    "    assert current_posting==None\n",
    "    \n",
    "    \n",
    "    current_posting=pl_reader.nextGEQ(54)\n",
    "    assert current_posting==None\n",
    "    \n",
    "    current_posting=pl_reader.nextGEQ(100000)\n",
    "    assert current_posting==None\n",
    "    \n",
    "    \n",
    "    lexicon_file.close()\n",
    "    doc_id_file.close()\n",
    "    freq_file.close()\n",
    "    block_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d75bf229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "def test_fake_test_to_delete_folders():\n",
    "    \n",
    "    if os.path.exists(DIR_LEXICON):\n",
    "        shutil.rmtree(DIR_LEXICON)\n",
    "                \n",
    "    if os.path.exists(DIR_INVERTED_INDEX):\n",
    "        shutil.rmtree(DIR_INVERTED_INDEX)\n",
    "    \n",
    "    if os.path.exists(DIR_DOC_INDEX):\n",
    "        shutil.rmtree(DIR_DOC_INDEX)\n",
    "    \n",
    "    if os.path.exists(DIR_TEMP_FOLDER):\n",
    "        shutil.rmtree(DIR_TEMP_FOLDER)\n",
    "        \n",
    "    assert 1==1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
