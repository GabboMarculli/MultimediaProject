{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7104e7ab-9d07-43de-82c0-5061dfb7f246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\pre_processing\\..\\pre_processing\\TextProcessor.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\pre_processing\\..\\utilities\\General_Utilities.ipynb\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import concurrent.futures\n",
    "\n",
    "import sys\n",
    "import import_ipynb\n",
    "\n",
    "from typing import TextIO, BinaryIO\n",
    "from typing import List,Iterator\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from pre_processing.TextProcessor import TextProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acc5e51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This class is used to avoid to load the entire data collection in memory and process a row at a time.\n",
    "    In particular it is designed to load just a partial portion of the entire collection, then uncompress it\n",
    "    and applying the pre-processing tecniques.\n",
    "    It also gives the possibility to run a test-collection, all in complete transparency of the program who uses it.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class Collection_Reader:\n",
    "    \n",
    "    __collection_file_name:str\n",
    "         \n",
    "    __escape_first_row_description:bool\n",
    "    __use_steamming_and_remove_stop_words:bool    \n",
    "        \n",
    "    __num_parallel_processes:int\n",
    "    __max_nr_of_documents_in_memory:int\n",
    "  \n",
    "    __text_processor:TextProcessor\n",
    "    __collection_file: BinaryIO\n",
    "        \n",
    "    __documents:List[str]\n",
    "    __finished:bool\n",
    "        \n",
    "    __test_mode:bool\n",
    "    \n",
    "    def __init__(self,path_collection_file_name:str,\n",
    "                 max_nr_of_documents_in_memory:int,\n",
    "                 num_parallel_processes:int,\n",
    "                 use_steamming_and_remove_stop_words:bool,\n",
    "                 escape_first_row_description:bool,\n",
    "                 collection_test:List[str]=[]):    \n",
    "        \"\"\" Constructor methods for initialization:\n",
    "\n",
    "            Args:\n",
    "                path_collection_file_name: the file location of the collection to be read.\n",
    "                max_nr_of_documents_in_memory: specifies the number of maxium document to be load in main memory. \n",
    "                num_parallel_processes: specifies the number of parallel process to be used during execution of pre-processing stage, in order to speed up the elaboration\n",
    "                use_steamming_and_remove_stop_words: a boolean indicating if using steamming_and_remove_stop_words\n",
    "                escape_first_row_description: a boolean indicating if the first line of document should be treat differently\n",
    "                collection_test: (optional) if present uses the collection indicated instead of the one indicated in path_collection_file_name\n",
    "            \n",
    "        \"\"\"\n",
    "        self.__text_processor = TextProcessor(use_steamming_and_remove_stop_words)\n",
    "        self.__finished=False\n",
    "        \n",
    "        if (collection_test):\n",
    "            self.__test_mode=True\n",
    "            self.__num_parallel_processes=1\n",
    "            \n",
    "            \n",
    "            if (escape_first_row_description):\n",
    "                collection_test[0]=\"0\\t\"+self.__text_processor.process_text(re.sub(r\".*?0\\t\", \"\", collection_test[0]))\n",
    "                \n",
    "                \n",
    "            doc_pre_processed=[]\n",
    "            for doc in collection_test:\n",
    "                doc_pre_processed.append(self.__text_processor.process_text(doc))\n",
    "            \n",
    "            self.__documents=doc_pre_processed\n",
    "            \n",
    "        else:\n",
    "            self.__test_mode=False\n",
    "            \n",
    "            self.__collection_file_name = path_collection_file_name\n",
    "            self.__max_nr_of_documents_in_memory = max_nr_of_documents_in_memory\n",
    "            self.__num_parallel_processes=num_parallel_processes\n",
    "            self.__use_steamming_and_remove_stop_words=use_steamming_and_remove_stop_words\n",
    "\n",
    "            if (self.__max_nr_of_documents_in_memory<=0):\n",
    "                raise ValueError(\"Please enter a nr of documents >=1\")\n",
    "            \n",
    "            if (self.__num_parallel_processes<=0):\n",
    "                raise ValueError(\"Please enter a nr of parallel processes >=1\")\n",
    "            \n",
    "\n",
    "            self.__text_processor = TextProcessor(use_steamming_and_remove_stop_words)\n",
    "            self.__collection_file= gzip.open(self.__collection_file_name, 'rt', encoding='utf-8')\n",
    "            self.__escape_first_row_description=escape_first_row_description\n",
    "\n",
    "            if (escape_first_row_description):\n",
    "                line = self.__collection_file.readline()\n",
    "                result_string = \"0\\t\"+self.__text_processor.process_text(re.sub(r\".*?0\\t\", \"\", line))\n",
    "            \n",
    "                self.__documents=[result_string]\n",
    "            else:\n",
    "                self.__documents=[]\n",
    "                \n",
    "                \n",
    "        print (\"Collection_Reader Costructor\")\n",
    "        \n",
    "        print (\"Using: \")\n",
    "        if (collection_test):\n",
    "            print(\"Testing Mode : True\")\n",
    "            print(\"No. of documents in the test collection: \"+str(len(collection_test)))\n",
    "        else:\n",
    "            print(\"Testing Mode: False \")\n",
    "            print(\"Max Document in memory: \"+str(self.__max_nr_of_documents_in_memory))\n",
    "            print(\"No. of parallel processes: \"+str(self.__num_parallel_processes))\n",
    "            print(\"Use Stemming and stop word removal: \"+str(self.__use_steamming_and_remove_stop_words))\n",
    "        \n",
    "        if (self.__num_parallel_processes>=2):\n",
    "            print(\"No. of parallel processes>=2 be sure to executing  this program outside a jupyter notebook.\")\n",
    "            self.__executor=concurrent.futures.ProcessPoolExecutor(max_workers=self.__num_parallel_processes)\n",
    "            \n",
    "        else:\n",
    "            print(\"No. of parallel processes=1, you can execute it also inside a jupyter notebook.\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "            \n",
    "            \n",
    "    def __close_collection_file(self)->None:\n",
    "        self.__collection_file.close()\n",
    "        self.__finished=True\n",
    "      \n",
    "    def __iter__(self)->None:\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        \"\"\"\n",
    "            This is the operator to call each time you want a new document from the collection.\n",
    "            This method hide to the external, the logic in which a document is returned.\n",
    "            If the max_nr_of_documents_in_memory is reached then it reads from collection disk,\n",
    "            preprocess it, according to the TextProcessor logic module and saves into __documents list. \n",
    "        \"\"\"\n",
    "        if (self.__test_mode):\n",
    "            #In test mode, the disk is not present and the collection is entirly present in memory, so just need a pop.\n",
    "            if (self.__documents):\n",
    "                return self.__documents.pop(0)\n",
    "            else:\n",
    "                raise StopIteration()\n",
    "        else: \n",
    "            #In production, check first if there are documents in memory otherwise call the function to load from disk.\n",
    "            if (self.__documents):\n",
    "                return self.__documents.pop(0)\n",
    "            \n",
    "            if (not self.__documents and not self.__finished):\n",
    "                return self.__read_part_of_collection()\n",
    "            else:\n",
    "                raise StopIteration()\n",
    "            \n",
    "            \n",
    "    def __read_part_of_collection(self)->str:\n",
    "        \"\"\" \n",
    "            This function is responsable for reading to the disk, preprocess the document,save it in local list and\n",
    "            after returned on demand. This function is the most heavyest part so it is called only when no other document\n",
    "            are present in memory.\n",
    "            Pay attention: if this function is called inside a jupyter notebook, check if the number of processes are set to 1,\n",
    "            if chosen >1 it does not run beacuse of the structure of a notebook.\n",
    "            When number of processes>1 multiple parallel processes in a pool are handled by an executor in order to save time during\n",
    "            the creation of the index.\n",
    "              \n",
    "        Returns:\n",
    "            the first text document loaded and preprocessed from disk or None if all the documents on disk are read.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        nr_doc=0\n",
    "        max_doc_per_buffer=math.ceil(self.__max_nr_of_documents_in_memory/self.__num_parallel_processes)\n",
    "        \n",
    "        #Splitting the buffer of documents for each different process.\n",
    "        current_buffer_index=0\n",
    "        array_buffers = [list() for _ in range(self.__num_parallel_processes)]\n",
    "        \n",
    "        while(True):\n",
    "           \n",
    "            if (nr_doc==self.__max_nr_of_documents_in_memory):\n",
    "                break\n",
    "                \n",
    "            line = self.__collection_file.readline()\n",
    "            \n",
    "            #Stop reading from disk contition\n",
    "            if (line==\"\"):\n",
    "                self.__close_collection_file()\n",
    "                break\n",
    "            \n",
    "            array_buffers[current_buffer_index].append(line)\n",
    "            nr_doc+=1 \n",
    "            \n",
    "            if (nr_doc%(max_doc_per_buffer)==0):\n",
    "                current_buffer_index+=1\n",
    "                \n",
    "        if (nr_doc!=0):\n",
    "            \n",
    "            #If execute NOT inside a jupyter notebook\n",
    "            if (self.__num_parallel_processes>=2):\n",
    "                # Submit processing tasks for each line\n",
    "\n",
    "                #print(\"Prima submit executor\")\n",
    "                futures = {self.__executor.submit(execute_CPU_BOUND_preprocessing, i,buffer,self.__use_steamming_and_remove_stop_words): i for i,buffer in enumerate(array_buffers) if len(buffer)>0 }\n",
    "                #print(\"Dopo submit executor\")\n",
    "\n",
    "                try:\n",
    "                    # Wait for all tasks to complete\n",
    "                    concurrent.futures.wait(futures)\n",
    "\n",
    "                    #Join the results from different processes.\n",
    "                    return_list=[]\n",
    "                    for future in futures:\n",
    "                        result = future.result()\n",
    "                        return_list.extend(result)\n",
    "\n",
    "                    self.__documents.clear()\n",
    "                    self.__documents.extend(return_list)\n",
    "                    for buffer in array_buffers:\n",
    "                        buffer.clear()\n",
    "\n",
    "                    return self.__documents.pop(0)\n",
    "                except Exception as e:  \n",
    "                    #print (\"CATCH ESTERNO\")\n",
    "                    print(e)\n",
    "                    \n",
    "            else:\n",
    "                #If execute inside a jupyter notebook, costrained to one single process.\n",
    "                pre_processed_docs=execute_CPU_BOUND_preprocessing(0,array_buffers[0],self.__use_steamming_and_remove_stop_words)\n",
    "                self.__documents=pre_processed_docs\n",
    "        \n",
    "                if (self.__documents):\n",
    "                    return self.__documents.pop(0)\n",
    "                \n",
    "                return None\n",
    "         \n",
    "        #end_part_loop = time.time()\n",
    "        \n",
    "        #print(\"Doc_processed: \"+str(nr_doc)+\" time:\"+str(end_part_loop-start_time_loop))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17bb36ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is istantiated 1 time only and not for each process.\n",
    "tp= TextProcessor(True)\n",
    "\n",
    "def execute_CPU_BOUND_preprocessing(index,buffer,use_steamming_and_remove_stop_words)->List[str]:\n",
    "    \"\"\" This function does simply an CPU bound task that consist in applying preprocessing to a specific document.\n",
    "        Depending on the parameter passed it can simply execute pre-processing or additional appling steam and stop-word\n",
    "        removal. Of course the benefit is obtained when it is used with the second option with multiple parallel processes.\n",
    "    \n",
    "    Args:\n",
    "        index: just a number to differentiate in debug the number of process that is doing the job and how much time it takes.\n",
    "        buffer: a list of string representing each element a different document to process\n",
    "        use_steamming_and_remove_stop_words: boolean to indicate if using steamming and stop words removal\n",
    "    \n",
    "    Returns:\n",
    "        A list of string represented each document preprocessed\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "\n",
    "        #print (\"START execute_CPU_BOUND_preprocessing \"+str(index))\n",
    "        array_return=[]\n",
    "        tp.use_stemming_and_stop_words=use_steamming_and_remove_stop_words\n",
    "        for line in buffer:\n",
    "            pre_processed=tp.process_text(line)\n",
    "            if (pre_processed.strip()!=\"\"):\n",
    "                array_return.append(pre_processed)        \n",
    "       \n",
    "    except Exception as e:  \n",
    "        #print (\"entro dentro il catch del execute_CPU_BOUND_preprocessing\")\n",
    "        print(e)\n",
    "\n",
    "    finally:\n",
    "        return array_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "479a54c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasciare commentato, può tornare utile per vedere una riga specifica della collection,\n",
    "#ci vorrà un pò ma perlomeno ci s'ha un modo già pronto.\n",
    "\n",
    "# i=0\n",
    "# with gzip.open(\"C:/Users/Davide/IR/collection.tar.gz\", 'rt', encoding='utf-8') as gzipped_file:\n",
    "#     while(True):\n",
    "#         line = gzipped_file.readline()\n",
    "#         if (line==\"\"):\n",
    "#             print(\"Fine lettura\")\n",
    "#             break\n",
    "#         if (i==5):\n",
    "#             print(line)\n",
    "#             break\n",
    "#         i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
