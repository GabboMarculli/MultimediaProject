{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d78c54c9-f807-403e-8ab8-cea626f683ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from D:\\GitHub\\developing\\query_performance\\..\\query_processing\\Query_processer.ipynb\n",
      "importing Jupyter notebook from D:\\GitHub\\developing\\query_performance\\..\\pre_processing\\TextProcessor.ipynb\n",
      "importing Jupyter notebook from D:\\GitHub\\developing\\query_performance\\..\\utilities\\General_Utilities.ipynb\n",
      "importing Jupyter notebook from D:\\GitHub\\developing\\query_performance\\..\\query_processing\\DAAT.ipynb\n",
      "importing Jupyter notebook from D:\\GitHub\\developing\\query_performance\\..\\structures\\DocumentIndex.ipynb\n",
      "importing Jupyter notebook from D:\\GitHub\\developing\\query_performance\\..\\structures\\DocumentIndexRow.ipynb\n",
      "importing Jupyter notebook from D:\\GitHub\\developing\\query_performance\\..\\structures\\Lexicon.ipynb\n",
      "importing Jupyter notebook from D:\\GitHub\\developing\\query_performance\\..\\structures\\LexiconRow.ipynb\n",
      "importing Jupyter notebook from D:\\GitHub\\developing\\query_performance\\..\\building_data_structures\\CollectionStatistics.ipynb\n",
      "importing Jupyter notebook from D:\\GitHub\\developing\\query_performance\\..\\structures\\PostingListHandler.ipynb\n",
      "importing Jupyter notebook from D:\\GitHub\\developing\\query_performance\\..\\structures\\InvertedIndex.ipynb\n",
      "importing Jupyter notebook from D:\\GitHub\\developing\\query_performance\\..\\utilities\\Compression.ipynb\n",
      "importing Jupyter notebook from D:\\GitHub\\developing\\query_performance\\..\\structures\\BlockDescriptor.ipynb\n",
      "importing Jupyter notebook from D:\\GitHub\\developing\\query_performance\\..\\query_processing\\Scoring.ipynb\n",
      "[(2.1715948774213896, 7098672), (2.2339165679132975, 7921810), (2.8208321981349416, 8185306)]\n",
      "ci ha messo:  0.0  secondi\n",
      "importing Jupyter notebook from D:\\GitHub\\developing\\query_performance\\..\\query_processing\\MaxScore.ipynb\n",
      "Query 0: []\n",
      "Tempo impiegato per la query: 0.0000 secondi\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      9\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../\u001b[39m\u001b[38;5;124m'\u001b[39m)  \n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mquery_processing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mQuery_processer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Query_processer\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:674\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:634\u001b[0m, in \u001b[0;36m_load_backward_compatible\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\import_ipynb.py:61\u001b[0m, in \u001b[0;36mNotebookLoader.load_module\u001b[1;34m(self, fullname)\u001b[0m\n\u001b[0;32m     59\u001b[0m         code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39minput_transformer_manager\u001b[38;5;241m.\u001b[39mtransform_cell(cell\u001b[38;5;241m.\u001b[39msource)\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;66;03m# run the code in themodule\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m         exec(code, mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns \u001b[38;5;241m=\u001b[39m save_user_ns\n",
      "File \u001b[1;32m<string>:100\u001b[0m\n",
      "File \u001b[1;32m<string>:40\u001b[0m, in \u001b[0;36mprocess_query\u001b[1;34m(self, query, scoring_function, alg, k, isConjunctive)\u001b[0m\n",
      "File \u001b[1;32m<string>:80\u001b[0m, in \u001b[0;36mscoreQuery\u001b[1;34m(self, k, choice_function, tokens, isConjunctive)\u001b[0m\n",
      "File \u001b[1;32m<string>:132\u001b[0m, in \u001b[0;36mupdate_heap\u001b[1;34m(self, choice_function, docToProcess, term_freq, k, dft)\u001b[0m\n",
      "File \u001b[1;32m<string>:21\u001b[0m, in \u001b[0;36mchoose_scoring_function\u001b[1;34m(self, choice, doc_id, term_freq, dft)\u001b[0m\n",
      "File \u001b[1;32m<string>:33\u001b[0m, in \u001b[0;36mcompute_BM25_term\u001b[1;34m(self, doc_id, term_freq, dft, k1, b)\u001b[0m\n",
      "File \u001b[1;32m<string>:86\u001b[0m, in \u001b[0;36mread_doc_index_row_on_disk\u001b[1;34m(self, file, offset)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import csv\n",
    "from typing import Dict, List\n",
    "\n",
    "import import_ipynb\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')  \n",
    "from query_processing.Query_processer import Query_processer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a565b9-77c6-4bdc-8aea-03c1c81eb13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_performance_path = \"../query_performance/msmarco-test2019-queries\"\n",
    "query_file1 = \"msmarco-test2019-queries.tsv\"\n",
    "relevance_file1 = \"2019qrels-pass.txt\"\n",
    "query_file2 = \"msmarco-test2020-queries.tsv\"\n",
    "relevance_file2 = \"2020qrels-pass.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0971448d-4ab0-480c-b677-eb2e4f5e1239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_queries(file_path: str) -> Dict[int, str]:\n",
    "    queries = {}\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.reader(file, delimiter=\"\\t\")\n",
    "        for row in reader:\n",
    "            query_id, query_text = int(row[0]), row[1]\n",
    "            queries[query_id] = query_text # sono 200, essendo poche le metto tutte in memoria\n",
    "    return queries\n",
    "\n",
    "def load_relevance(file_path: str) -> Dict[int, Dict[int, int]]:\n",
    "    relevance = {}\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.reader(file, delimiter=\" \")\n",
    "        for row in reader:\n",
    "            query_id, _, document_id, relevance_score = int(row[0]), row[1], int(row[2]), int(row[3])\n",
    "            if query_id not in relevance:\n",
    "                relevance[query_id] = {}\n",
    "            relevance[query_id][document_id] = relevance_score\n",
    "    return relevance\n",
    "\n",
    "def precision_at_k(query_results: list, relevance_data: dict, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate precision at k for a list of query results, so the number of relevant document between the first k document returner by the system.\n",
    "\n",
    "    Args:\n",
    "        query_results (list): List of query results.\n",
    "        relevance_data (dict): Dictionary mapping document IDs to their relevance scores.\n",
    "        k (int): The number of top results to consider.\n",
    "\n",
    "    Returns:\n",
    "        float: Precision at k.\n",
    "    \"\"\"\n",
    "    # Check if the list of query results is empty\n",
    "    if len(query_results) == 0:\n",
    "        return 0\n",
    "\n",
    "    # # Adjust k if it is greater than the number of query results\n",
    "    # if k > len(query_results):\n",
    "    #     k = len(query_results)\n",
    "\n",
    "    relevant = 0\n",
    "    for i in range(min(k, len(query_results))): # Iterate over the top k results or all results if k is greater than the list length\n",
    "        # Check if the relevance score for the current document ID is greater than 0\n",
    "        if relevance_data.get(query_results[i], 0) > 0:\n",
    "            relevant += 1\n",
    "\n",
    "    # Calculate precision at k\n",
    "    return relevant / k\n",
    "\n",
    "# Mettiamo che una query ha ritornato questi documenti piÃ¹ rilevanti: [2071723,8412682,2874503,527690]\n",
    "# Nella struttura dati relevance vado a cercare col query_id di quella query, mettiamo sia 19335 -> relevance[19335] = {1017759: 0, 1082489: 1, ... }\n",
    "# Qui infine chiamerei:  average_precision([2071723,8412682,2874503,527690], relevance[19335])\n",
    "def average_precision(query_results: List[int], relevance_data: dict) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Average Precision for a list of query results.\n",
    "\n",
    "    Args:\n",
    "        query_results (List[int]): List of document IDs representing the order of query results.\n",
    "        relevance_data (dict): Dictionary mapping document IDs to their relevance scores.\n",
    "\n",
    "    Returns:\n",
    "        float: Average Precision.\n",
    "    \"\"\"\n",
    "    # Check if the list of query results is empty\n",
    "    if len(query_results) == 0:\n",
    "        return 0\n",
    "\n",
    "    kRB = len(query_results)\n",
    "    total_sum = 0\n",
    "\n",
    "    # Iterate from 1 to kRB (inclusive)\n",
    "    for i in range(1, kRB + 1):\n",
    "        # Accumulate precision at each cutoff point\n",
    "        total_sum += precision_at_k(query_results, relevance_data, i)\n",
    "\n",
    "    # Calculate average precision\n",
    "    return total_sum / kRB\n",
    "\n",
    "def mean_average_precision(query_results_list: List[List[int]], relevance_data: List[dict]) -> float:\n",
    "    \"\"\"\n",
    "    Compute Mean Average Precision (MAP) for a list of query results.\n",
    "\n",
    "    Args:\n",
    "        query_results_list (list): List of query results, where each element is a list of document IDs.\n",
    "        relevance_data (dict): Dictionary mapping document IDs to their relevance scores.\n",
    "\n",
    "    Returns:\n",
    "        float: Mean Average Precision (MAP).\n",
    "    \"\"\"\n",
    "    total_queries = len(query_results_list)\n",
    "    if total_queries == 0:\n",
    "        return 0\n",
    "\n",
    "    average_precision_sum = 0\n",
    "    for i, query_results in enumerate(query_results_list):\n",
    "        average_precision_sum += average_precision(query_results_list[i], relevance_data[i])\n",
    "\n",
    "    return average_precision_sum / total_queries\n",
    "\n",
    "def recall_at_k(query_results: List[int], relevance_data: dict, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate recall at k for a list of query results.\n",
    "    The ratio between relevant documents found by system and the total relevant document for that query.\n",
    "\n",
    "    Args:\n",
    "        query_results (list): List of query results.\n",
    "        relevance_data (dict): Dictionary mapping document IDs to their relevance scores.\n",
    "        k (int): The number of top results to consider.\n",
    "\n",
    "    Returns:\n",
    "        float: Recall at k.\n",
    "    \"\"\"\n",
    "    # Check if the list of query results is empty\n",
    "    if len(query_results) == 0:\n",
    "        return 0\n",
    "\n",
    "    # Get the total number of relevant documents\n",
    "    total_relevant = sum(1 for doc_id, relevance in relevance_data if relevance > 0)\n",
    "\n",
    "    # Adjust k if it is greater than the number of query results\n",
    "    if k > len(query_results):\n",
    "        k = len(query_results)\n",
    "\n",
    "    # Count the number of relevant documents among the top k results\n",
    "    relevant_in_top_k = sum(1 for i in range(min(k, len(query_results))) if relevance_data.get(query_results[i], 0) > 0)\n",
    "\n",
    "    # Calculate recall at k\n",
    "    return relevant_in_top_k / total_relevant if total_relevant > 0 else 0\n",
    "\n",
    "def discounted_cumulative_gain(b: int, k: int, query_results: List[int], relevance_data: dict) -> float:        \n",
    "    \"\"\"\n",
    "    Calculate Discounted Cumulative Gain (DCG) for a list of query results.\n",
    "\n",
    "    Args:\n",
    "        b (int): Base of the logarithm.\n",
    "        k (int): The number of top results to consider.\n",
    "        query_results (List[int]): List of document IDs representing the order of query results.\n",
    "        relevance_data (dict): Dictionary mapping document IDs to their relevance scores.\n",
    "\n",
    "    Returns:\n",
    "        float: Discounted Cumulative Gain (DCG).\n",
    "    \"\"\"\n",
    "    DCG = 0\n",
    "\n",
    "    # Iterate from 1 to min(k, len(query_results))\n",
    "    for i in range(1, min(k, len(query_results)) + 1):\n",
    "        # Calculate the gain for the current document and add it to DCG\n",
    "        DCG += ((dict(relevance_data).get(query_results[i-1], 0)) / max(1, math.log(i, b)))\n",
    "\n",
    "    return DCG\n",
    "\n",
    "def evaluate_queries(queries, relevance, flag: bool, scoring_function: str, alg: str) -> Dict[int, List[int]]:\n",
    "    results = {}\n",
    "    query_processer = Query_processer(flag)\n",
    "    \n",
    "    for query_id_to_check, query_text in queries.items():\n",
    "        doc_id_score_pairs = relevance.get(query_id_to_check, {}).items()\n",
    "        # print(\"Quelli giusti erano: \", doc_id_score_pairs)\n",
    "        if len(doc_id_score_pairs) == 0:\n",
    "            continue\n",
    "\n",
    "        result = query_processer.process_query(query_text, scoring_function, alg, 10, False)\n",
    "        # print(\"Trovati: \", result)\n",
    "        print(\"query_id_to_check: \", query_id_to_check)\n",
    "          \n",
    "        # discounted_cumulative_gain(2, 5,result ,doc_id_score_pairs)\n",
    "        print(\"discounted_cumulative_gain: \", discounted_cumulative_gain(10, 10,result ,doc_id_score_pairs))\n",
    "        print(\"recall_at_k: \", recall_at_k(result, doc_id_score_pairs,10))\n",
    "        print(\"average_precision: \", average_precision(result, doc_id_score_pairs))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# def write_sorted_queries_to_file(queries: Dict[int, str], output_file: str) -> None:\n",
    "#     sorted_queries = sorted(queries.items(), key=lambda x: x[0])\n",
    "\n",
    "#     with open(output_file, \"w\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "#         writer = csv.writer(file, delimiter=\"\\t\")\n",
    "#         for query_id, query_text in sorted_queries:\n",
    "#             writer.writerow([query_id, query_text])\n",
    "\n",
    "\n",
    "\n",
    "queries = load_queries(query_performance_path + \"/\" + query_file1)\n",
    "#print(queries)\n",
    "# for id, text in queries.items():\n",
    "#     print(id, \" : \", text)\n",
    "\n",
    "# write_queries_to_file(queries, \"sorted_queries.tsv\") # riscrivili ordinatamente\n",
    "            \n",
    "relevance = load_relevance(query_performance_path + \"/\" + relevance_file1)\n",
    "# print(relevance)\n",
    "# for query_id, document_scores in relevance.items():\n",
    "#     print(f\"Query ID: {query_id}\")\n",
    "#     for doc_id, score in document_scores.items():\n",
    "#         print(f\"  Document ID: {doc_id}, Score: {score}\")\n",
    "# unique_query_ids = len(relevance)\n",
    "# print(unique_query_ids) # solo 43 query_id su 200 totali......\n",
    "# print(relevance[19335]) \n",
    "\n",
    "# print(mean_average_precision([ [2071723,8412682,2874503,527690], [1720388,4379804,8151642,527690] ], [relevance[19335], relevance[131843] ]))\n",
    "# print(discounted_cumulative_gain(2, 5, [8412682,2046505], relevance[19335]))\n",
    "\n",
    "# query_processer = Query_processer(False)\n",
    "# result = query_processer.process_query(queries[19335], \"bm25\", \"daat\", 7, False)\n",
    "# print(result)\n",
    "\n",
    "query_results = evaluate_queries(queries, relevance, False, \"bm25\", \"daat\")\n",
    "\n",
    "# for query_id, result in query_results.items():\n",
    "#     print(f\"Query {query_id}: Relevant Documents {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160e7202-7429-4689-8549-703be97ae2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
