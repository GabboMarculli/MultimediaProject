{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e83cc5-e662-4eaf-a37a-02733de0c323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\query_processing\\..\\structures\\DocumentIndex.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\query_processing\\..\\utilities\\General_Utilities.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\query_processing\\..\\structures\\DocumentIndexRow.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\query_processing\\..\\structures\\Lexicon.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\query_processing\\..\\structures\\LexiconRow.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\query_processing\\..\\building_data_structures\\CollectionStatistics.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\query_processing\\..\\structures\\PostingListHandler.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\query_processing\\..\\structures\\InvertedIndex.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\query_processing\\..\\utilities\\Compression.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\query_processing\\..\\structures\\BlockDescriptor.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\query_processing\\..\\query_processing\\Scoring.ipynb\n"
     ]
    }
   ],
   "source": [
    "#Implemented but not used in this part of the project\n",
    "import heapq\n",
    "import bisect\n",
    "from typing import List, Tuple, BinaryIO\n",
    "from io import BufferedReader\n",
    "\n",
    "import import_ipynb\n",
    "import sys\n",
    "sys.path.append('../')  # Go up two folders to the project root\n",
    "\n",
    "from structures.DocumentIndex import DocumentIndex\n",
    "from structures.Lexicon import Lexicon\n",
    "from structures.PostingListHandler import Posting_List_Reader\n",
    "from query_processing.Scoring import Scoring\n",
    "from building_data_structures.CollectionStatistics import Collection_statistics\n",
    "from structures.InvertedIndex import Posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fdce3ea-2ac4-4fa6-a91f-a99466be4d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_INVERTED_INDEX=\"../building_data_structures/INV_INDEX\"\n",
    "PATH_FINAL_DOC_IDS=\"doc_ids.bin\"\n",
    "PATH_FINAL_FREQ=\"freq.bin\"\n",
    "PATH_FINAL_BLOCK_DESCRIPTOR=\"block_descriptors.bin\"\n",
    "DIR_LEXICON=\"../building_data_structures/LEXICON\"\n",
    "PATH_FINAL_LEXICON=\"lexicon.bin\"\n",
    "\n",
    "DIR_DOC_INDEX=\"../building_data_structures/DOC_INDEX\"\n",
    "PATH_COLLECTION_STATISTICS=\"collection_statistics.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "640c0633-f328-4e1e-b6de-87f1c029b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Max_Score:\n",
    "    lexicon: Lexicon \n",
    "    collection_statistics: Collection_statistics\n",
    "    scorer: Scoring\n",
    "    \n",
    "    file_DocIds: BinaryIO\n",
    "    file_Freq: BinaryIO \n",
    "    file_blocks: BinaryIO \n",
    "    file_lexicon: BinaryIO\n",
    "\n",
    "    posting_readers_list: List[Tuple[Posting_List_Reader, float]] = []\n",
    "    top_k_documents: List[Tuple[float, int]] = []\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lexicon = Lexicon(512)\n",
    "        self.collection_statistics = Collection_statistics(DIR_DOC_INDEX+\"/\"+PATH_COLLECTION_STATISTICS)\n",
    "        self.collection_statistics.read_binary_mode()\n",
    "        self.scorer = Scoring(self.collection_statistics)\n",
    "        # ############### VALUTARE SE APRIRE TUTTE LE POSTING UNA SOLA VOLTA NEL COSTRUTTORE E CHIUDERLE A FINE PROGRAMMA PER RISPARMIARE TEMPO\n",
    "        self.open_all_posting_lists()\n",
    "    \n",
    "    def open_all_posting_lists(self) -> None: \n",
    "        self.file_DocIds = open(DIR_INVERTED_INDEX+\"/\"+PATH_FINAL_DOC_IDS, 'rb') \n",
    "        self.file_Freq = open(DIR_INVERTED_INDEX+\"/\"+PATH_FINAL_FREQ, 'rb') \n",
    "        self.file_blocks = open(DIR_INVERTED_INDEX+\"/\"+PATH_FINAL_BLOCK_DESCRIPTOR, 'rb')\n",
    "        self.file_lexicon = open(DIR_LEXICON+\"/\"+PATH_FINAL_LEXICON, 'rb') \n",
    "\n",
    "        self.scorer.open_files()\n",
    "\n",
    "    def reset_lists(self) -> None:\n",
    "        # This list will contain pointer to the posting lists of all terms \n",
    "        self.posting_readers = []\n",
    "        # This list will contain the k most relevant document\n",
    "        self.top_k_documents = []\n",
    "\n",
    "    def close_all_posting_lists(self):\n",
    "        for file in [self.file_DocIds, self.file_Freq, self.file_blocks, self.file_lexicon]:\n",
    "            file.close()  \n",
    "\n",
    "    def initialize_and_sort_posting_lists(self,tokens: List[str], scoring_function: str) -> None:\n",
    "        \"\"\"\n",
    "        Initializes posting lists for the given tokens, sorted by term upper bound (from smallest to highest).\n",
    "\n",
    "        Args:\n",
    "            tokens (List[str]): List of query tokens.\n",
    "        \"\"\"\n",
    "        for token in tokens:\n",
    "            term_lexicon_row = self.lexicon.get_entry(token)\n",
    "            \n",
    "            if term_lexicon_row is not None:\n",
    "                \n",
    "                if scoring_function == \"tfidf\":\n",
    "                    term_upper_bound = term_lexicon_row.maxTFIDF\n",
    "                elif scoring_function == \"bm25\":\n",
    "                    term_upper_bound = term_lexicon_row.maxBM25\n",
    "                else:\n",
    "                    raise ValueError(\"Not supported scoring function\")\n",
    "\n",
    "                dft = term_lexicon_row.dft\n",
    "                reader = Posting_List_Reader(term_lexicon_row, False, self.file_DocIds, self.file_Freq, self.file_blocks)\n",
    "                self.posting_readers.append({\"reader\": reader, \"term_upper_bound\": term_upper_bound, \"dft\": dft})\n",
    "\n",
    "        self.posting_readers.sort(key=lambda x: x.get(\"term_upper_bound\", float('inf')))\n",
    "        \n",
    "        for reader in self.posting_readers:\n",
    "            next(reader[\"reader\"])\n",
    "\n",
    "    \n",
    "    def all_lists_exhausted(self) -> Tuple[bool, List[Posting]]:\n",
    "        \"\"\"\n",
    "        Checks if all posting lists are exhausted.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[bool, List[[Posting]]: Tuple containing a boolean indicating whether all lists are exhausted\n",
    "            and a list of the current documents in each posting list.\n",
    "        \"\"\"\n",
    "        # Read the next document from each posting list\n",
    "        current_docs = [{\"reader\": reader[\"reader\"].get_current_posting(), \"dft\": reader[\"dft\"]} for reader in self.posting_readers]\n",
    "        \n",
    "        # Check if all readers have reached the end of the list\n",
    "        return all(doc[\"reader\"] is None for doc in current_docs), current_docs\n",
    "            \n",
    "    def min_doc(self, index_first_essential_posting_lists:int) -> Tuple[int, int, int]:\n",
    "        \"\"\"\n",
    "        Retrieves the minimum document ID and its frequency among the current documents in all essential posting lists.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[int, int]: Tuple containing the minimum document ID and its frequency.\n",
    "        \"\"\"\n",
    "        end, _ = self.all_lists_exhausted()\n",
    "\n",
    "        # Check if all readers have reached the end of the list\n",
    "        if end == True:\n",
    "            return -1,-1, -1\n",
    "\n",
    "        current_docs = [{\"reader\": self.posting_readers[i][\"reader\"].get_current_posting(), \"dft\": self.posting_readers[i][\"dft\"]} for i in range(index_first_essential_posting_lists, len(self.posting_readers))]\n",
    "\n",
    "        # Fetch only not null documents\n",
    "        valid_docs = [doc for doc in current_docs if doc[\"reader\"] is not None]\n",
    "\n",
    "        if not valid_docs:\n",
    "            return -1, -1\n",
    "\n",
    "        # Retrieve the documents with min doc_id\n",
    "        min_doc = min(valid_docs, key=lambda x: x[\"reader\"].doc_id)\n",
    "\n",
    "        # Return the minimum doc_id and its frequency\n",
    "        return min_doc[\"reader\"].doc_id, min_doc[\"reader\"].frequency, min_doc[\"dft\"]\n",
    "        \n",
    "    def process_essential_lists_daat(self, index_first_essential_posting_lists:int, doc_to_process: int, scoring_function:str, dft: int) -> float:\n",
    "        \"\"\"\n",
    "        Process the essential posting lists to calculate the partial score for a document.\n",
    "    \n",
    "        Arguments:\n",
    "            index_first_essential_posting_lists (int): Index of the first essential posting list.\n",
    "            doc_to_process (int): Document ID to process.\n",
    "            scoring_function (str): Scoring function to use.\n",
    "    \n",
    "        Returns:\n",
    "            float: Partial score for the document.\n",
    "        \"\"\"\n",
    "        partial_score = 0\n",
    "\n",
    "        for i in range(index_first_essential_posting_lists, len(self.posting_readers)):                \n",
    "            if self.posting_readers[i][\"reader\"].get_current_posting() is not None and self.posting_readers[i][\"reader\"].get_current_posting().doc_id == doc_to_process:\n",
    "                partial_score = partial_score + self.scorer.choose_scoring_function(scoring_function, doc_to_process, self.posting_readers[i][\"reader\"].get_current_posting().frequency, dft)\n",
    "                try:\n",
    "                    next(self.posting_readers[i][\"reader\"])\n",
    "                except StopIteration:\n",
    "                    continue\n",
    "\n",
    "        return partial_score\n",
    "    \n",
    "    def get_index_first_essential_posting_list(self, current_threshold: float) -> int:\n",
    "        \"\"\"\n",
    "        Find the index of the first essential posting list based on the current threshold.\n",
    "    \n",
    "        Arguments:\n",
    "            current_threshold (float): Current threshold value.\n",
    "    \n",
    "        Returns:\n",
    "            int: Index of the first essential posting list, or -1 if not found.\n",
    "        \"\"\"\n",
    "        summed_scores = 0\n",
    "\n",
    "        for index, reader in enumerate(self.posting_readers):\n",
    "            summed_scores = summed_scores + reader[\"term_upper_bound\"]\n",
    "\n",
    "            if summed_scores > current_threshold:\n",
    "                return index\n",
    "\n",
    "        return -1\n",
    "\n",
    "    \"\"\"\n",
    "    get the scores for the input document, given by the non-essential posting list in the array list of the sorted lists\n",
    "    \"\"\"\n",
    "    def process_non_essential_lists_with_skipping(self, first_essential_pl_index: int, doc_to_process: int, scoring_function: str, dft: int) -> float:\n",
    "        \"\"\"\n",
    "        Process non-essential posting lists, skipping documents up to doc_to_process, and calculate the non-essential score.\n",
    "    \n",
    "        Arguments:\n",
    "            first_essential_pl_index (int): Index of the first essential posting list.\n",
    "            doc_to_process (int): Document ID to process.\n",
    "            scoring_function (str): Scoring function to use.\n",
    "    \n",
    "        Returns:\n",
    "            float: Non-essential score for the document.\n",
    "        \"\"\"\n",
    "        nonEssentialScore = 0\n",
    "\n",
    "        for i in range(0, first_essential_pl_index):\n",
    "            if self.posting_readers[i][\"reader\"] is not None and self.posting_readers[i][\"reader\"].get_current_posting().doc_id == doc_to_process:\n",
    "                nonEssentialScore = nonEssentialScore + self.scorer.choose_scoring_function(scoring_function, doc_to_process, self.posting_readers[i][\"reader\"].get_current_posting().frequency, dft)\n",
    "                \n",
    "                try:\n",
    "                    next(self.posting_readers[i][\"reader\"])\n",
    "                except StopIteration:\n",
    "                    continue\n",
    "                    \n",
    "                continue\n",
    "    \n",
    "            posting = self.posting_readers[i][\"reader\"].nextGeq(doc_to_process)\n",
    "            if posting != -1 and posting == doc_to_process:\n",
    "               nonEssentialScore = nonEssentialScore + self.scorer.choose_scoring_function(scoring_function, doc_to_process, posting.frequency, dft)\n",
    "               try:\n",
    "                    next(self.posting_readers[i][\"reader\"])\n",
    "               except StopIteration:\n",
    "                    continue\n",
    "\n",
    "        return nonEssentialScore\n",
    "\n",
    "    def compute_non_essential_tub(self, first_essential_pl_index: int) -> float:\n",
    "        \"\"\"\n",
    "        Compute the sum of term upper bounds for all non-essential posting lists.\n",
    "    \n",
    "        Arguments:\n",
    "            first_essential_pl_index (int): Index of the first essential posting list.\n",
    "    \n",
    "        Returns:\n",
    "            float: Sum of term upper bounds for non-essential posting lists.\n",
    "        \"\"\"\n",
    "        # Sum the term upper bounds for all non-essential posting lists and save them in non_essential_tub\n",
    "        non_essential_tub = 0\n",
    "        for i in range(first_essential_pl_index):\n",
    "            if self.posting_readers[i] is not None:\n",
    "                non_essential_tub += self.posting_readers[i][\"term_upper_bound\"]\n",
    "\n",
    "        return non_essential_tub\n",
    "        \n",
    "    def update_heap(self, k: int, document_upper_bound: float, doc_to_process: int, curr_threshold: float) -> float:\n",
    "        \"\"\"\n",
    "        Update the MinHeap with the current document's information.\n",
    "    \n",
    "        Arguments:\n",
    "            k (int): Size of the MinHeap.\n",
    "            document_upper_bound (float): Upper bound score for the document.\n",
    "            doc_to_process (int): Document ID to process.\n",
    "            curr_threshold (float): Current threshold value.\n",
    "    \n",
    "        Returns:\n",
    "            float: Updated threshold value.\n",
    "        \"\"\"\n",
    "        # Check if the MinHeap is full\n",
    "        heapq.heappush(self.top_k_documents, (document_upper_bound, doc_to_process)) \n",
    "    \n",
    "        # Keep the priority queue of size k.\n",
    "        if len(self.top_k_documents) > k:\n",
    "            _, new_threshold_candidate = heapq.heappop(self.top_k_documents)\n",
    "\n",
    "            # Restituisci il nuovo threshold solo se Ã¨ inferiore al threshold corrente\n",
    "            return min(new_threshold_candidate, curr_threshold)\n",
    "        else:\n",
    "            return curr_threshold        \n",
    "    \n",
    "    def scoreQuery(self, k: int, scoring_function: str, tokens: List[str], isConjunctive: bool) -> List[Tuple[float, int]]:\n",
    "        \"\"\"\n",
    "        Score the given query using a specified scoring function and return the top-k results.\n",
    "    \n",
    "        Args:\n",
    "            k (int): The number of top results to retrieve.\n",
    "            scoring_function (str): The scoring function to use (e.g., \"tfidf\", \"bm25\").\n",
    "            tokens (List[str]): List of query tokens.\n",
    "            isConjunctive (bool): Flag indicating whether to use conjunctive mode.\n",
    "    \n",
    "        Returns:\n",
    "            List[Tuple[float, int]]: List of tuples containing the score and document ID for the top-k results.\n",
    "        \"\"\"\n",
    "        self.open_all_posting_lists()\n",
    "        self.reset_lists()\n",
    "        self.initialize_and_sort_posting_lists(tokens, scoring_function)\n",
    "\n",
    "        # Initialization of current threshold to enter the MinHeap of the results\n",
    "        curr_threshold = -1\n",
    "        curr_threshold_has_been_updated = True\n",
    "        first_essential_pl_index = 0\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                partial_score, document_upper_bound = 0, 0\n",
    "\n",
    "                # Variable to store the sum of term upper bounds of non-essential posting lists\n",
    "                non_essential_tub = 0\n",
    "                \n",
    "                # Check if we must update the division in essential and non-essential posting lists\n",
    "                if curr_threshold_has_been_updated:\n",
    "                    # Divide posting lists to be scored into essential and non-essential posting lists\n",
    "                    first_essential_pl_index = self.get_index_first_essential_posting_list(curr_threshold)\n",
    "\n",
    "                    if first_essential_pl_index == -1:\n",
    "                        break\n",
    "\n",
    "                # Search for the minimum docid to be scored among essential posting lists\n",
    "                doc_to_process, _ , dft = self.min_doc(first_essential_pl_index)\n",
    "    \n",
    "                # Check if there is no docid to be processed\n",
    "                if doc_to_process == -1:\n",
    "                    break\n",
    "\n",
    "                # if conjunctive_mode:\n",
    "                #     doc_to_process = next_geq(doc_to_process)\n",
    "                #     if doc_to_process == -1:\n",
    "                #        break\n",
    "\n",
    "                non_essential_tub = self.compute_non_essential_tub(first_essential_pl_index)\n",
    "                partial_score = self.process_essential_lists_daat(first_essential_pl_index, doc_to_process, scoring_function, dft)\n",
    "                document_upper_bound = non_essential_tub + partial_score\n",
    "\n",
    "                # Check if non-essential posting lists must be processed or not\n",
    "                if document_upper_bound > curr_threshold:\n",
    "                    # Process non-essential posting list skipping all documents up to doc_to_process\n",
    "                    non_essential_scores = self.process_non_essential_lists_with_skipping(first_essential_pl_index, doc_to_process, scoring_function, dft)\n",
    "                    \n",
    "                    # Update document upper bound\n",
    "                    # document_upper_bound = document_upper_bound - non_essential_tub + non_essential_scores\n",
    "                    document_upper_bound = partial_score + non_essential_scores\n",
    "        \n",
    "                    # Check if the document can enter the MinHeap\n",
    "                    if document_upper_bound > curr_threshold:\n",
    "                        curr_threshold = self.update_heap(k, document_upper_bound, doc_to_process, curr_threshold)\n",
    "        \n",
    "                # Check if the current threshold has been updated or not\n",
    "                curr_threshold_has_been_updated = (curr_threshold == document_upper_bound)\n",
    "            except StopIteration:             \n",
    "                end, _ = self.all_lists_exhausted()\n",
    "            \n",
    "                if end == True:\n",
    "                    curr_threshold = self.update_heap(k, document_upper_bound, doc_to_process, curr_threshold)\n",
    "                    break  \n",
    "                else:\n",
    "                    continue \n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error during execution: {e}\")\n",
    "                break\n",
    "    \n",
    "        self.close_all_posting_lists()\n",
    "        self.scorer.close_files()\n",
    "        # self.top_k_documents.sort(key=lambda x: x[0], reverse= True)\n",
    "        \n",
    "        return self.top_k_documents\n",
    "\n",
    "##############################################################################################\n",
    "    def nextGEQ(self, docid_to_process: int) -> int:\n",
    "        \"\"\"\n",
    "        Find the next document ID greater than or equal to the specified docid_to_process.\n",
    "    \n",
    "        Args:\n",
    "            docid_to_process (int): Document ID to process.\n",
    "    \n",
    "        Returns:\n",
    "            int: The next document ID greater than or equal to docid_to_process.\n",
    "                 Returns -1 if no such document is found.\n",
    "        \"\"\"\n",
    "        next_geq = docid_to_process # Initialize the candidate next document ID\n",
    "\n",
    "        for i in range(len(self.posting_readers)): # Iterate over each posting list\n",
    "            curr_posting_list = self.posting_readers[i][\"reader\"] # Get the i-th posting list\n",
    "\n",
    "            # check if there are postings to iterate in the i-th posting list\n",
    "            if curr_posting_list is not None:\n",
    "                pointed_posting = curr_posting_list.get_current_posting() # Get the current pointed posting in the i-th posting list\n",
    "\n",
    "                if pointed_posting is None: # If there are no more postings in the current list\n",
    "                    return -1\n",
    "\n",
    "                # If the current pointed posting has a doc_id less than docid_to_process,\n",
    "                # move to the next posting with a doc_id greater than or equal to docid_to_process\n",
    "                if pointed_posting.doc_id < next_geq:\n",
    "                    # If there is no doc_id greater than or equal to docid_to_process in the current posting list, return -1\n",
    "                    pointed_posting = curr_posting_list.nextGEQ(next_geq)\n",
    "\n",
    "                    # check if in the current posting list there is no docid >= docidToProcess to be processed\n",
    "                    if pointed_posting is None:\n",
    "                        return -1\n",
    "\n",
    "                # If the current posting list has a doc_id greater than docid_to_process,\n",
    "                # update next_geq to the new candidate next docid and reset the loop index to check all posting lists again\n",
    "                if pointed_posting.doc_id > next_geq:\n",
    "                    next_geq = pointed_posting.doc_id\n",
    "                    i = -1\n",
    "\n",
    "        return next_geq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "637f3a89-4d59-4fde-a63e-039204882cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_score = Max_Score()\n",
    "\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "\n",
    "# print(max_score.scoreQuery(10, \"bm25\", [\"I\", \"am\" ,\"very\", \"hungry\", \"ciao\"], False))\n",
    "# print(\"ci ha messo: \", time.time() - start_time , \" secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdb2d705-65c4-4873-a299-e29c34d37357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list = []\n",
    "\n",
    "# reader = \"a\"\n",
    "# term_upper_bound = 15\n",
    "# index_to_insert = bisect.bisect_left([entry[1] for entry in list], term_upper_bound)\n",
    "# list.insert(index_to_insert, (reader, term_upper_bound))\n",
    "# print(list)\n",
    "\n",
    "# reader = \"b\"\n",
    "# term_upper_bound = 20\n",
    "# index_to_insert = bisect.bisect_left([entry[1] for entry in list], term_upper_bound)\n",
    "# list.insert(index_to_insert, (reader, term_upper_bound))\n",
    "# print(list)\n",
    "\n",
    "# reader = \"c\"\n",
    "# term_upper_bound = 5\n",
    "# index_to_insert = bisect.bisect_left([entry[1] for entry in list], term_upper_bound)\n",
    "# list.insert(index_to_insert, (reader, term_upper_bound))\n",
    "# print(list)\n",
    "\n",
    "# reader = \"d\"\n",
    "# term_upper_bound = 2\n",
    "# index_to_insert = bisect.bisect_left([entry[1] for entry in list], term_upper_bound)\n",
    "# list.insert(index_to_insert, (reader, term_upper_bound))\n",
    "# print(list)\n",
    "\n",
    "# reader = \"e\"\n",
    "# term_upper_bound = 17\n",
    "# index_to_insert = bisect.bisect_left([entry[1] for entry in list], term_upper_bound)\n",
    "# list.insert(index_to_insert, (reader, term_upper_bound))\n",
    "# print(list)\n",
    "\n",
    "# reader = \"f\"\n",
    "# term_upper_bound = 12\n",
    "# index_to_insert = bisect.bisect_left([entry[1] for entry in list], term_upper_bound)\n",
    "# list.insert(index_to_insert, (reader, term_upper_bound))\n",
    "# print(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85426769-1c08-4f7b-8a47-d6a95139ebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_sc = Max_Score()\n",
    "# max_sc.open_all_posting_lists()\n",
    "# max_sc.reset_lists()\n",
    "# max_sc.initialize_and_sort_posting_lists([\"cat\",\"dog\"], \"tfidf\")\n",
    "\n",
    "# # Divide posting lists to be scored into essential and non-essential posting lists\n",
    "# first_essential_pl_index = max_sc.get_index_first_essential_posting_list(-1)\n",
    "\n",
    "# print(\"first_essential_pl_index: \", first_essential_pl_index)\n",
    "\n",
    "# doc_to_process, freq, dft = max_sc.min_doc(first_essential_pl_index)\n",
    "\n",
    "# print(\"doc_to_process: \", doc_to_process)\n",
    "# print(\"freq: \", freq)\n",
    "# print(\"dft: \", dft)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
