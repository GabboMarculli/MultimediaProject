{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0736fda9-8217-4c09-93f7-d7da2991db5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\query_processing\\..\\structures\\DocumentIndex.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\query_processing\\..\\utilities\\General_Utilities.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\query_processing\\..\\structures\\DocumentIndexRow.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\query_processing\\..\\structures\\Lexicon.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\query_processing\\..\\structures\\LexiconRow.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\query_processing\\..\\building_data_structures\\CollectionStatistics.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\query_processing\\..\\structures\\PostingListHandler.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\query_processing\\..\\structures\\InvertedIndex.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\query_processing\\..\\utilities\\Compression.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\query_processing\\..\\structures\\BlockDescriptor.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\query_processing\\..\\query_processing\\Scoring.ipynb\n"
     ]
    }
   ],
   "source": [
    "#Implemented but not used in this part of the project\n",
    "import heapq\n",
    "from typing import List, Tuple\n",
    "from io import BufferedReader\n",
    "import os\n",
    "import time\n",
    "\n",
    "import import_ipynb\n",
    "import sys\n",
    "sys.path.append('../')  # Go up two folders to the project root\n",
    "\n",
    "from structures.DocumentIndex import DocumentIndex\n",
    "from structures.Lexicon import Lexicon\n",
    "from structures.PostingListHandler import Posting_List_Reader\n",
    "from query_processing.Scoring import Scoring\n",
    "from building_data_structures.CollectionStatistics import Collection_statistics\n",
    "from structures.InvertedIndex import Posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01c44daf-7d90-4fc4-a557-2c299d9afef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_INVERTED_INDEX=\"../building_data_structures/INV_INDEX\"\n",
    "PATH_FINAL_DOC_IDS=\"doc_ids.bin\"\n",
    "PATH_FINAL_FREQ=\"freq.bin\"\n",
    "PATH_FINAL_BLOCK_DESCRIPTOR=\"block_descriptors.bin\"\n",
    "DIR_LEXICON=\"../building_data_structures/LEXICON\"\n",
    "PATH_FINAL_LEXICON=\"lexicon.bin\"\n",
    "\n",
    "DIR_DOC_INDEX=\"../building_data_structures/DOC_INDEX\"\n",
    "PATH_COLLECTION_STATISTICS=\"collection_statistics.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be38f306-76bc-4967-8009-b575f5f29bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAAT():\n",
    "    file_DocIds: BufferedReader\n",
    "    file_Freq: BufferedReader\n",
    "    file_blocks: BufferedReader\n",
    "    posting_readers: List[Tuple[Posting_List_Reader, int]] = []\n",
    "    top_k_documents: List[Tuple[float, int]] = []\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lexicon = Lexicon(512)\n",
    "        self.collection_statistics = Collection_statistics(DIR_DOC_INDEX+\"/\"+PATH_COLLECTION_STATISTICS)\n",
    "        self.collection_statistics.read_binary_mode()\n",
    "        self.scorer = Scoring(self.collection_statistics)\n",
    "    \n",
    "    def open_all_posting_lists(self) -> None: \n",
    "        self.file_DocIds = open(DIR_INVERTED_INDEX+\"/\"+PATH_FINAL_DOC_IDS, 'rb') \n",
    "        self.file_Freq = open(DIR_INVERTED_INDEX+\"/\"+PATH_FINAL_FREQ, 'rb') \n",
    "        self.file_blocks = open(DIR_INVERTED_INDEX+\"/\"+PATH_FINAL_BLOCK_DESCRIPTOR, 'rb')\n",
    "        self.file_lexicon = open(DIR_LEXICON+\"/\"+PATH_FINAL_LEXICON, 'rb') \n",
    "        \n",
    "        self.scorer.open_files()\n",
    "\n",
    "    def reset_lists(self) -> None:\n",
    "        # This list will contain pointer to the posting lists of all terms \n",
    "        self.posting_readers = []\n",
    "        # This list will contain the k most relevant document\n",
    "        self.top_k_documents = []\n",
    "\n",
    "    def close_all_posting_lists(self):\n",
    "        for file in [self.file_DocIds, self.file_Freq, self.file_blocks, self.file_lexicon]:\n",
    "            file.close()      \n",
    "\n",
    "    def scoreQuery(self, k: int, choice_function: str, tokens: List[str], isConjunctive: bool) -> List[Tuple[float, int]]:\n",
    "        \"\"\"\n",
    "        Scores a query and returns the top-k documents.\n",
    "\n",
    "        Args:\n",
    "            k (int): The number of top documents to retrieve.\n",
    "            choice_function (str): The scoring function to use.\n",
    "            tokens (List[str]): List of query tokens.\n",
    "            isConjunctive (bool): Whether the query is conjunctive.\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[float, int]]: List of top-k documents with their scores.\n",
    "        \"\"\"        \n",
    "        tempo_inizio=time.time()\n",
    "        self.open_all_posting_lists()\n",
    "        self.reset_lists()\n",
    "        tempo_fine=time.time()\n",
    "        print(\"Tempo configuraazioni:\"+str(tempo_fine-tempo_inizio))\n",
    "        \n",
    "        tempo_inizio=time.time()\n",
    "        self.initialize_posting_lists(tokens)\n",
    "        tempo_fine=time.time()\n",
    "        \n",
    "        print(\"Tempo ricerca binaria:\"+str(tempo_fine-tempo_inizio))\n",
    "        old_doc_id = -1 # used for the last posting list\n",
    "        counter = 0\n",
    "        tot_time=0\n",
    "        tot_min=0\n",
    "        tot_time_heap=0\n",
    "        tempo_inizio_loop=time.time()\n",
    "        while True:\n",
    "            try:\n",
    "                # Retrieve the minimum doc_id, the next to process\n",
    "                tempo_inizio_min=time.time()\n",
    "                docToProcess, term_freq, dft = self.min_doc()\n",
    "                tempo_fine_min=time.time()\n",
    "                tot_min+=tempo_fine_min-tempo_inizio_min\n",
    "                #print(\"MIN_DOC:\"+str(docToProcess))\n",
    "                # Check if there are no other doc to process\n",
    "                if docToProcess == -1:\n",
    "                    break\n",
    "                counter+=1\n",
    "                # If i have read a new doc_id\n",
    "                if docToProcess != old_doc_id:\n",
    "                    term_freq = 0 # reset term_freq\n",
    "                    old_doc_id = docToProcess # update old doc_id\n",
    "\n",
    "                if isConjunctive:\n",
    "                    current_docs = [reader[\"reader\"] for reader in self.posting_readers if reader[\"reader\"].get_current_posting() is not None]\n",
    "\n",
    "                    # Check if the document is present in all posting lists\n",
    "                    if any(post.get_current_posting().doc_id != docToProcess for post in current_docs):\n",
    "                        for reader in current_docs: \n",
    "                            if reader.get_current_posting().doc_id == docToProcess: # next if doc_id equal to min_doc_id\n",
    "                                next(reader)\n",
    "                        continue\n",
    "                tempo_inizio=time.time()\n",
    "                for reader in self.posting_readers:\n",
    "                    if reader[\"reader\"].get_current_posting() is not None and reader[\"reader\"].get_current_posting().doc_id == docToProcess:\n",
    "                        term_freq += reader[\"reader\"].get_current_posting().frequency\n",
    "                        next(reader[\"reader\"])\n",
    "                tempo_fine=time.time()\n",
    "                tot_time+=(tempo_fine-tempo_inizio)\n",
    "                \n",
    "                tempo_inizio_heap=time.time()\n",
    "                self.update_heap(choice_function, docToProcess, term_freq, k, dft)\n",
    "                tempo_fine_heap=time.time()\n",
    "                tot_time_heap+=(tempo_fine_heap-tempo_inizio_heap)\n",
    "                \n",
    "            except StopIteration:\n",
    "                    end, _ = self.all_lists_exhausted()\n",
    "                \n",
    "                    if end == True:\n",
    "                        self.update_heap(choice_function, docToProcess, term_freq, k, dft)\n",
    "                        break  \n",
    "                    else:\n",
    "                        continue  \n",
    "            except Exception as e:\n",
    "                print(f\"Error during execution: {e}\")\n",
    "                break\n",
    "        tempo_fine_loop=time.time()   \n",
    "        \n",
    "        self.close_all_posting_lists()\n",
    "        self.scorer.close_files()\n",
    "        print(\"TOT_READING:\"+str(tot_time))\n",
    "        print(\"counter:\"+str(counter))\n",
    "        print(\"TOT_MIN:\"+str(tot_min))\n",
    "        print(\"TOT_HEAP:\"+str(tot_time_heap))\n",
    "        print(\"TOT_LOOP:\"+str(tempo_fine_loop-tempo_inizio_loop))\n",
    "        return self.top_k_documents\n",
    "\n",
    "    def initialize_posting_lists(self,tokens: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Initializes posting lists for the given tokens.\n",
    "\n",
    "        Args:\n",
    "            tokens (List[str]): List of query tokens.\n",
    "        \"\"\"\n",
    "        for token in tokens:\n",
    "            term_lexicon_row = self.lexicon.get_entry(token)\n",
    "        \n",
    "            if term_lexicon_row is not None:\n",
    "                dft = term_lexicon_row.dft\n",
    "                reader = Posting_List_Reader(term_lexicon_row, False, self.file_DocIds, self.file_Freq,self.file_blocks)\n",
    "                \n",
    "                # passare anche  term_lexicon_row.docidOffset, term_lexicon_row.frequencyOffset? \n",
    "                self.posting_readers.append({\"reader\": reader, \"dft\": dft})\n",
    "                \n",
    "        for reader in self.posting_readers:\n",
    "            try:\n",
    "                next(reader[\"reader\"])\n",
    "            except StopIteration:\n",
    "                continue\n",
    "\n",
    "    def update_heap(self,choice_function: str, docToProcess: int, term_freq: int, k: int, dft:int) -> None:\n",
    "        \"\"\"\n",
    "        Updates the priority queue (heap) with the latest document score.\n",
    "\n",
    "        Args:\n",
    "            choice_function (str): The scoring function to use.\n",
    "            docToProcess (int): Document ID to process.\n",
    "            term_freq (int): Term frequency in the document.\n",
    "            k (int): The number of top documents to retrieve.\n",
    "        \"\"\"\n",
    "        doc_score = self.scorer.choose_scoring_function(choice_function, docToProcess, term_freq, dft)\n",
    "        \n",
    "        # Add the element to the priority queue\n",
    "        heapq.heappush(self.top_k_documents, (doc_score, docToProcess)) \n",
    "    \n",
    "        # Keep the priority queue of size k.\n",
    "        if len(self.top_k_documents) > k:\n",
    "            heapq.heappop(self.top_k_documents) \n",
    "\n",
    "    def all_lists_exhausted(self) -> Tuple[bool, List[Posting]]:\n",
    "        \"\"\"\n",
    "        Checks if all posting lists are exhausted.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[bool, List[[Posting]]: Tuple containing a boolean indicating whether all lists are exhausted\n",
    "            and a list of the current documents in each posting list.\n",
    "        \"\"\"\n",
    "        # Read the next document from each posting list\n",
    "        current_docs = [{\"reader\": reader[\"reader\"].get_current_posting(), \"dft\": reader[\"dft\"]} for reader in self.posting_readers]\n",
    "        \n",
    "        # Check if all readers have reached the end of the list\n",
    "        return all(doc[\"reader\"] is None for doc in current_docs), current_docs\n",
    "\n",
    "    def min_doc(self) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Retrieves the minimum document ID and its frequency among the current documents in all posting lists.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[int, int]: Tuple containing the minimum document ID and its frequency.\n",
    "        \"\"\"\n",
    "        \n",
    "        end, current_docs = self.all_lists_exhausted()\n",
    "        \n",
    "        if end == True:\n",
    "            return -1, -1, -1\n",
    "\n",
    "        # Fetch only not null documents                      \n",
    "        valid_docs = [doc for doc in current_docs if doc[\"reader\"] is not None]\n",
    "\n",
    "        # Retrieve the documents with min doc_id\n",
    "        min_doc = min(valid_docs, key=lambda x: x[\"reader\"].doc_id)\n",
    "\n",
    "        # Return the minimum doc_id and its frequency\n",
    "        return min_doc[\"reader\"].doc_id, min_doc[\"reader\"].frequency, min_doc[\"dft\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06608c5f-1b7c-490f-ab78-792fcee5be52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# daat = DAAT()\n",
    "# # my_list = [\"dogs\", \"are\",\"beautiful\"]\n",
    "# my_list = [\"comparisonsof\", \"countrythe\",\"gwine\"]\n",
    "# import time\n",
    "\n",
    "# start_time = time.time()\n",
    "# print(daat.scoreQuery(5, \"bm25\", [\"gwine\"] , False))\n",
    "# print(\"ci ha messo: \", time.time() - start_time , \" secondi\")\n",
    "\n",
    "# daat.open_all_posting_lists()\n",
    "# daat.reset_lists()\n",
    "# daat.initialize_posting_lists(my_list)\n",
    "\n",
    "# term_lexicon_row = daat.lexicon.get_entry(\"comparisonsof\")\n",
    "# reader = Posting_List_Reader(term_lexicon_row, False, daat.file_DocIds, daat.file_Freq, daat.file_blocks)\n",
    "# daat.posting_readers.append({\"reader\": reader, \"dft\": term_lexicon_row.dft})\n",
    "# print(term_lexicon_row.dft)\n",
    "\n",
    "# while True:\n",
    "#     next(reader)\n",
    "#     print(reader.get_current_posting())\n",
    "#     print(reader.lexicon_elem.term)\n",
    "\n",
    "# term_lexicon_row = daat.lexicon.get_entry(\"countrythe\")\n",
    "# reader = Posting_List_Reader(term_lexicon_row, False, daat.file_DocIds, daat.file_Freq, daat.file_blocks)\n",
    "# next(reader)\n",
    "# daat.posting_readers.append({\"reader\": reader, \"dft\": term_lexicon_row.dft})\n",
    "# print(reader.lexicon_elem.term)\n",
    "# print(term_lexicon_row.dft)\n",
    "# print(reader.get_current_posting())\n",
    "\n",
    "# term_lexicon_row = daat.lexicon.get_entry(\"gwine\")\n",
    "# reader = Posting_List_Reader(term_lexicon_row, False, daat.file_DocIds, daat.file_Freq, daat.file_blocks)\n",
    "# next(reader)\n",
    "# daat.posting_readers.append({\"reader\": reader, \"dft\": term_lexicon_row.dft})\n",
    "# print(reader.lexicon_elem.term)\n",
    "# print(term_lexicon_row.dft)\n",
    "# print(reader.get_current_posting())\n",
    "\n",
    "# file_path = \"../building_data_structures/INV_INDEX/inverted_index.txt\"\n",
    "# with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#     for line_number, line in enumerate(file, start=1):\n",
    "#         # 7098672:1 7921810:1 8185306:1\n",
    "#         if \"gwine\" in line:\n",
    "#             print(f\"Trovata la parola colega alla riga {line_number}:\\n{line}\")\n",
    "#         # 808155:1 808156:1\n",
    "#         if \"comparisonsof\" in line:\n",
    "#             print(f\"Trovata la parola comparisonsof alla riga {line_number}:\\n{line}\")\n",
    "#         # 472327:1 472334:1 908563:1 1791331:1\n",
    "#         if \"countrythe\" in line:\n",
    "#             print(f\"Trovata la parola countrythe alla riga {line_number}:\\n{line}\")\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b74b4305-2f68-4a1b-8b8b-c7755dbf12f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# VALUTARE SE UNA CLASSE SIMILE RENDEREBBE PIU' LENTA/VELOCE L'ESECUZIONE\n",
    "# Al posto di avere la lista top_k_documents e un parametro k \n",
    "####################################################\n",
    "\n",
    "# class MinHeap:\n",
    "#     def __init__(self, k: int):\n",
    "#         self.heap = []\n",
    "#         self.k = k\n",
    "\n",
    "#     def push(self, item: Tuple[float, int]) -> None:\n",
    "#         heapq.heappush(self.heap, item)\n",
    "#         if len(self.heap) > self.k:\n",
    "#             heapq.heappop(self.heap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bc4a102-32c8-4bf1-b00b-898690307cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se facessi cosi???\n",
    "\n",
    "# from queue import PriorityQueue\n",
    "\n",
    "# class YourClass:\n",
    "#     def __init__(self):\n",
    "#         self.top_k_documents = PriorityQueue()\n",
    "\n",
    "#     def update_heap(self, choice_function: str, docToProcess: int, term_freq: int, k: int) -> None:\n",
    "#         doc_score = self.scorer.choose_scoring_function(choice_function, docToProcess, term_freq)\n",
    "#         # Aggiungi l'elemento alla coda con priorità\n",
    "#         self.top_k_documents.put((doc_score, docToProcess))\n",
    "\n",
    "#         # Mantieni la coda con priorità di dimensione k\n",
    "#         if self.top_k_documents.qsize() > k:\n",
    "#             self.top_k_documents.get()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0d1d3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<structures.LexiconRow.LexiconRow object at 0x000001ECF6C25070>\n",
      "Il termine countrythe dovrebbe stampare 472327:1 472334:1 908563:1 1791331:1\n",
      "Posting(doc_id=472327, frequency=1)\n",
      "Posting(doc_id=472334, frequency=1)\n",
      "Posting(doc_id=908563, frequency=1)\n",
      "Posting(doc_id=1791331, frequency=1)\n",
      "Il termine comparisonof dovrebbe stampare 808155:1 808156:1\n",
      "Posting(doc_id=808155, frequency=1)\n",
      "Posting(doc_id=808156, frequency=1)\n"
     ]
    }
   ],
   "source": [
    "daat = DAAT()\n",
    "daat.open_all_posting_lists()\n",
    "daat.reset_lists()\n",
    "daat.initialize_posting_lists([\"comparisonsof\", \"countrythe\"])\n",
    "\n",
    "term_lexicon_row = daat.lexicon.get_entry(\"countrythe\") \n",
    "\n",
    "print(term_lexicon_row)\n",
    "\n",
    "reader = Posting_List_Reader(term_lexicon_row, False, daat.file_DocIds, daat.file_Freq, daat.file_blocks)\n",
    "\n",
    "\n",
    "print(\"Il termine countrythe dovrebbe stampare 472327:1 472334:1 908563:1 1791331:1\")\n",
    "while True:\n",
    "    try:\n",
    "        next(reader)\n",
    "        print(reader.get_current_posting())\n",
    "    except StopIteration:\n",
    "        break\n",
    "\n",
    "print(\"Il termine comparisonof dovrebbe stampare 808155:1 808156:1\")\n",
    "term_lexicon_row = daat.lexicon.get_entry(\"comparisonsof\") \n",
    "reader = Posting_List_Reader(term_lexicon_row, False, daat.file_DocIds, daat.file_Freq, daat.file_blocks)\n",
    "while True:\n",
    "    try:\n",
    "        next(reader)\n",
    "        print(reader.get_current_posting()) \n",
    "    except StopIteration:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3fe60d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
