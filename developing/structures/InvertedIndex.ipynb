{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b3a22a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\structures\\..\\structures\\LexiconRow.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\structures\\..\\structures\\DocumentIndex.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\structures\\..\\utilities\\General_Utilities.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\structures\\..\\structures\\DocumentIndexRow.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Davide\\IR\\Progetto\\structures\\..\\utilities\\Compression.ipynb\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import struct\n",
    "import import_ipynb\n",
    "from typing import List, Dict, Tuple, Union, Any, Callable\n",
    "from collections import Counter, defaultdict,OrderedDict\n",
    "from dataclasses import dataclass\n",
    "from typing import TextIO, BinaryIO\n",
    "\n",
    "\n",
    "sys.path.append('../')  # Go up two folders to the project root\n",
    "#from structures.Lexicon import Lexicon\n",
    "from structures.LexiconRow import LexiconRow\n",
    "from utilities.Compression import Compression \n",
    "from structures.DocumentIndex import DocumentIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6320a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Costants\n",
    "\n",
    "TYPE_DOC_ID=\"type_doc_id\"\n",
    "TYPE_FREQ=\"type_freq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adedd696",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Posting:\n",
    "    \n",
    "    SIZE_POSTING=16\n",
    "    \n",
    "    doc_id: int\n",
    "    frequency: Any = None\n",
    "    \n",
    "    #The following methods are used mainly for debug and tests.\n",
    "    \n",
    "    @classmethod \n",
    "    def from_string(cls, description:str):\n",
    "        docId,payl=description.split(\":\")\n",
    "        doc_id=int(docId)\n",
    "        frequency=int(payl)\n",
    "        return cls(doc_id, frequency)\n",
    "    \n",
    "    def write_to_disk(self,file_path:str,arg:str, offset:int=0)->None:\n",
    "        \"\"\"Function to open a file and write on it a single posting information in a specific position\n",
    "        \n",
    "        Args:\n",
    "            file_path: the file name to be opened in append mode\n",
    "            arg: indicates TYPE_DOC_ID or TYPE_FREQ the single information to be written\n",
    "            offset: the position in bytes inside a file to be written\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(file_path, 'ab') as file:\n",
    "            return self.write_to_disk_from_opened_file(file,arg,offset)\n",
    "        \n",
    "    def write_to_disk_from_opened_file(self,file:BinaryIO,arg,offset:int=0):\n",
    "        \"\"\"Function to write on a opened file an information of a posting in a specific position\n",
    "        \n",
    "        Args:\n",
    "            file: the file to be used\n",
    "            arg: inidcates TYPE_DOC_ID or TYPE_FREQ to write a single information\n",
    "            offset: the position in bytes inside a file to be written\n",
    "            \n",
    "        Returns:\n",
    "               the next offset position in the file after writing the information \n",
    "        \"\"\"\n",
    "        file.seek(offset)\n",
    "        if (arg==TYPE_DOC_ID):\n",
    "            binary_data = struct.pack(\"i\",self.doc_id)\n",
    "        if (arg==TYPE_FREQ):\n",
    "            binary_data = struct.pack(\"i\",self.frequency)\n",
    "    \n",
    "        file.write(binary_data)\n",
    "            \n",
    "        return struct.calcsize('i')+offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5e8eb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._index = defaultdict(list[Posting])\n",
    "         \n",
    "    def add_posting(self, term: str, doc_id: int, frequency: Any=None) -> None:\n",
    "        \"\"\"Adds a document to the posting list of a term.\n",
    "\n",
    "        Args:\n",
    "            term: the term to be added to the inveted index\n",
    "            doc_id: the document id to be added linked to a specific term\n",
    "            frequency: the frequency of a term inside a specific doc_id to be added \n",
    "            \n",
    "        \"\"\"\n",
    "        # append new posting to the posting list, we process one doc at time so\n",
    "        #it's not possible to have multiple doc_ids with same value.\n",
    "        if (self.get_postings(term)==None):\n",
    "            self._index[term]=[]\n",
    "        \n",
    "        self._index[term].append(Posting(doc_id,frequency))\n",
    "             \n",
    "    def get_postings(self, term: str) -> List[Posting]:\n",
    "        \"\"\"Fetches the posting list for a given term.\n",
    "        \n",
    "        Args:\n",
    "            term: the term to be found inside the inverted index\n",
    "\n",
    "        Returns:\n",
    "            the list of postings associated to the term\n",
    "        \"\"\"\n",
    "        \n",
    "        if (term in self._index):\n",
    "            return self._index[term]\n",
    "        return None\n",
    "    \n",
    "    def is_empty(self)->bool:\n",
    "        \"\"\"Check if there is no term in the inverted index.\"\"\"\n",
    "        return len(self.get_terms())==0\n",
    "    \n",
    "    def get_terms(self) -> List[str]:\n",
    "        \"\"\"Returns all unique terms in the index.\"\"\"\n",
    "        return self._index.keys() \n",
    "    \n",
    "    def clear_structure(self)->None:\n",
    "        \"\"\" It clears the inverted index data structure present in main memory.\"\"\"\n",
    "        self._index.clear()\n",
    "    \n",
    "    def get_structure(self)->None:\n",
    "        \"\"\"Returns the inverted index data structure.\"\"\"\n",
    "        return self._index\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def merge_posting_lists(posting_list_1:List[Posting],posting_list_2:List[Posting])->None:\n",
    "        \"\"\" Given two posting lists, it merges the two list in one single posting list and return the result.\n",
    "        Args:\n",
    "            posting_list_1: the first posting list to be merged\n",
    "            posting_list_2: the second posting list used to merge the first one\n",
    "            \n",
    "        Returns:\n",
    "            unique posting list of the concatenation of the 2 in input\n",
    "        \"\"\"\n",
    "        return posting_list_1+posting_list_2\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_max_term_frequency_of_posting_list(posting_list:List[Posting]) -> int:\n",
    "        \"\"\"\n",
    "        Given a postings list of a term, compute the maximum term frequency.\n",
    "    \n",
    "        Args:\n",
    "            postings_list: A posting list containing doc_ids and frequency.\n",
    "        \"\"\"\n",
    "        if not isinstance(posting_list, List):\n",
    "            raise ValueError(\"Invalid postings list.\")\n",
    "    \n",
    "        if len(posting_list) == 0:\n",
    "            return 0\n",
    "            \n",
    "        max_freq = max(posting_list, key=lambda x: x.frequency)    \n",
    "        return  max_freq.frequency  \n",
    "\n",
    "    @staticmethod\n",
    "    def write_to_files_a_posting_list(list_of_posting:List[Posting],compression_mode:bool,file_doc_ids:BinaryIO,file_freq:BinaryIO,offset_doc_ids:int,offset_freq:int)-> Tuple[int, int]:\n",
    "        \"\"\"Static method to write a posting list in 2 distinct files. \n",
    "           One file is used to save doc_ids and an other is used to save freqs.\n",
    "        \n",
    "        Args:\n",
    "            list_of_posting: the posting list to be saved\n",
    "            compression_mode: if the information should be compressed or not\n",
    "            file_doc_ids: the file used to save doc_ids\n",
    "            file_freq: the file used to save frequencies\n",
    "            offset_doc_ids: the start offset position for writing the list of doc_ids\n",
    "            offset_freq: the start offset position for writing the list of freq\n",
    "        Returns:\n",
    "            the new free offset position in the file_doc_ids after writing\n",
    "            the new free offset position in the file_freq after writing\n",
    "        \n",
    "        \"\"\"\n",
    "        doc_ids=[posting.doc_id for posting in list_of_posting]\n",
    "        freqs=[posting.frequency for posting in list_of_posting]\n",
    "        \n",
    "        file_doc_ids.seek(offset_doc_ids)  \n",
    "        file_freq.seek(offset_freq)\n",
    "           \n",
    "        if (not compression_mode):\n",
    "            packed_data = struct.pack('!{}i'.format(len(doc_ids)), *doc_ids)\n",
    "            file_doc_ids.write(packed_data)\n",
    "\n",
    "            packed_data = struct.pack('!{}i'.format(len(freqs)), *freqs)\n",
    "            file_freq.write(packed_data)\n",
    "            return offset_doc_ids+struct.calcsize('!{}i'.format(len(doc_ids))),offset_freq+struct.calcsize('!{}i'.format(len(freqs)))\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            compressed_doc_ids=Compression.d_gap_compression(doc_ids)\n",
    "            compressed_freq=Compression.unary_compression_integer_list(freqs)\n",
    "            file_doc_ids.write(compressed_doc_ids)\n",
    "            file_freq.write(compressed_freq)\n",
    "            \n",
    "            return offset_doc_ids+len(compressed_doc_ids),offset_freq+len(compressed_freq)\n",
    "        \n",
    "    @staticmethod\n",
    "    def read_from_files_a_posting_list(file_doc_ids:BinaryIO,file_freq:BinaryIO,\n",
    "                                       compression_mode:bool,\n",
    "                                       offset_doc_ids:int,offset_freq:int,\n",
    "                                       nr_postings:int,\n",
    "                                       doc_ids_byte_size:int=0,freq_byte_size:int=0,\n",
    "                                       min_doc_id:int=0)-> Tuple[List[Posting],int, int]:\n",
    "        \n",
    "        \"\"\"Static method to read 'a number of postings' of a posting list from 2 distinct files:\n",
    "           One file contains the saved doc_ids and the other contains the saved freqs.\n",
    "           \n",
    "        Args:\n",
    "            file_doc_ids: the file where doc_ids are saved\n",
    "            file_freq: the file where frequencies are saved\n",
    "            compression_mode: to specify if the bytes to be read must be decompressed or not\n",
    "            offset_doc_ids: the start offset position for reading the list of doc_ids\n",
    "            offset_freq: the start offset position for reading the list of frequency\n",
    "            nr_postings: indicates the number of elements to be read\n",
    "            \n",
    "            doc_ids_byte_size: used only if compression_mode is True instead of nr_postings and indicates the dimension in bytes of doc_ids to be read\n",
    "            freq_byte_size: used only if compression_mode is True instead of nr_postings the dimension in bytes of freqs to be read\n",
    "            min_doc_id: used only if compression_mode is True and indicates the number of elements to be read\n",
    "            \n",
    "           \n",
    "        Returns:\n",
    "            posting_list: read from the 2 files\n",
    "            offset_doc_id: new offset of the read file doc_ids\n",
    "            offeset_freqs: new offset of the read file freqs\n",
    "        \"\"\"\n",
    "        \n",
    "        file_doc_ids.seek(offset_doc_ids)  \n",
    "        file_freq.seek(offset_freq)\n",
    "        \n",
    "        if (not compression_mode):\n",
    "        \n",
    "            bytes_to_read=struct.calcsize('!{}i'.format(nr_postings))\n",
    "        \n",
    "            packed_data = file_doc_ids.read(bytes_to_read)\n",
    "            doc_ids = struct.unpack('!{}i'.format(nr_postings), packed_data)\n",
    "\n",
    "            packed_data = file_freq.read(bytes_to_read)\n",
    "            freqs = struct.unpack('!{}i'.format(nr_postings), packed_data)\n",
    "            \n",
    "            doc_ids_byte_size=bytes_to_read\n",
    "            freq_byte_size=bytes_to_read\n",
    "\n",
    "        else:\n",
    "            #In this case the number of posting represents also the number of freq to decompress.\n",
    "            freq_compressed_bytes = file_freq.read(freq_byte_size)\n",
    "            freqs=Compression.unary_decompression_integer_list(freq_compressed_bytes,nr_postings)\n",
    "            \n",
    "            doc_ids_compressed_bytes = file_doc_ids.read(doc_ids_byte_size)\n",
    "            doc_ids=Compression.d_gap_decompression(doc_ids_compressed_bytes,min_doc_id)\n",
    "            \n",
    "            \n",
    "        posting_list=[]\n",
    "        for doc_id, freq in zip(doc_ids, freqs):\n",
    "            posting_list.append(Posting(doc_id,freq))\n",
    "                \n",
    "        return posting_list,offset_doc_ids+doc_ids_byte_size,offset_freq+freq_byte_size\n",
    "        \n",
    "    \n",
    "    def write_to_block_all_index_in_memory(self,document_index:DocumentIndex,path_lexicon: str,path_doc_ids:str,path_freq:str)-> None:\n",
    "        \"\"\" Function to write the overall index in main memory to a file \"block\" during the SPIMI phase.\n",
    "            Pay attention, here the postings in a non compressed mode!\n",
    "            \n",
    "        Args:\n",
    "            path_lexicon: the position where to write the block related to the temporal lexicon.\n",
    "            path_doc_ids: the position where to write the block related to the temporal doc_ids.\n",
    "            path_freq: the position where to write the block related to the temporal freqs.\n",
    "            \n",
    "        \"\"\"\n",
    "        sorted_lexicon=sorted(self._index.items())\n",
    "        \n",
    "        with open(path_freq, \"ab\") as f_freq:\n",
    "            \n",
    "            with open(path_doc_ids, \"ab\") as f_doc_id:\n",
    "        \n",
    "                with open(path_lexicon, \"ab\") as f_lexicon:\n",
    "\n",
    "                    offset_lexicon=0\n",
    "                    offset_doc_ids=0\n",
    "                    offset_freq=0\n",
    "                    for term,postings in sorted_lexicon:\n",
    "                        #Istantiate a lexicon row\n",
    "                        lexRow=LexiconRow(term,len(postings),0,0,0,0,0,offset_doc_ids,offset_freq,0,0)\n",
    "                        \n",
    "                        for posting in postings:\n",
    "                            lexRow.update_term_upper_bound_bm25(posting.frequency, document_index.get_document(posting.doc_id).document_length)    \n",
    "                        \n",
    "                        #Save posting list to specifics file\n",
    "                        offset_doc_ids,offset_freq=InvertedIndex.write_to_files_a_posting_list(postings,False,f_doc_id,f_freq,offset_doc_ids,offset_freq)     \n",
    "                        \n",
    "                        #Save the related lexicon row.\n",
    "                        offset_lexicon=lexRow.write_lexicon_row_on_disk_to_opened_file(f_lexicon,offset_lexicon)\n",
    "\n",
    "    #Debugging output functions                     \n",
    "    \n",
    "    @staticmethod\n",
    "    \n",
    "    def write_to_file_a_posting_list_debug_mode(file_debug:TextIO,term:str, posting_list:List, new_term:bool) -> None:\n",
    "        \"\"\" \n",
    "        This method write in human readable format the entire inverted index processed. It concatenates \n",
    "        the posting lists amongs blocks on the same output file.\n",
    "        \n",
    "        Args:\n",
    "            file_debug: the file used for append contented on disk\n",
    "            term: the term related to the posting_list\n",
    "            posting_list: the elements to save on disk\n",
    "            new_term: indicates if a new term comes or not, during the partial saving of the posting list.\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        if (new_term):\n",
    "            file_debug.write(\"\\n\")\n",
    "            file_debug.write(term)\n",
    "\n",
    "        for posting in posting_list:\n",
    "            file_debug.write(f\" {posting.doc_id}\")\n",
    "            if posting.frequency:\n",
    "                file_debug.write(f\":{str(posting.frequency)}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def read_from_block_all_index_in_memory(self, path_lexicon: str,path_doc_ids:str, path_freq:str)-> None:\n",
    "        \"\"\" Function to read an inverted index in main memory related to a specific lexicon file.\n",
    "            This function is used for debug and test reasons.\n",
    "        \n",
    "        Args:\n",
    "            path_lexicon: the position where to read the lexicon related to a specific inverted index\n",
    "            path_doc_ids: the position where to read the information of doc_ids.\n",
    "            path_freq: the position where to read the information of freq.\n",
    "            \n",
    "        Returns:\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        self.clear_structure()\n",
    "        \n",
    "        offset_lexicon=0\n",
    "        offset_doc_id=0\n",
    "        offset_freq=0\n",
    "        \n",
    "        with open(path_freq, \"rb\") as f_freq:\n",
    "            with open(path_doc_ids, \"rb\") as f_doc_id:\n",
    "                with open(path_lexicon, \"rb\") as f_lexicon:\n",
    "                \n",
    "                    #Initialize empty lexicon row.\n",
    "                    lexiconTerm=LexiconRow(\"\",0)\n",
    "                    \n",
    "                    while (True):\n",
    "                        \n",
    "                        offset_lexicon=lexiconTerm.read_lexicon_row_on_disk_from_opened_file(f_lexicon,offset_lexicon)\n",
    "                        if(offset_lexicon==None):\n",
    "                            #Here, I finished to read all the lexicon block.\n",
    "                            return\n",
    "                        \n",
    "                        posting_list,offset_doc_id,offset_freq=InvertedIndex.read_from_files_a_posting_list(f_doc_id,f_freq,False,offset_doc_id,offset_freq,lexiconTerm.dft)\n",
    "                                                                                                         \n",
    "                        for posting in posting_list:\n",
    "                            self.add_posting(lexiconTerm.term,posting.doc_id,posting.frequency)\n",
    "                           \n",
    "    def write_to_block_debug_mode(self,file_name_index: str) -> None:\n",
    "        \"\"\" Write the inverted index (in memory) in lexicographical oreder into a file_name_index on disk human readable.\n",
    "            It is used just for debugging the spimi phase.\n",
    "        Args:\n",
    "            file_name_index: the name of the file where the inverted index will be written in clear\n",
    "            \n",
    "        \"\"\"\n",
    "        sorted_lexicon=sorted(self._index.items())\n",
    "        with open(file_name_index, \"w\") as f:\n",
    "            for term,postings in sorted_lexicon:\n",
    "                f.write(term)\n",
    "                for posting in postings:\n",
    "                    f.write(f\" {posting.doc_id}\")\n",
    "                    if posting.frequency:\n",
    "                        f.write(f\":{str(posting.frequency)}\")\n",
    "                f.write(\"\\n\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
