{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90a1843c",
   "metadata": {},
   "source": [
    "# NLP Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f7ec708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, there are defined some methods and functions to be used later in order to pre-process texts\n",
    "# and extracting useful informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70418e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import demoji\n",
    "import spacy \n",
    "import nltk \n",
    "import re\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ebc42fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading and installing the list of terms to be used on spacy\n",
    "\n",
    "#Uncomment the following line to re-execute the download and installation\n",
    "\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0da9614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.0/en_core_web_lg-2.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23eb182f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Davide\\AppData\\Local\\Temp\\ipykernel_5536\\2299328559.py:1: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    }
   ],
   "source": [
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57a823e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "\n",
    "############################## FIRST STEP ####################################\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "#Function suited for removing all most common type of emoji from text\n",
    "def remove_emoji(text):\n",
    "\n",
    "    dem = demoji.findall(text)\n",
    "    for item in dem.keys():\n",
    "        text = text.replace(item, \"\")\n",
    "    return text\n",
    "\n",
    "#Removing particular sequence or special charaters using regex:\n",
    "\n",
    "    #Carriege return and tabulation\n",
    "    #Special charaters/symbols like @,?,emoji(left), other metacharaters \n",
    "    #Date/times or numbers, money, percentages.\n",
    "    #Bulleted lists with particular simbols ex : o elem ,  - elem\n",
    "    #Links, emails,particular sequence of charaters\n",
    "    \n",
    "    #The reason why i dont replace at fly the numbers is because may be part of a noum or a word like  3D, MP3, MySQl2.4 or Python3 HTML5:\n",
    "    #removing the number the meaning of the word is lost.\n",
    "\n",
    "def remove_expressions(text,debug=False):\n",
    "    regex=r\"\\no\\t|\\n-|\\n|\\t|\\[u]\\S+|\\u2022|\\u2028|\\u2605|\\u25cf|\\uf0b7|\\u00ba|\\u2122|\\u270d|\\u200d|\\uf076|\\uf0a7|\\ðŸ”²|\\d[.]|[$]\\d+\\w*|\\d+[%]|\\d+/\\d+|\\d[+]|\\d+[:]\\d+|[+]\\d+|[@?~]|\\s&\\s|\\u25ba|https://\\S+|http://\\S+|\\S+@\\S+|www.\\S+|No. 1|<3\"\n",
    "\n",
    "    if (debug):\n",
    "        matches = re.findall(regex, text)\n",
    "        print(matches)\n",
    "\n",
    "    newText=re.sub(regex, \" \", text)\n",
    "    return newText\n",
    "\n",
    "#Used to remove special charaters using regex\n",
    "def clean_raw_text(text,debug=False):\n",
    "    text = text.lower() # all to lowercase\n",
    "    \n",
    "    text = remove_emoji(text) # remove all emoji from text\n",
    "    text = remove_expressions(text) #use reguo\n",
    "    \n",
    "    text = \" \".join(text.split())  # remove extra whitespace\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "############################## SECOND STEP ###################################\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "\n",
    "#Using spacy to analyze and remove additional un-useful information derived from the \n",
    "# structure of the sentences.\n",
    "\n",
    "\n",
    "#Input raw text and output a pre-process text using spacy library\n",
    "def process_text(text,debug=False):\n",
    "    \n",
    "    #nlp=spacy.blank(\"en\")\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    #Tokenization - Remove stopwords and token recognized as numbers  - Lemmatization\n",
    "    newText=\"\"\n",
    "    tokens=[]\n",
    "    for token in doc:\n",
    "        #token_text = token.text\n",
    "\n",
    "        if (token.is_stop==False \n",
    "            and token.pos_!=\"PUNCT\" \n",
    "            and token.pos_!=\"NUM\" \n",
    "            and token.pos_!=\"SYM\" \n",
    "            and not(token.text==\".\" and token.pos_==\"PROPN\") #Special case\n",
    "           ):\n",
    "            newText+=token.lemma_+\" \"\n",
    "\n",
    "        #token_text = token.text\n",
    "        #token_pos = token.pos_\n",
    "        #token_dep = token.dep_\n",
    "        #token_vect= token.has_vector\n",
    "        # This is for formatting only\n",
    "        #print(f\"{token_text:<12}{token_pos:<10}{token_dep:<10}{token_vect}\" )\n",
    "   \n",
    "    #stemmer = PorterStemmer()\n",
    "\n",
    "    #stem_text = []\n",
    "\n",
    "    #for word in doc:\n",
    "    #    stem_text.append(stemmer.stem(word.text))\n",
    "\n",
    "    #print (\"\\n\\nSteammed text:\")\n",
    "    #print(stem_text)    \n",
    "        \n",
    "    return newText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70704d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to call from outside      \n",
    "def clean_text(text,debug=False):\n",
    "     \n",
    "    if (debug==True):\n",
    "        print (\"Original text:\")\n",
    "        print(text)\n",
    "        \n",
    "    text=clean_raw_text(text,debug)\n",
    "    \n",
    "    if (debug==True):\n",
    "        print (\"\\nCleaned text:\")\n",
    "        print(text)\n",
    "    \n",
    "    text=process_text(text,debug)\n",
    "    \n",
    "    if (debug==True):\n",
    "        print (\"\\nFinal text after Spacy:\")\n",
    "        print(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "320b1de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot cloud word\n",
    "def plotWordCloud(text):\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    \n",
    "    doc=nlp(text)\n",
    "    tokens=[]\n",
    "    for token in doc:\n",
    "        token_text = token.text\n",
    "        tokens.append(token.text)\n",
    "    \n",
    "    word_frequency_distribution = nltk.FreqDist(tokens)\n",
    "    most_common = word_frequency_distribution.most_common(20)\n",
    "    print(\"\\n\")\n",
    "    print(most_common)\n",
    "    \n",
    "    word_cloud = WordCloud(collocations=False).generate(\" \".join(tokens))\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(word_cloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
